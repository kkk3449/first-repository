{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 추가 설치 사항\n",
    "- matplotlib와 numpy 버젼 안맞는 오류가 나서 numpy 버젼을 올려줌\n",
    "- conda install numpy==1.16\n",
    "- pip install opencv-python\n",
    "- pip install opencv-contrib-python\n",
    "- 재부팅 후 import matplotlib.pyplot as plt 관련 ModuleNotFoundError: No module named 'cycler'에러가 발생했으나 conda install cycler 로 해결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils import data\n",
    "from torch.utils.data import DataLoader #데이터 불러오기\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "#훈련\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "#시각화\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "import timeit\n",
    "from datetime import datetime\n",
    "import os\n",
    "import glob\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import yaml\n",
    "from addict import Dict\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch includes\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import make_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorboard include\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Custom includes\\nfrom dataloaders import cityscapes\\nfrom dataloaders import utils\\nfrom dataloaders import augmentation as augment\\nfrom models.liteseg import LiteSeg\\nfrom utils import loss as losses\\nfrom utils import iou_eval\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Custom includes\n",
    "from dataloaders import cityscapes\n",
    "from dataloaders import utils\n",
    "from dataloaders import augmentation as augment\n",
    "from models.liteseg import LiteSeg\n",
    "from utils import loss as losses\n",
    "from utils import iou_eval\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4.1\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LiteSeg-master/dataloaders/utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[ 7 20 23]\n",
      " [20 12 19]]\n"
     ]
    }
   ],
   "source": [
    "#LiteSeg-master/dataloaders/utils.py\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def listFiles(rootdir='.', suffix='png'):\n",
    "    \"\"\"Performs recursive glob with given suffix and rootdir\n",
    "        :param rootdir is the root directory\n",
    "        :param suffix is the suffix to be searched as PNG or JPG\n",
    "    \"\"\"\n",
    "    return [os.path.join(looproot, filename)\n",
    "        for looproot, _, filenames in os.walk(rootdir)\n",
    "        for filename in filenames if filename.endswith(suffix)]\n",
    "\n",
    "cityscapes_valid_classes = [7, 8, 11, 12, 13, 17, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 31, 32, 33]\n",
    "class_map = dict(zip( range(19),cityscapes_valid_classes))\n",
    "\n",
    "def convertTrainIdToClassId(img):\n",
    "         temp=np.copy(img)\n",
    "         for trainID in range(19):\n",
    "             #print(trainID,\" \" ,class_map[trainID])\n",
    "             temp[img==trainID]=class_map[trainID]\n",
    "             \n",
    "         return temp\n",
    "\n",
    "\n",
    "def get_cityscapes_labels():\n",
    "    return np.array([\n",
    "         #[  0,   0,   0],\n",
    "        [128, 64, 128],\n",
    "        [244, 35, 232],\n",
    "        [70, 70, 70],\n",
    "        [102, 102, 156],\n",
    "        [190, 153, 153],\n",
    "        [153, 153, 153],\n",
    "        [250, 170, 30],\n",
    "        [220, 220, 0],\n",
    "        [107, 142, 35],\n",
    "        [152, 251, 152],\n",
    "        [0, 130, 180],\n",
    "        [220, 20, 60],\n",
    "        [255, 0, 0],\n",
    "        [0, 0, 142],\n",
    "        [0, 0, 70],\n",
    "        [0, 60, 100],\n",
    "        [0, 80, 100],\n",
    "        [0, 0, 230],\n",
    "        [119, 11, 32]\n",
    "        ])\n",
    "\n",
    "\n",
    "\n",
    "def encode_segmap(mask):\n",
    "    \"\"\"Encode segmentation label images as pascal classes\n",
    "    Args:\n",
    "        mask (np.ndarray): raw segmentation label image of dimension\n",
    "          (M, N, 3), in which the Pascal classes are encoded as colours.\n",
    "    Returns:\n",
    "        (np.ndarray): class map with dimensions (M,N), where the value at\n",
    "        a given location is the integer denoting the class index.\n",
    "    \"\"\"\n",
    "    mask = mask.astype(int)\n",
    "    label_mask = np.zeros((mask.shape[0], mask.shape[1]), dtype=np.int16)\n",
    "    for ii, label in enumerate(get_cityscapes_labels()):#get_pascal_labels()\n",
    "        label_mask[np.where(np.all(mask == label, axis=-1))[:2]] = ii\n",
    "    label_mask = label_mask.astype(int)\n",
    "    return label_mask\n",
    "\n",
    "\n",
    "def decode_seg_map_sequence(label_masks, dataset='pascal'):\n",
    "    rgb_masks = []\n",
    "    for label_mask in label_masks:\n",
    "        rgb_mask = decode_segmap(label_mask, dataset)\n",
    "        rgb_masks.append(rgb_mask)\n",
    "    rgb_masks = torch.from_numpy(np.array(rgb_masks).transpose([0, 3, 1, 2]))\n",
    "    return rgb_masks\n",
    "\n",
    "def decode_segmap(label_mask, dataset, plot=False):\n",
    "    \"\"\"Decode segmentation class labels into a color image\n",
    "    Args:\n",
    "        label_mask (np.ndarray): an (M,N) array of integer values denoting\n",
    "          the class label at each spatial location.\n",
    "        plot (bool, optional): whether to show the resulting color image\n",
    "          in a figure.\n",
    "    Returns:\n",
    "        (np.ndarray, optional): the resulting decoded color image.\n",
    "    \"\"\"\n",
    "    if dataset == 'pascal':\n",
    "      print()\n",
    "    elif dataset == 'cityscapes':\n",
    "        n_classes = 19\n",
    "        label_colours = get_cityscapes_labels()\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    r = label_mask.copy()\n",
    "    g = label_mask.copy()\n",
    "    b = label_mask.copy()\n",
    "    for ll in range(0, n_classes):\n",
    "        r[label_mask == ll] = label_colours[ll, 0]\n",
    "        g[label_mask == ll] = label_colours[ll, 1]\n",
    "        b[label_mask == ll] = label_colours[ll, 2]\n",
    "    \n",
    "    r[label_mask == 255] = 0\n",
    "    g[label_mask == 255] = 0\n",
    "    b[label_mask == 255] =0\n",
    "    \n",
    "    rgb = np.zeros((label_mask.shape[0], label_mask.shape[1], 3))\n",
    "   # rgb = np.zeros((label_mask.shape[1], label_mask.shape[2], 3))\n",
    "    #replace blue with red as opencv uses bgr\n",
    "    rgb[:, :, 0] = r /255.0     \n",
    "    rgb[:, :, 1] = g /255.0\n",
    "    rgb[:, :, 2] = b /255.0\n",
    "#    \n",
    "#    rgb = np.zeros((label_mask.shape[1], label_mask.shape[2], 3))\n",
    "#    #replace blue with red as opencv uses bgr\n",
    "#    rgb[:, :, 0] = b #/255.0     \n",
    "#    rgb[:, :, 1] = g #/255.0\n",
    "#    rgb[:, :, 2] = r #/255.0\n",
    "#    \n",
    "    if plot:\n",
    "        plt.imshow(rgb)\n",
    "        plt.show()\n",
    "    else:\n",
    "        return rgb\n",
    "def decode_segmap_cv(label_mask, dataset, plot=False):\n",
    "    \"\"\"Decode segmentation class labels into a color image\n",
    "    Args:\n",
    "        label_mask (np.ndarray): an (M,N) array of integer values denoting\n",
    "          the class label at each spatial location.\n",
    "        plot (bool, optional): whether to show the resulting color image\n",
    "          in a figure.\n",
    "    Returns:\n",
    "        (np.ndarray, optional): the resulting decoded color image.\n",
    "    \"\"\"\n",
    "    if dataset == 'pascal':\n",
    "      print()\n",
    "    elif dataset == 'cityscapes':\n",
    "        n_classes = 19\n",
    "        label_colours = get_cityscapes_labels()\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    r = label_mask.copy()\n",
    "    g = label_mask.copy()\n",
    "    b = label_mask.copy()\n",
    "    for ll in range(0, n_classes):\n",
    "        r[label_mask == ll] = label_colours[ll, 0]\n",
    "        g[label_mask == ll] = label_colours[ll, 1]\n",
    "        b[label_mask == ll] = label_colours[ll, 2]\n",
    "    \n",
    "    r[label_mask == 255] = 0\n",
    "    g[label_mask == 255] = 0\n",
    "    b[label_mask == 255] =0\n",
    "    \n",
    "#    rgb = np.zeros((label_mask.shape[0], label_mask.shape[1], 3))\n",
    "#   # rgb = np.zeros((label_mask.shape[1], label_mask.shape[2], 3))\n",
    "#    #replace blue with red as opencv uses bgr\n",
    "#    rgb[:, :, 0] = r /255.0     \n",
    "#    rgb[:, :, 1] = g /255.0\n",
    "#    rgb[:, :, 2] = b /255.0\n",
    "#    \n",
    "    rgb = np.zeros((label_mask.shape[1], label_mask.shape[2], 3))\n",
    "    #replace blue with red as opencv uses bgr\n",
    "    rgb[:, :, 0] = b #/255.0     \n",
    "    rgb[:, :, 1] = g #/255.0\n",
    "    rgb[:, :, 2] = r #/255.0\n",
    "#    \n",
    "    if plot:\n",
    "        plt.imshow(rgb)\n",
    "        plt.show()\n",
    "    else:\n",
    "        return rgb\n",
    "def generate_param_report(logfile, param):\n",
    "    log_file = open(logfile, 'w')\n",
    "    for key, val in param.items():\n",
    "        log_file.write(key + ':' + str(val) + '\\n')\n",
    "    log_file.close()\n",
    "\n",
    "\n",
    "def lr_poly(base_lr, iter_, max_iter=100, power=0.9):\n",
    "    return base_lr * ((1 - float(iter_) / max_iter) ** power)\n",
    "\n",
    "\n",
    "    \n",
    "from torchvision import transforms \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print()\n",
    "    ar=np.array([[0,7,10],[7,3,6]])\n",
    "    z=convertTrainIdToClassId(ar)\n",
    "#    img3= transforms.ToPILImage()(torch.from_numpy(ou).type(torch.FloatTensor))#.detach().cpu()\n",
    "#    img3.save(oupath)\n",
    "    print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cityscapes code\n",
    "- LiteSeg-master/dataloaders/cityscapes.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#from torch.utils.data import TensorDataset, DataLoader\n",
    "#from dataloaders.utils import listFiles #모르겠음\n",
    "\n",
    "class Cityscapes(data.Dataset):\n",
    "\n",
    "    def __init__(self, root='path', split=\"train\", transform=None,extra=False):\n",
    "        \"\"\"\n",
    "        Cityscapes dataset folder has two folders, 'leftImg8bit' folder for images and 'gtFine_trainvaltest' \n",
    "        folder for annotated images with fine annotations 'labels'.\n",
    "        \"\"\"\n",
    "        self.root = root\n",
    "        self.split = split #train, validation, and test sets\n",
    "        self.transform = transform\n",
    "        self.files = {}\n",
    "        self.n_classes = 19\n",
    "        self.extra=extra\n",
    "\n",
    "        if not self.extra:\n",
    "            print(\"Using fine dataset\")\n",
    "            self.images_path = os.path.join(self.root, 'leftImg8bit_trainvaltest','leftImg8bit', self.split) #실제이미지\n",
    "            self.labels_path = os.path.join(self.root, 'gtFine_trainvaltest', 'gtFine', self.split) #세그멘테이션 이미지\n",
    "        else:\n",
    "            print(\"Using Coarse dataset\")\n",
    "\n",
    "            self.images_path = os.path.join(self.root, 'leftImg8bit', self.split)\n",
    "            self.labels_path = os.path.join(self.root, 'gtCoarse', 'gtCoarse', self.split)            \n",
    "            \n",
    "        #print(self.images_path)\n",
    "        self.files[split] = listFiles(rootdir=self.images_path, suffix='.png')#list of the pathes to images\n",
    "\n",
    "        self.void_classes = [0, 1, 2, 3, 4, 5, 6, 9, 10, 14, 15, 16, 18, 29, 30, -1] #not to train\n",
    "        self.valid_classes = [7, 8, 11, 12, 13, 17, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 31, 32, 33]\n",
    "        self.class_names = ['road', 'sidewalk', 'building', 'wall', 'fence', \\\n",
    "                            'pole', 'traffic_light', 'traffic_sign', 'vegetation', 'terrain', \\\n",
    "                            'sky', 'person', 'rider', 'car', 'truck', 'bus', 'train', \\\n",
    "                            'motorcycle', 'bicycle']\n",
    "        \n",
    "        self.ignore_index = 255\n",
    "        self.class_map = dict(zip(self.valid_classes, range(self.n_classes)))\n",
    "        #print(self.class_map)\n",
    "        \n",
    "        if not self.files[split]:\n",
    "            raise Exception(\"No files for split=[%s] found in %s\" % (split, self.images.path))\n",
    "\n",
    "        print(\"Found %d %s images\" % (len(self.files[split]), split))\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.files[self.split])\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.files[self.split][index].rstrip()\n",
    "        #print(image_path)\n",
    "        if not self.extra:\n",
    "            label_path = os.path.join(self.labels_path,\n",
    "                                image_path.split(os.sep)[-2],\n",
    "                                os.path.basename(image_path)[:-15] + 'gtFine_labelIds.png')\n",
    "        else:\n",
    "            label_path = os.path.join(self.labels_path,\n",
    "                                image_path.split(os.sep)[-2],\n",
    "                                os.path.basename(image_path)[:-15] + 'gtCoarse_labelIds.png')\n",
    "        _img = Image.open(image_path).convert('RGB')\n",
    "        _tmp = np.array(Image.open(label_path), dtype=np.uint8)\n",
    "        _tmp = self.encode_segmap(_tmp)\n",
    "\n",
    "        _target = Image.fromarray(_tmp)\n",
    "\n",
    "        sample = {'image': _img, 'label': _target}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n",
    "    \n",
    "    def encode_segmap(self, mask):\n",
    "        # Put all void classes to ignore_index\n",
    "        for _voidc in self.void_classes:\n",
    "            mask[mask == _voidc] = self.ignore_index\n",
    "        for _validc in self.valid_classes:\n",
    "            mask[mask == _validc] = self.class_map[_validc]\n",
    "        return mask\n",
    "\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentation code\n",
    "- LiteSeg-master/dataloaders/augmentation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import numbers\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "class RandomCrop(object):\n",
    "    def __init__(self, size, padding=0):\n",
    "        if isinstance(size, numbers.Number):\n",
    "            self.size = (int(size), int(size))\n",
    "        else:\n",
    "            self.size = size # h, w\n",
    "        self.padding = padding\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        img, mask = sample['image'], sample['label']\n",
    "\n",
    "        if self.padding > 0:\n",
    "            img = ImageOps.expand(img, border=self.padding, fill=255)\n",
    "            mask = ImageOps.expand(mask, border=self.padding, fill=255)\n",
    "\n",
    "        assert img.size == mask.size\n",
    "        w, h = img.size\n",
    "        th, tw = self.size # target size\n",
    "        if w == tw and h == th:\n",
    "            return {'image': img,\n",
    "                    'label': mask}\n",
    "        if w < tw or h < th:\n",
    "            img = img.resize((tw, th), Image.BILINEAR)\n",
    "            mask = mask.resize((tw, th), Image.NEAREST)\n",
    "            return {'image': img,\n",
    "                    'label': mask}\n",
    "\n",
    "        x1 = random.randint(0, w - tw)\n",
    "        y1 = random.randint(0, h - th)\n",
    "        img = img.crop((x1, y1, x1 + tw, y1 + th))\n",
    "        mask = mask.crop((x1, y1, x1 + tw, y1 + th))\n",
    "\n",
    "        return {'image': img,\n",
    "                'label': mask}\n",
    "\n",
    "\n",
    "class CenterCrop(object):\n",
    "    def __init__(self, size):\n",
    "        if isinstance(size, numbers.Number):\n",
    "            self.size = (int(size), int(size))\n",
    "        else:\n",
    "            self.size = size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        img = sample['image']\n",
    "        mask = sample['label']\n",
    "        assert img.size == mask.size\n",
    "        w, h = img.size\n",
    "        th, tw = self.size\n",
    "        x1 = int(round((w - tw) / 2.))\n",
    "        y1 = int(round((h - th) / 2.))\n",
    "        img = img.crop((x1, y1, x1 + tw, y1 + th))\n",
    "        mask = mask.crop((x1, y1, x1 + tw, y1 + th))\n",
    "\n",
    "        return {'image': img,\n",
    "                'label': mask}\n",
    "\n",
    "\n",
    "class RandomHorizontalFlip(object):\n",
    "    def __call__(self, sample):\n",
    "        img = sample['image']\n",
    "        mask = sample['label']\n",
    "        if random.random() < 0.5:\n",
    "            img = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "            mask = mask.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "\n",
    "        return {'image': img,\n",
    "                'label': mask}\n",
    "\n",
    "\n",
    "class Normalize(object):\n",
    "    \"\"\"Normalize a tensor image with mean and standard deviation.\n",
    "    Args:\n",
    "        mean (tuple): means for each channel.\n",
    "        std (tuple): standard deviations for each channel.\n",
    "    \"\"\"\n",
    "    def __init__(self, mean=(0., 0., 0.), std=(1., 1., 1.)):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        img = np.array(sample['image']).astype(np.float32)\n",
    "        mask = np.array(sample['label']).astype(np.float32)\n",
    "        img /= 255.0\n",
    "        img -= self.mean\n",
    "        img /= self.std\n",
    "\n",
    "        return {'image': img,\n",
    "                'label': mask}\n",
    "\n",
    "\n",
    "class Normalize_cityscapes(object):\n",
    "    \"\"\"Normalize a tensor image with mean and standard deviation.\n",
    "    Args:\n",
    "        mean (tuple): means for each channel.\n",
    "        std (tuple): standard deviations for each channel.\n",
    "    \"\"\"\n",
    "    def __init__(self, mean=(0., 0., 0.)):\n",
    "        self.mean = mean\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        img = np.array(sample['image']).astype(np.float32)\n",
    "        mask = np.array(sample['label']).astype(np.float32)\n",
    "        img -= self.mean\n",
    "        img /= 255.0\n",
    "\n",
    "        return {'image': img,\n",
    "                'label': mask}\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        img = np.array(sample['image']).astype(np.float32).transpose((2, 0, 1))\n",
    "        mask = np.expand_dims(np.array(sample['label']).astype(np.float32), -1).transpose((2, 0, 1))\n",
    "#        mask[mask == 255] = 19\n",
    "\n",
    "        img = torch.from_numpy(img).float()\n",
    "        mask = torch.from_numpy(mask).float()\n",
    "#        print(\"MAsk from Custom transforsms\"+str(mask.size()))\n",
    "\n",
    "        return {'image': img,\n",
    "                'label': mask}\n",
    "\n",
    "\n",
    "class FixedResize(object):\n",
    "    def __init__(self, size):\n",
    "        self.size = tuple(reversed(size))  # size: (h, w)\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        img = sample['image']\n",
    "        mask = sample['label']\n",
    "\n",
    "        assert img.size == mask.size\n",
    "\n",
    "        img = img.resize(self.size, Image.BILINEAR)\n",
    "        mask = mask.resize(self.size, Image.NEAREST)\n",
    "\n",
    "        return {'image': img,\n",
    "                'label': mask}\n",
    "\n",
    "\n",
    "class Scale(object):\n",
    "    def __init__(self, size):\n",
    "        if isinstance(size, numbers.Number):\n",
    "            self.size = (int(size), int(size))\n",
    "        else:\n",
    "            self.size = size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        img = sample['image']\n",
    "        mask = sample['label']\n",
    "        assert img.size == mask.size\n",
    "        w, h = img.size\n",
    "\n",
    "        if (w >= h and w == self.size[1]) or (h >= w and h == self.size[0]):\n",
    "            return {'image': img,\n",
    "                    'label': mask}\n",
    "        oh, ow = self.size\n",
    "        img = img.resize((ow, oh), Image.BILINEAR)\n",
    "        mask = mask.resize((ow, oh), Image.NEAREST)\n",
    "\n",
    "        return {'image': img,\n",
    "                'label': mask}\n",
    "\n",
    "\n",
    "class RandomSizedCrop(object):\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        img = sample['image']\n",
    "        mask = sample['label']\n",
    "        assert img.size == mask.size\n",
    "        for attempt in range(10):\n",
    "            area = img.size[0] * img.size[1]\n",
    "            target_area = random.uniform(0.45, 1.0) * area\n",
    "            aspect_ratio = random.uniform(0.5, 2)\n",
    "\n",
    "            w = int(round(math.sqrt(target_area * aspect_ratio)))\n",
    "            h = int(round(math.sqrt(target_area / aspect_ratio)))\n",
    "\n",
    "            if random.random() < 0.5:\n",
    "                w, h = h, w\n",
    "\n",
    "            if w <= img.size[0] and h <= img.size[1]:\n",
    "                x1 = random.randint(0, img.size[0] - w)\n",
    "                y1 = random.randint(0, img.size[1] - h)\n",
    "\n",
    "                img = img.crop((x1, y1, x1 + w, y1 + h))\n",
    "                mask = mask.crop((x1, y1, x1 + w, y1 + h))\n",
    "                assert (img.size == (w, h))\n",
    "\n",
    "                img = img.resize((self.size, self.size), Image.BILINEAR)\n",
    "                mask = mask.resize((self.size, self.size), Image.NEAREST)\n",
    "\n",
    "                return {'image': img,\n",
    "                        'label': mask}\n",
    "\n",
    "        # Fallback\n",
    "        scale = Scale(self.size)\n",
    "        crop = CenterCrop(self.size)\n",
    "        sample = crop(scale(sample))\n",
    "        return sample\n",
    "\n",
    "\n",
    "class RandomRotate(object):\n",
    "    def __init__(self, degree):\n",
    "        self.degree = degree\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        img = sample['image']\n",
    "        mask = sample['label']\n",
    "        rotate_degree = random.random() * 2 * self.degree - self.degree\n",
    "        img = img.rotate(rotate_degree, Image.BILINEAR)\n",
    "        #mask = mask.rotate(rotate_degree, Image.NEAREST)\n",
    "        im2 = mask.convert('RGBA')\n",
    "        # rotated image\n",
    "        rot = im2.rotate(rotate_degree, Image.NEAREST)\n",
    "        # a white image same size as rotated image\n",
    "        fff = Image.new('RGBA', rot.size, (255,)*4)\n",
    "        # create a composite image using the alpha layer of rot as a mask\n",
    "        out = Image.composite(rot, fff, rot)\n",
    "        # save your work (converting back to mode='1' or whatever..)\n",
    "        out=out.convert(mask.mode)\n",
    "\n",
    "        return {'image': img,\n",
    "                'label': out}\n",
    "\n",
    "\n",
    "class RandomSized(object):\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "        self.scale = Scale(self.size)\n",
    "        self.crop = RandomCrop(self.size)\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        img = sample['image']\n",
    "        mask = sample['label']\n",
    "        assert img.size == mask.size\n",
    "\n",
    "        w = int(random.uniform(0.8, 2.5) * img.size[0])\n",
    "        h = int(random.uniform(0.8, 2.5) * img.size[1])\n",
    "\n",
    "        img, mask = img.resize((w, h), Image.BILINEAR), mask.resize((w, h), Image.NEAREST)\n",
    "        sample = {'image': img, 'label': mask}\n",
    "\n",
    "        return self.crop(self.scale(sample))\n",
    "class RandomScaleCrop(object):\n",
    "    def __init__(self, base_size, crop_size, fill=255):\n",
    "        self.base_size = base_size\n",
    "        self.crop_size = crop_size\n",
    "        self.fill = fill\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        img = sample['image']\n",
    "        mask = sample['label']\n",
    "        # random scale (short edge)\n",
    "        short_size = random.randint(int(self.base_size * 0.5), int(self.base_size * 2.0))\n",
    "        w, h = img.size\n",
    "        if h > w:\n",
    "            ow = short_size\n",
    "            oh = int(1.0 * h * ow / w)\n",
    "        else:\n",
    "            oh = short_size\n",
    "            ow = int(1.0 * w * oh / h)\n",
    "        img = img.resize((ow, oh), Image.BILINEAR)\n",
    "        mask = mask.resize((ow, oh), Image.NEAREST)\n",
    "        # pad crop\n",
    "        if short_size < self.crop_size:\n",
    "            padh = self.crop_size - oh if oh < self.crop_size else 0\n",
    "            padw = self.crop_size - ow if ow < self.crop_size else 0\n",
    "            img = ImageOps.expand(img, border=(0, 0, padw, padh), fill=0)\n",
    "            mask = ImageOps.expand(mask, border=(0, 0, padw, padh), fill=self.fill)\n",
    "        # random crop crop_size\n",
    "        w, h = img.size\n",
    "        x1 = random.randint(0, w - self.crop_size)\n",
    "        y1 = random.randint(0, h - self.crop_size)\n",
    "        img = img.crop((x1, y1, x1 + self.crop_size, y1 + self.crop_size))\n",
    "        mask = mask.crop((x1, y1, x1 + self.crop_size, y1 + self.crop_size))\n",
    "\n",
    "        return {'image': img,\n",
    "                'label': mask}\n",
    "        \n",
    "class RandomScale(object):\n",
    "    def __init__(self, limit):\n",
    "        self.limit = limit\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        img = sample['image']\n",
    "        mask = sample['label']\n",
    "        assert img.size == mask.size\n",
    "\n",
    "        scale = random.uniform(self.limit[0], self.limit[1])\n",
    "        w = int(scale * img.size[0])\n",
    "        h = int(scale * img.size[1])\n",
    "\n",
    "        img, mask = img.resize((w, h), Image.BILINEAR), mask.resize((w, h), Image.NEAREST)\n",
    "\n",
    "        return {'image': img, 'label': mask}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj17/anaconda3/envs/sia/lib/python3.7/site-packages/scipy/__init__.py:140: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.15.4)\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "import lightnet.network as lnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LiteSeg-master/models/backbone_networks/darknet.py\n",
    "- git = pytorch 0.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"os =32\\n ('1_convbatch',     lnn.layer.Conv2dBatchReLU(input_channels, 32, 3, 1, 1)),\\n                ('2_max',           nn.MaxPool2d(2, 2)),\\n                ('3_convbatch',     lnn.layer.Conv2dBatchReLU(32, 64, 3, 1, 1)),\\n                ('4_max',           nn.MaxPool2d(2, 2)),\\n                ('5_convbatch',     lnn.layer.Conv2dBatchReLU(64, 128, 3, 1, 1)),\\n                ('6_convbatch',     lnn.layer.Conv2dBatchReLU(128, 64, 1, 1, 0)),\\n                ('7_convbatch',     lnn.layer.Conv2dBatchReLU(64, 128, 3, 1, 1)),\\n                ('8_max',           nn.MaxPool2d(2,2)),\\n                ('9_convbatch',     lnn.layer.Conv2dBatchReLU(128, 256, 3, 1, 1)),#2>1\\n                ('10_convbatch',    lnn.layer.Conv2dBatchReLU(256, 128, 1, 1, 0)),\\n                ('11_convbatch',    lnn.layer.Conv2dBatchReLU(128, 256, 3, 1, 1)),\\n                ('12_max',          nn.MaxPool2d(2, 2)),\\n                ('13_convbatch',    lnn.layer.Conv2dBatchReLU(256, 512, 3, 1, 1,dilation=2)),\\n                ('14_convbatch',    lnn.layer.Conv2dBatchReLU(512, 256, 1,1, 0,dilation=2)),\\n                ('15_convbatch',    lnn.layer.Conv2dBatchReLU(256, 512, 3, 1, 1,dilation=2)),\\n                ('16_convbatch',    lnn.layer.Conv2dBatchReLU(512, 256, 1, 1, 0,dilation=2)),\\n                ('17_convbatch',    lnn.layer.Conv2dBatchReLU(256, 512, 3, 1, 1,dilation=2)),\\n                ('18_max',          nn.MaxPool2d(1,1)),\\n                ('19_convbatch',    lnn.layer.Conv2dBatchReLU(512, 1024, 3, 1, 1,dilation=4)),\\n                ('20_convbatch',    lnn.layer.Conv2dBatchReLU(1024, 512, 1, 1, 0,dilation=4)),\\n                ('21_convbatch',    lnn.layer.Conv2dBatchReLU(512, 1024, 3, 1, 1,dilation=4)),\\n                ('22_convbatch',    lnn.layer.Conv2dBatchReLU(1024, 512, 1, 1, 0,dilation=4)),\\n                ('23_convbatch',    lnn.layer.Conv2dBatchReLU(512, 1024, 3, 1, 1,dilation=4))\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Nov 19 21:47:01 2018\n",
    "\n",
    "@author: Taha Emara  @email: taha@emaraic.com 'based on lightnet library by EAVISE' \n",
    "\"\"\"\n",
    "\n",
    "from collections import OrderedDict\n",
    "import torch.nn as nn\n",
    "import lightnet.network as lnn\n",
    "\n",
    "\n",
    "\n",
    "class Darknet19(lnn.module.Darknet):\n",
    "    \"\"\" `Darknet19`_ implementation with pytorch.\n",
    "\n",
    "    Args:\n",
    "        weights_file (str, optional): Path to the saved weights; Default **None**\n",
    "        input_channels (Number, optional): Number of input channels; Default **3**\n",
    "\n",
    "    .. _Darknet19: https://github.com/pjreddie/darknet/blob/master/cfg/darknet19.cfg\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, weights_file=None, input_channels=3):\n",
    "        \"\"\" Network initialisation \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        # Network\n",
    "        self.layers = nn.Sequential(\n",
    "            OrderedDict([\n",
    "                ('1_convbatch',     lnn.layer.Conv2dBatchReLU(input_channels, 32, 3, 1, 1)),\n",
    "                ('2_max',           nn.MaxPool2d(2, 2)),\n",
    "                ('3_convbatch',     lnn.layer.Conv2dBatchReLU(32, 64, 3, 1, 1)),\n",
    "                ('4_max',           nn.MaxPool2d(2, 2)),\n",
    "                ('5_convbatch',     lnn.layer.Conv2dBatchReLU(64, 128, 3, 1, 1)),\n",
    "                ('6_convbatch',     lnn.layer.Conv2dBatchReLU(128, 64, 1, 1, 0)),\n",
    "                ('7_convbatch',     lnn.layer.Conv2dBatchReLU(64, 128, 3, 1, 1)),\n",
    "                ('8_max',           nn.MaxPool2d(2,2)),\n",
    "                ('9_convbatch',     lnn.layer.Conv2dBatchReLU(128, 256, 3, 1, 1)),#2>1\n",
    "                ('10_convbatch',    lnn.layer.Conv2dBatchReLU(256, 128, 1, 1, 0)),\n",
    "                ('11_convbatch',    lnn.layer.Conv2dBatchReLU(128, 256, 3, 1, 1)),\n",
    "                ('12_max',          nn.MaxPool2d(2,2)),\n",
    "                ('13_convbatch',    lnn.layer.Conv2dBatchReLU(256, 512, 3, 1, 1,dilation=1)),\n",
    "                ('14_convbatch',    lnn.layer.Conv2dBatchReLU(512, 256, 1,1, 1,dilation=1)),\n",
    "                ('15_convbatch',    lnn.layer.Conv2dBatchReLU(256, 512, 3, 1, 1,dilation=1)),\n",
    "                ('16_convbatch',    lnn.layer.Conv2dBatchReLU(512, 256, 1, 1, 1,dilation=1)),\n",
    "                ('17_convbatch',    lnn.layer.Conv2dBatchReLU(256, 512, 3, 1, 1,dilation=1)),\n",
    "                ('18_max',          nn.MaxPool2d(1,1)),\n",
    "                ('19_convbatch',    lnn.layer.Conv2dBatchReLU(512, 1024, 3, 1, 1,dilation=2)),\n",
    "                ('20_convbatch',    lnn.layer.Conv2dBatchReLU(1024, 512, 1, 1, 1,dilation=2)),\n",
    "                ('21_convbatch',    lnn.layer.Conv2dBatchReLU(512, 1024, 3, 1, 1,dilation=2)),\n",
    "                ('22_convbatch',    lnn.layer.Conv2dBatchReLU(1024, 512, 1, 1, 0,dilation=2)),\n",
    "                ('23_convbatch',    lnn.layer.Conv2dBatchReLU(512, 1024, 3, 1, 1,dilation=2))\n",
    "            ])\n",
    "        )\n",
    "\n",
    "        if weights_file is not None:\n",
    "            self.load(weights_file)\n",
    "\n",
    "    def _forward(self, x):\n",
    "        i=0\n",
    "        for i,l in enumerate( self.layers[:7]):\n",
    "            #print(str(i),x.size())\n",
    "            x = l(x)\n",
    "        keep = x\n",
    "        for z,l in enumerate(self.layers[7:]):\n",
    "            #print(str(z+i),x.size())\n",
    "            x = l(x)\n",
    "        return x,keep\n",
    "\n",
    "\"\"\" os =16\n",
    "   ('1_convbatch',     lnn.layer.Conv2dBatchReLU(input_channels, 32, 3, 1, 1)),\n",
    "                ('2_max',           nn.MaxPool2d(2, 2)),\n",
    "                ('3_convbatch',     lnn.layer.Conv2dBatchReLU(32, 64, 3, 1, 1)),\n",
    "                ('4_max',           nn.MaxPool2d(2, 2)),\n",
    "                ('5_convbatch',     lnn.layer.Conv2dBatchReLU(64, 128, 3, 1, 1)),\n",
    "                ('6_convbatch',     lnn.layer.Conv2dBatchReLU(128, 64, 1, 1, 0)),\n",
    "                ('7_convbatch',     lnn.layer.Conv2dBatchReLU(64, 128, 3, 1, 1)),\n",
    "                ('8_max',           nn.MaxPool2d(2,2)),\n",
    "                ('9_convbatch',     lnn.layer.Conv2dBatchReLU(128, 256, 3, 1, 1)),#2>1\n",
    "                ('10_convbatch',    lnn.layer.Conv2dBatchReLU(256, 128, 1, 1, 0)),\n",
    "                ('11_convbatch',    lnn.layer.Conv2dBatchReLU(128, 256, 3, 1, 1)),\n",
    "                ('12_max',          nn.MaxPool2d(2, 2)),\n",
    "                ('13_convbatch',    lnn.layer.Conv2dBatchReLU(256, 512, 3, 1, 1,dilation=1)),\n",
    "                ('14_convbatch',    lnn.layer.Conv2dBatchReLU(512, 256, 1,1, 1,dilation=1)),\n",
    "                ('15_convbatch',    lnn.layer.Conv2dBatchReLU(256, 512, 3, 1, 1,dilation=1)),\n",
    "                ('16_convbatch',    lnn.layer.Conv2dBatchReLU(512, 256, 1, 1, 1,dilation=1)),\n",
    "                ('17_convbatch',    lnn.layer.Conv2dBatchReLU(256, 512, 3, 1, 1,dilation=1)),\n",
    "                ('18_max',          nn.MaxPool2d(1,1)),\n",
    "                ('19_convbatch',    lnn.layer.Conv2dBatchReLU(512, 1024, 3, 1, 1,dilation=2)),\n",
    "                ('20_convbatch',    lnn.layer.Conv2dBatchReLU(1024, 512, 1, 1, 1,dilation=2)),\n",
    "                ('21_convbatch',    lnn.layer.Conv2dBatchReLU(512, 1024, 3, 1, 1,dilation=2)),\n",
    "                ('22_convbatch',    lnn.layer.Conv2dBatchReLU(1024, 512, 1, 1, 0,dilation=2)),\n",
    "                ('23_convbatch',    lnn.layer.Conv2dBatchReLU(512, 1024, 3, 1, 1,dilation=2))\n",
    "                \n",
    "\"\"\"\n",
    "\"\"\"os =32\n",
    " ('1_convbatch',     lnn.layer.Conv2dBatchReLU(input_channels, 32, 3, 1, 1)),\n",
    "                ('2_max',           nn.MaxPool2d(2, 2)),\n",
    "                ('3_convbatch',     lnn.layer.Conv2dBatchReLU(32, 64, 3, 1, 1)),\n",
    "                ('4_max',           nn.MaxPool2d(2, 2)),\n",
    "                ('5_convbatch',     lnn.layer.Conv2dBatchReLU(64, 128, 3, 1, 1)),\n",
    "                ('6_convbatch',     lnn.layer.Conv2dBatchReLU(128, 64, 1, 1, 0)),\n",
    "                ('7_convbatch',     lnn.layer.Conv2dBatchReLU(64, 128, 3, 1, 1)),\n",
    "                ('8_max',           nn.MaxPool2d(2,2)),\n",
    "                ('9_convbatch',     lnn.layer.Conv2dBatchReLU(128, 256, 3, 1, 1)),#2>1\n",
    "                ('10_convbatch',    lnn.layer.Conv2dBatchReLU(256, 128, 1, 1, 0)),\n",
    "                ('11_convbatch',    lnn.layer.Conv2dBatchReLU(128, 256, 3, 1, 1)),\n",
    "                ('12_max',          nn.MaxPool2d(2, 2)),\n",
    "                ('13_convbatch',    lnn.layer.Conv2dBatchReLU(256, 512, 3, 1, 1,dilation=2)),\n",
    "                ('14_convbatch',    lnn.layer.Conv2dBatchReLU(512, 256, 1,1, 0,dilation=2)),\n",
    "                ('15_convbatch',    lnn.layer.Conv2dBatchReLU(256, 512, 3, 1, 1,dilation=2)),\n",
    "                ('16_convbatch',    lnn.layer.Conv2dBatchReLU(512, 256, 1, 1, 0,dilation=2)),\n",
    "                ('17_convbatch',    lnn.layer.Conv2dBatchReLU(256, 512, 3, 1, 1,dilation=2)),\n",
    "                ('18_max',          nn.MaxPool2d(1,1)),\n",
    "                ('19_convbatch',    lnn.layer.Conv2dBatchReLU(512, 1024, 3, 1, 1,dilation=4)),\n",
    "                ('20_convbatch',    lnn.layer.Conv2dBatchReLU(1024, 512, 1, 1, 0,dilation=4)),\n",
    "                ('21_convbatch',    lnn.layer.Conv2dBatchReLU(512, 1024, 3, 1, 1,dilation=4)),\n",
    "                ('22_convbatch',    lnn.layer.Conv2dBatchReLU(1024, 512, 1, 1, 0,dilation=4)),\n",
    "                ('23_convbatch',    lnn.layer.Conv2dBatchReLU(512, 1024, 3, 1, 1,dilation=4))\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LiteSeg-master/models/aspp.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Nov 19 18:02:59 2018\n",
    "\n",
    "@author: Taha Emara  @email: taha@emaraic.com\n",
    "\"\"\"\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "class SeparableConv2d(nn.Module):\n",
    "    def __init__(self,in_channels,out_channels,kernel_size=1,stride=1,padding=0,dilation=1,bias=False):\n",
    "        super(SeparableConv2d,self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels,in_channels,kernel_size,stride,padding,dilation,groups=in_channels,bias=bias)\n",
    "        self.pointwise = nn.Conv2d(in_channels,out_channels,1,1,0,1,1,bias=bias)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pointwise(x)\n",
    "        return x\n",
    "class ASPP(nn.Module):\n",
    "    \n",
    "    def __init__(self, inplanes, planes, rate):\n",
    "        super(ASPP, self).__init__()\n",
    "        self.rate=rate\n",
    "        if rate == 1:\n",
    "            kernel_size = 1\n",
    "            padding = 0\n",
    "        else:\n",
    "            kernel_size = 3\n",
    "            padding = rate\n",
    "            #self.conv1 = nn.Conv2d(planes, planes, kernel_size=3, bias=False,padding=1)\n",
    "            self.conv1 =SeparableConv2d(planes,planes,3,1,1)\n",
    "            self.bn1 = nn.BatchNorm2d(planes)\n",
    "            self.relu1 = nn.ReLU()\n",
    "   \n",
    "            #self.atrous_convolution = nn.Conv2d(inplanes, planes, kernel_size=kernel_size,\n",
    "            #                         stride=1, padding=padding, dilation=rate, bias=False)\n",
    "        self.atrous_convolution = SeparableConv2d(inplanes,planes,kernel_size,1,padding,rate)\n",
    "        self.bn = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        \n",
    "        self._init_weight()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.atrous_convolution(x)\n",
    "        x = self.bn(x)\n",
    "        #x = self.relu(x)\n",
    "        if self.rate!=1:\n",
    "            x=self.conv1(x)\n",
    "            x=self.bn1(x)\n",
    "            x=self.relu1(x)\n",
    "        return x\n",
    "\n",
    "    def _init_weight(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                torch.nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LiteSeg-master/models/separableconv.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sun Jan 13 11:05:01 2019\n",
    "\n",
    "@author: Taha Emara  @email: taha@emaraic.com\n",
    "\"\"\"\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class SeparableConv2d(nn.Module):\n",
    "    def __init__(self,in_channels,out_channels,kernel_size=1,stride=1,padding=0,dilation=1,bias=False):\n",
    "        super(SeparableConv2d,self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels,in_channels,kernel_size,stride,padding,dilation,groups=in_channels,bias=bias)\n",
    "        self.pointwise = nn.Conv2d(in_channels,out_channels,1,1,0,1,1,bias=bias)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pointwise(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LiteSeg-master/models/liteseg_darknet.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Nov 19 22:20:16 2018\n",
    "\n",
    "@author: Taha Emara  @email: taha@emaraic.com\n",
    "\"\"\"\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "#from models.backbone_networks import darknet\n",
    "#from models import aspp\n",
    "#from models.separableconv import SeparableConv2d \n",
    "\n",
    "# 클래스명 RT -> Darknet_RT로 수정\n",
    "# super(RT, -> super(Darknet_RT, 로 수정\n",
    "\n",
    "class Darknet_RT(nn.Module):\n",
    "    \n",
    "    def __init__(self,  n_classes=19, pretrained=True,PRETRAINED_WEIGHTS=\".\"):\n",
    "       \n",
    "        super(Darknet_RT, self).__init__()\n",
    "        print(\"LiteSeg-DarkNet...\")\n",
    "        if pretrained:\n",
    "            self.resnet_features=Darknet19(weights_file=PRETRAINED_WEIGHTS)\n",
    "            print('ooooooooooooooooo')\n",
    "        else:\n",
    "            self.resnet_features=Darknet19(weights_file=None)\n",
    "            print('xxxxxxxxxxxxxxxxx')\n",
    "        rates = [1, 3, 6, 9]\n",
    "\n",
    "        self.aspp1 = ASPP(1024, 96, rate=rates[0])\n",
    "        self.aspp2 = ASPP(1024, 96, rate=rates[1])\n",
    "        self.aspp3 = ASPP(1024, 96, rate=rates[2])\n",
    "        self.aspp4 = ASPP(1024, 96, rate=rates[3])\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.global_avg_pool = nn.Sequential(nn.AdaptiveAvgPool2d((1, 1)),\n",
    "                                             nn.Conv2d(1024, 96, 1, stride=1, bias=False),\n",
    "                                             nn.BatchNorm2d(96),\n",
    "                                             nn.ReLU())\n",
    "        \n",
    "        #self.conv1 = nn.Conv2d(1504, 96, 1, bias=False)#480 1504\n",
    "        self.conv1 =SeparableConv2d(1504,96,1)\n",
    "        self.bn1 = nn.BatchNorm2d(96)\n",
    "\n",
    "        # adopt [1x1, 48] for channel reduction.\n",
    "        #self.conv2 = nn.Conv2d(128, 32, 1, bias=False)#128 for no previous feature 7  ,64 ---3\n",
    "        self.conv2 = SeparableConv2d(128,32,1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "#    \n",
    "        self.last_conv = nn.Sequential(#nn.Conv2d(128, 96, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "                                       SeparableConv2d(128,96,3,1,1),\n",
    "                                       nn.BatchNorm2d(96),\n",
    "                                       nn.ReLU(),\n",
    "                                       #nn.Conv2d(96, 96, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "                                       SeparableConv2d(96,96,3,1,1),\n",
    "                                       nn.BatchNorm2d(96),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.Conv2d(96, n_classes, kernel_size=1, stride=1))\n",
    " \n",
    "    def forward(self, input):\n",
    "        x, low_level_features = self.resnet_features(input)\n",
    "        #print('x ',x.size(),' low features',low_level_features.size())\n",
    "        x1 = self.aspp1(x)\n",
    "        x2 = self.aspp2(x)\n",
    "        x3 = self.aspp3(x)\n",
    "        x4 = self.aspp4(x)\n",
    "        x5 = self.global_avg_pool(x)\n",
    "        x5 = F.interpolate(x5, size=x4.size()[2:], mode='bilinear', align_corners=True)\n",
    "        #print('x1',x1.size())\n",
    "        x = torch.cat((x,x1, x2, x3, x4, x5), dim=1)#x = torch.cat((x,x1, x2, x3, x4, x5), dim=1)\n",
    "\n",
    "        #ablation=torch.max(x, 1)[1]\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = F.interpolate(x, size=(int(math.ceil(input.size()[-2]/4)),##/4 --7  /2 ---3\n",
    "                                int(math.ceil(input.size()[-1]/4))), mode='bilinear', align_corners=True)\n",
    "        #ablation=torch.max(x, 1)[1]\n",
    "        #print('x',x.size())\n",
    "\n",
    "        ##comment to remove low feature\n",
    "        #print('low level features',low_level_features.size())\n",
    "        low_level_features = self.conv2(low_level_features)\n",
    "        low_level_features = self.bn2(low_level_features)\n",
    "        low_level_features = self.relu(low_level_features)\n",
    "        x = torch.cat((x, low_level_features), dim=1)\n",
    "\n",
    "        \n",
    "        #ablation=torch.max(x, 1)[1]\n",
    "        x = self.last_conv(x)\n",
    "        x = F.interpolate(x, size=input.size()[2:], mode='bilinear', align_corners=True)\n",
    "\n",
    "        return x #,ablation\n",
    "\n",
    "    def freeze_bn(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.BatchNorm2d):\n",
    "                m.eval()\n",
    "\n",
    "    def __init_weight(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                torch.nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## siaiffel/LiteSeg-master/models/bacbone_networks/MobileNetV2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Created on Mon Nov 19 21:47:01 2018\n",
    "\n",
    "@author: https://github.com/ericsun99/MobileNet-V2-Pytorch, edited by:Taha Emara  @email: taha@emaraic.com\n",
    "\"\"\"\n",
    "\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "def conv_bn(inp, oup, stride):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        nn.ReLU6(inplace=True)\n",
    "    )\n",
    "\n",
    "\n",
    "def conv_1x1_bn(inp, oup):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        nn.ReLU6(inplace=True)\n",
    "    )\n",
    "\n",
    "\n",
    "class InvertedResidual(nn.Module):\n",
    "    def __init__(self, inp, oup, stride, expand_ratio):\n",
    "        super(InvertedResidual, self).__init__()\n",
    "        self.stride = stride\n",
    "        assert stride in [1, 2]\n",
    "\n",
    "        hidden_dim = round(inp * expand_ratio)\n",
    "        self.use_res_connect = self.stride == 1 and inp == oup\n",
    "\n",
    "        if expand_ratio == 1:\n",
    "            self.conv = nn.Sequential(\n",
    "                # dw\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.ReLU6(inplace=True),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "        else:\n",
    "            self.conv = nn.Sequential(\n",
    "                # pw\n",
    "                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.ReLU6(inplace=True),\n",
    "                # dw\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.ReLU6(inplace=True),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_res_connect:\n",
    "            return x + self.conv(x)\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "\n",
    "class MobileNetV2(nn.Module):\n",
    "    def __init__(self, n_class=1000, input_size=224, width_mult=1.):\n",
    "        super(MobileNetV2, self).__init__()\n",
    "        block = InvertedResidual\n",
    "        input_channel = 32\n",
    "        last_channel = 1280\n",
    "        interverted_residual_setting = [\n",
    "            # t, c, n, s\n",
    "            [1, 16, 1, 1],\n",
    "            [6, 24, 2, 2],\n",
    "            [6, 32, 3, 2],\n",
    "            [6, 64, 4, 2],\n",
    "            [6, 96, 3, 1],\n",
    "            [6, 160, 3, 2],\n",
    "            [6, 320, 1, 1],\n",
    "        ]\n",
    "\n",
    "        # building first layer\n",
    "        assert input_size % 32 == 0\n",
    "        input_channel = int(input_channel * width_mult)\n",
    "        self.last_channel = int(last_channel * width_mult) if width_mult > 1.0 else last_channel\n",
    "        self.features = [conv_bn(3, input_channel, 2)]\n",
    "        # building inverted residual blocks\n",
    "        for t, c, n, s in interverted_residual_setting:\n",
    "            output_channel = int(c * width_mult)\n",
    "            for i in range(n):\n",
    "                if i == 0:\n",
    "                    self.features.append(block(input_channel, output_channel, s, expand_ratio=t))\n",
    "                else:\n",
    "                    self.features.append(block(input_channel, output_channel, 1, expand_ratio=t))\n",
    "                input_channel = output_channel\n",
    "        # building last several layers\n",
    "        self.features.append(conv_1x1_bn(input_channel, self.last_channel))\n",
    "        # make it nn.Sequential\n",
    "        self.features = nn.Sequential(*self.features)\n",
    "\n",
    "        # building classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(self.last_channel, n_class),\n",
    "        )\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        i=0\n",
    "        for i,l in enumerate( self.features[:4]):\n",
    "            #print(str(i),x.size())\n",
    "            x = l(x)\n",
    "        keep = x\n",
    "        for z,l in enumerate(self.features[4:]):\n",
    "            #print(str(z+i+1),x.size())\n",
    "            x = l(x)\n",
    "        return x,keep\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                n = m.weight.size(1)\n",
    "                m.weight.data.normal_(0, 0.01)\n",
    "                m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## siaiffel/LiteSeg-master/models/liteseg_mobilenet.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sun Dec 16 11:20:32 2018\n",
    "\n",
    "@author: Taha Emara  @email: taha@emaraic.com\n",
    "\"\"\"\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import math\n",
    "\n",
    "#from models.backbone_networks import MobileNetV2\n",
    "#from models import aspp\n",
    "#from models.separableconv import SeparableConv2d \n",
    "\n",
    "\n",
    "\n",
    "# 클래스명 RT -> MobileNet_RT로 수정\n",
    "# super(RT, -> super(MobileNet_RT, 로 수정\n",
    "class MobileNet_RT(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_classes=19,PRETRAINED_WEIGHTS=\".\", pretrained=True):\n",
    "        \n",
    "        super(MobileNet_RT, self).__init__()\n",
    "        print(\"LiteSeg-MobileNet...\")\n",
    "\n",
    "        self.mobile_features=MobileNetV2()\n",
    "        if pretrained:\n",
    "            state_dict = torch.load(PRETRAINED_WEIGHTS)\n",
    "            self.mobile_features.load_state_dict(state_dict)\n",
    "        \n",
    "        rates = [1, 3, 6, 9]\n",
    "\n",
    "\n",
    "        self.aspp1 = ASPP(1280, 96, rate=rates[0])\n",
    "        self.aspp2 = ASPP(1280, 96, rate=rates[1])\n",
    "        self.aspp3 = ASPP(1280, 96, rate=rates[2])\n",
    "        self.aspp4 = ASPP(1280, 96, rate=rates[3])\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.global_avg_pool = nn.Sequential(nn.AdaptiveAvgPool2d((1, 1)),\n",
    "                                             nn.Conv2d(1280, 96, 1, stride=1, bias=False),\n",
    "                                             nn.BatchNorm2d(96),\n",
    "                                             nn.ReLU())\n",
    "        #self.conv1 = nn.Conv2d(480+1280, 96, 1, bias=False)\n",
    "        self.conv1 =SeparableConv2d(480+1280,96,1)\n",
    "        self.bn1 = nn.BatchNorm2d(96)\n",
    "\n",
    "        #adopt [1x1, 48] for channel reduction.\n",
    "        #self.conv2 = nn.Conv2d(24, 32, 1, bias=False)\n",
    "        #self.bn2 = nn.BatchNorm2d(32)\n",
    "    \n",
    "        self.last_conv = nn.Sequential(#nn.Conv2d(24+96, 96, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "                                       SeparableConv2d(24+96,96,3,1,1),\n",
    "                                       nn.BatchNorm2d(96),\n",
    "                                       nn.ReLU(),\n",
    "                                       #nn.Conv2d(96, 96, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "                                       SeparableConv2d(96,96,3,1,1),\n",
    "                                       nn.BatchNorm2d(96),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.Conv2d(96, n_classes, kernel_size=1, stride=1))\n",
    "        \n",
    "    def forward(self, input):\n",
    "        x, low_level_features = self.mobile_features(input)\n",
    "        #print(x.size())\n",
    "        x1 = self.aspp1(x)\n",
    "        x2 = self.aspp2(x)\n",
    "        x3 = self.aspp3(x)\n",
    "        x4 = self.aspp4(x)\n",
    "        x5 = self.global_avg_pool(x)\n",
    "        x5 = F.interpolate(x5, size=x4.size()[2:], mode='bilinear', align_corners=True)\n",
    "\n",
    "        x = torch.cat((x,x1, x2, x3, x4, x5), dim=1)\n",
    "        #print('after aspp cat',x.size())\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = F.interpolate(x, size=(int(math.ceil(input.size()[-2]/4)),\n",
    "                                int(math.ceil(input.size()[-1]/4))), mode='bilinear', align_corners=True)\n",
    "       # ablation=torch.max(low_level_features, 1)[1]\n",
    "        #print('after con on aspp output',x.size())\n",
    "\n",
    "        ##comment to remove low feature\n",
    "        #low_level_features = self.conv2(low_level_features)\n",
    "        #low_level_features = self.bn2(low_level_features)\n",
    "        #low_level_features = self.relu(low_level_features)\n",
    "        #print(\"low\",low_level_features.size())\n",
    "        \n",
    "        x = torch.cat((x, low_level_features), dim=1)\n",
    "        #print('after cat low feature with output of aspp',x.size())\n",
    "\n",
    "        x = self.last_conv(x)\n",
    "        x = F.interpolate(x, size=input.size()[2:], mode='bilinear', align_corners=True)\n",
    "\n",
    "        return x#,ablation\n",
    "\n",
    "    def freeze_bn(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.BatchNorm2d):\n",
    "                m.eval()\n",
    "\n",
    "    def __init_weight(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                torch.nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## siaiffel/LiteSeg-master/models/liteseg.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sun Jan 13 12:00:52 2019\n",
    "\n",
    "@author: Taha Emara  @email: taha@emaraic.com\n",
    "\"\"\"\n",
    "import torch \n",
    "\n",
    "#from models import liteseg_shufflenet as shufflenet\n",
    "#from models import liteseg_darknet as darknet\n",
    "#from models import liteseg_mobilenet as mobilenet\n",
    "\n",
    "\n",
    "\n",
    "class LiteSeg():\n",
    "    \n",
    "    #수정 사항 : Darknet_RT, MobileNet_RT \n",
    "    def build(backbone_network,modelpath,CONFIG,is_train=True):\n",
    "                \n",
    "        if backbone_network.lower() == 'darknet':\n",
    "            net = Darknet_RT(n_classes=19, pretrained=is_train,PRETRAINED_WEIGHTS=CONFIG.PRETRAINED_DarkNET19)\n",
    "        elif backbone_network.lower() == 'shufflenet':\n",
    "            net = shufflenet.RT(n_classes=19, pretrained=is_train, PRETRAINED_WEIGHTS=CONFIG.PRETRAINED_SHUFFLENET)\n",
    "        elif backbone_network.lower() == 'mobilenet':\n",
    "            net = MobileNet_RT(n_classes=19,pretrained=is_train, PRETRAINED_WEIGHTS=CONFIG.PRETRAINED_MOBILENET)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "            \n",
    "        if modelpath is not None:\n",
    "            net.load_state_dict(torch.load(modelpath))\n",
    "            \n",
    "        print(\"Using LiteSeg with\",backbone_network)\n",
    "        return net\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## siaiffel/LiteSeg-master/utils/loss.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Oct 31 17:52:22 2018\n",
    "\n",
    "@author: Taha Emara  @email: taha@emaraic.com\n",
    "\"\"\"\n",
    "import torch.nn as nn\n",
    "\n",
    "def cross_entropy2d(logit, target, ignore_index=255, weight=None, reduct='elementwise_mean'):\n",
    "    n, c, h, w = logit.size()\n",
    "    #print(\"Inside Cross entropy \",logit.size()  ,\"Target Size\",target.size())\n",
    "    # logit = logit.permute(0, 2, 3, 1)\n",
    "    target = target.squeeze(1)\n",
    "    if weight is None:\n",
    "        criterion = nn.CrossEntropyLoss(ignore_index=ignore_index,reduction=reduct)\n",
    "    else:\n",
    "        criterion = nn.CrossEntropyLoss(weight=weight, ignore_index=ignore_index,reduction=reduct)\n",
    "    loss = criterion(logit, target.long())\n",
    "\n",
    "    batch_average=True\n",
    "    if batch_average:\n",
    "        loss /= n\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## siaiffel/LiteSeg-master/utils/iou_eval.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Oct 31 18:28:07 2018\n",
    "\n",
    "@author: Taha Emara  @email: taha@emaraic.com\n",
    "\"\"\"\n",
    "\n",
    "import torch \n",
    "\n",
    "class Eval:\n",
    "\n",
    "    def __init__(self, nClasses, ignoreIndex=19):\n",
    "        self.nClasses = nClasses\n",
    "        self.ignoreIndex = ignoreIndex if nClasses>ignoreIndex else -1 #if ignoreIndex is larger than nClasses, consider no ignoreIndex\n",
    "        self.reset()\n",
    "\n",
    "    def reset (self):\n",
    "        classes = self.nClasses if self.ignoreIndex==-1 else self.nClasses-1\n",
    "        self.tp = torch.zeros(classes).double()\n",
    "        self.fp = torch.zeros(classes).double()\n",
    "        self.fn = torch.zeros(classes).double()        \n",
    "\n",
    "    def addBatch(self, x, y):   #x=preds, y=targets\n",
    "        #sizes should be \"batch_size x nClasses x H x W\"\n",
    "        \n",
    "        #print (\"X is cuda: \", x.is_cuda)\n",
    "        #print (\"Y is cuda: \", y.is_cuda)\n",
    "        x=x.type( torch.LongTensor)\n",
    "        y=y.type( torch.LongTensor)\n",
    "        if (x.is_cuda or y.is_cuda):\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "\n",
    "        #if size is \"batch_size x 1 x H x W\" scatter to onehot\n",
    "        if (x.size(1) == 1):\n",
    "            x_onehot = torch.zeros(x.size(0), self.nClasses, x.size(2), x.size(3))  \n",
    "            if x.is_cuda:\n",
    "                x_onehot = x_onehot.cuda()\n",
    "            x_onehot.scatter_(1, x, 1).float()\n",
    "        else:\n",
    "            x_onehot = x.float()\n",
    "\n",
    "        if (y.size(1) == 1):\n",
    "            y_onehot = torch.zeros(y.size(0), self.nClasses, y.size(2), y.size(3))\n",
    "            if y.is_cuda:\n",
    "                y_onehot = y_onehot.cuda()\n",
    "            y_onehot.scatter_(1, y, 1).float()\n",
    "        else:\n",
    "            y_onehot = y.float()\n",
    "\n",
    "        if (self.ignoreIndex != -1): \n",
    "            ignores = y_onehot[:,self.ignoreIndex].unsqueeze(1)\n",
    "            x_onehot = x_onehot[:, :self.ignoreIndex]\n",
    "            y_onehot = y_onehot[:, :self.ignoreIndex]\n",
    "        else:\n",
    "            ignores=0\n",
    "\n",
    "        #print(type(x_onehot))\n",
    "        #print(type(y_onehot))\n",
    "        #print(x_onehot.size())\n",
    "        #print(y_onehot.size())\n",
    "\n",
    "        tpmult = x_onehot * y_onehot    #times prediction and gt coincide is 1\n",
    "        tp = torch.sum(torch.sum(torch.sum(tpmult, dim=0, keepdim=True), dim=2, keepdim=True), dim=3, keepdim=True).squeeze()\n",
    "        fpmult = x_onehot * (1-y_onehot-ignores) #times prediction says its that class and gt says its not (subtracting cases when its ignore label!)\n",
    "        fp = torch.sum(torch.sum(torch.sum(fpmult, dim=0, keepdim=True), dim=2, keepdim=True), dim=3, keepdim=True).squeeze()\n",
    "        fnmult = (1-x_onehot) * (y_onehot) #times prediction says its not that class and gt says it is\n",
    "        fn = torch.sum(torch.sum(torch.sum(fnmult, dim=0, keepdim=True), dim=2, keepdim=True), dim=3, keepdim=True).squeeze() \n",
    "\n",
    "        self.tp += tp.double().cpu()\n",
    "        self.fp += fp.double().cpu()\n",
    "        self.fn += fn.double().cpu()\n",
    "\n",
    "    def getIoU(self):\n",
    "        num = self.tp\n",
    "        den = self.tp + self.fp + self.fn + 1e-15\n",
    "        iou = num / den\n",
    "        return torch.mean(iou), iou #returns \"iou m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train.py\n",
    "- !pip install yaml\n",
    "- !pip install addict\n",
    "- !pip install argparse\n",
    "- /home/aiffel-dj17/aiffel/siaiffel/LiteSeg-master/train.py\n",
    "- python train_ms.py --backbone_network darknet --model_path_coarse ./pretrained_models/liteseg-darknet-cityscapes.pth\n",
    "- weights 저장 경로 : /home/aiffel-dj17/aiffel/siaiffel/LiteSeg-master/handmade_weights\n",
    "- config 경로 : /home/aiffel-dj17/aiffel/siaiffel/LiteSeg-master/config/training.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Darknet\n",
    "- path 주의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport socket\\nimport timeit\\nfrom datetime import datetime\\nimport os\\nimport glob\\nfrom collections import OrderedDict\\nimport numpy as np\\nimport yaml\\nfrom addict import Dict\\nimport argparse\\n\\n# PyTorch includes\\nimport torch\\nfrom torch.autograd import Variable\\nimport torch.optim as optim\\nfrom torchvision import transforms\\nfrom torch.utils.data import DataLoader\\nfrom torchvision.utils import make_grid\\n\\n# Tensorboard include\\nfrom tensorboardX import SummaryWriter\\n\\n# Custom includes\\n#from dataloaders import cityscapes\\n#from dataloaders import utils\\n#from dataloaders import augmentation as augment\\n#from models.liteseg import LiteSeg\\n#from utils import loss as losses\\n#from utils import iou_eval\\n\\n\\n#To make reproducible results  \\ntorch.manual_seed(125)\\ntorch.backends.cudnn.deterministic = True\\ntorch.backends.cudnn.benchmark = False\\nnp.random.seed(125)\\n\\n#경로 변경 주의\\nCONFIG=Dict(yaml.load(open(\\'/home/aiffel-dj17/aiffel/siaiffel/LiteSeg-master/config/training.yaml\\')))\\n\\n#ap, args 함수 : shell 창에서 어떤 backbone 을 쓸지, model path, pretrained_mode\\n#ap = argparse.ArgumentParser()\\n#ap.add_argument(\\'--backbone_network\\', required=True,\\n                #help = \\'name of backbone network\\',default=\\'darknet\\')#shufflenet, mobilenet, and darknet\\n#ap.add_argument(\\'--model_path_coarse\\', required=False,\\n                #help = \\'path to pretrained model on coarse data\\',default=\\'pretrained_models/liteseg-darknet-cityscapes.pth\\')\\n#ap.add_argument(\\'--model_path_resume\\', required=False,\\n                #help = \\'path to a model to resume from\\',default=\\'pretrained_models/liteseg-darknet-cityscapes.pth\\')\\n\\n#args = ap.parse_args()\\n#backbone_network= \\'mobilenet\\'\\nbackbone_network= \\'darknet\\' # backbone_network 뭘로 할지 str로 입력\\n#model_path_resume=args.model_path_resume\\n#경로 변경 주의\\nmodel_path_coarse =\\'/home/aiffel-dj17/aiffel/siaiffel/LiteSeg-master/pretrained_models/liteseg-darknet-cityscapes.pth\\'\\n#model_path_coarse = \\'/home/aiffel-dj17/aiffel/siaiffel/LiteSeg-master/pretrained_models/liteseg-mobilenet-cityscapes.pth\\'\\nmodel_path_resume = \\'/home/aiffel-dj17/aiffel/siaiffel/LiteSeg-master/pretrained_models/liteseg-darknet-cityscapes.pth\\'\\n\\n\\n# Setting parameters\\nnEpochs =10  # Number of epochs for training 150 종료epoch\\nresume_epoch = 0  # Default is 0, change if want to resume 0 시작epoch\\n\\np = OrderedDict()  # Parameters to include in report\\np[\\'trainBatch\\'] =4  # Training batch size\\np[\\'lr\\'] =1e-7 # Learning rate  1e-8 for darknet and 1e-7 shufflenet and mobilenet\\np[\\'wd\\'] = 5e-4  # Weight decay\\np[\\'momentum\\'] = 0.9  # Momentum\\np[\\'epoch_size\\'] =5  # epochs to change learning rate\\n\\ntestBatch = 1  # Testing batch size\\nnValInterval = 2  # Run on test set every nTestInterval epochs\\nsnapshot = 2  # Store a model every snapshot epochs\\n\\n# 저장 경로 설정\\nsave_dir_root = os.getenv(\\'HOME\\')+\\'/aiffel/siaiffel/LiteSeg-master/handmade_weights\\'\\n\\ndataset_path=CONFIG.DATASET_FINE\\nif CONFIG.USING_COARSE:\\n    print(\"Training on Coarse Data\")\\n    dataset_path=CONFIG.DATASET_COARSE\\n    p[\\'epoch_size\\'] =10 #we increase the number of epochs to change LR as we train on one scale\\n\\n\\n#txt파일 이름 설정\\n# exp_name = os.path.dirname(os.path.abspath(__file__)).split(\\'/\\')[-1]\\nexp_name = \\'sangmin_0520\\'\\n\\nclass_weight = np.array([0.05570516, 0.32337477, 0.08998544, 1.03602707, 1.03413147, 1.68195437,\\n                                 5.58540548, 3.56563995, 0.12704978, 1.,         0.46783719, 1.34551528,\\n                                 5.29974114, 0.28342531, 0.9396095,  0.81551811, 0.42679146, 3.6399074,\\n                                 2.78376194], dtype=float)\\nclass_weight = torch.from_numpy(class_weight).float().cuda()\\n\\n\\n\\n#make a folder -with name of current time- for every experiment\\nexperiment_id=datetime.now().strftime(\"%Y-%m-%d_%H_%M\")\\nsave_path = os.path.join(save_dir_root, \\'experiments\\', \\'experiment_\\' + str(experiment_id))\\nprint(save_path)\\n\\n\\n# Network definition\\n# LiteSeg.build 괄호 첫번째에 backbone_network 뭘로 할지 str로 입력\\nmodelpath = model_path_coarse\\nnet=LiteSeg.build(backbone_network, modelpath ,CONFIG, is_train=True)\\n\\nif CONFIG.USING_GPU:\\n    torch.cuda.set_device(device=CONFIG.GPU_ID)\\n    net.cuda()\\n\\n\\n#using the trained model on the coarse data\\n#If you want to train model on fine data directley, comment the next 3 lines.\\nif not CONFIG.USING_COARSE:\\n    print(\"Using a weights from training coarse data from: {}...\".format(model_path_coarse))\\n    net.load_state_dict(torch.load(model_path_coarse)) \\n\\n\\n#resume tarining from a given model, \\n#Attention! the learnig rate which used for resuming training, is not the intial one.\\nif resume_epoch == 0:\\n    print(\"Training Network...\")\\nelse:\\n    print(\"Resume training from a model at: {}...\".format(model_path_resume))\\n    net.load_state_dict(torch.load(model_path_resume))\\n\\n    \\nmodelName = \\'LiteSeg-\\' + backbone_network + \\'-cityscapes\\'\\nprint(modelName)\\n\\ncriterion = cross_entropy2d\\n\\n# 여기서부터 시작\\nif resume_epoch != nEpochs+1:\\n    # Logging into Tensorboard / path 설정\\n    log_dir = os.path.join(save_path, \\'models\\', datetime.now().strftime(\\'%b%d_%H-%M-%S\\') + \\'_\\' + socket.gethostname())\\n    writer = SummaryWriter(log_dir=log_dir) #summary 설정\\n\\n    # Use the following optimizer 옵티마이저 설정\\n    optimizer = optim.SGD(net.parameters(), lr=p[\\'lr\\'], momentum=p[\\'momentum\\'], weight_decay=p[\\'wd\\'])\\n    #optimizer = optim.Adam(net.parameters(), 5e-4, (0.9, 0.999), eps=1e-08, weight_decay=1e-4) \\n    p[\\'optimizer\\'] = str(optimizer) \\n\\n    #augment\\n    #augment. 삭제\\n    composed_transforms_tr = transforms.Compose([\\n        RandomHorizontalFlip(),\\n        RandomScale((0.2, .8)),\\n        RandomCrop(( 512,1024)),\\n        RandomRotate(5),\\n        Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\\n        ToTensor()])\\n   \\n    composed_transforms_tr1 = transforms.Compose([\\n        RandomHorizontalFlip(),\\n        RandomScale((0.2, .8)),\\n        RandomCrop(( 768,1536)),\\n        Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\\n        ToTensor()])\\n    composed_transforms_tr2 = transforms.Compose([\\n        RandomHorizontalFlip(),\\n        RandomScale((0.2, .8)),\\n        RandomCrop(( 360,640)),\\n        Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\\n        ToTensor()])\\n    composed_transforms_tr3 = transforms.Compose([\\n        RandomHorizontalFlip(),\\n        RandomScale((0.2, .8)),\\n        RandomCrop(( 720,1280)),\\n        Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\\n        ToTensor()])\\n    \\n    composed_transforms_ts = transforms.Compose([\\n        RandomHorizontalFlip(),\\n        #augment.Scale((819, 1638)),\\n        CenterCrop(( 512,1024)),\\n        Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),#augment. Normalize_cityscapes(mean=(72.39, 82.91, 73.16)),\\n        ToTensor()])\\n#augment train, val data load\\n#cityscapes. 삭제\\n    cityscapes_train = Cityscapes(root=dataset_path,extra=CONFIG.USING_COARSE,split=\\'train\\',transform=composed_transforms_tr)\\n    cityscapes_train1 = Cityscapes(root=dataset_path,extra=CONFIG.USING_COARSE,split=\\'train\\',transform=composed_transforms_tr1)\\n    cityscapes_train2 = Cityscapes(root=dataset_path,extra=CONFIG.USING_COARSE,split=\\'train\\',transform=composed_transforms_tr2)\\n    cityscapes_train3 = Cityscapes(root=dataset_path,extra=CONFIG.USING_COARSE,split=\\'train\\',transform=composed_transforms_tr3)\\n    \\n    cityscapes_val = Cityscapes(root=dataset_path,extra=CONFIG.USING_COARSE,split=\\'val\\', transform=composed_transforms_ts)\\n    \\n#augment train, val data load , train batch ==4\\n    trainloader = DataLoader(cityscapes_train, batch_size=p[\\'trainBatch\\'], shuffle=True, num_workers=0)\\n    trainloader1 = DataLoader(cityscapes_train1, batch_size=p[\\'trainBatch\\'], shuffle=True, num_workers=0)\\n    trainloader2 = DataLoader(cityscapes_train2, batch_size=p[\\'trainBatch\\'], shuffle=True, num_workers=0)\\n    trainloader3 = DataLoader(cityscapes_train3, batch_size=p[\\'trainBatch\\'], shuffle=True, num_workers=0)\\n\\n\\n    valloader = DataLoader(cityscapes_val, batch_size=testBatch, shuffle=True, num_workers=0)\\n    \\n    if CONFIG.USING_COARSE:#in case of training coarse data, I just used one scale to train. 상황에 따라 일반적인 tarin data를 쓸지, augment한 train data를 쓸지 정함 \\n        loaders=[ trainloader ]\\n    else:\\n        loaders=[ trainloader ,trainloader1 ,trainloader2 ,trainloader3]\\n    #generate 앞에 utils. 삭제\\n    generate_param_report(os.path.join(save_path, exp_name + \\'.txt\\'), p)\\n\\n    num_img_tr = len(trainloader)\\n    num_img_vl = len(valloader)\\n    running_loss_tr = 0.0\\n    running_loss_vl = 0.0\\n    previous_miou = -1.0\\n    global_step = 0\\n    iev = Eval(20,19)\\n    \\n    # Main Training and Testing Loop 메인 학습 0부터 100까지\\n    for epoch in range(resume_epoch, nEpochs):\\n        start_time = timeit.default_timer()\\n\\n        if epoch % p[\\'epoch_size\\'] == p[\\'epoch_size\\'] - 1:\\n            lr_ = lr_poly(p[\\'lr\\'], epoch, nEpochs, 0.9)\\n            print(\\'(poly lr policy) learning rate: \\', lr_)\\n            optimizer = optim.SGD(net.parameters(), lr=lr_, momentum=p[\\'momentum\\'], weight_decay=p[\\'wd\\'])\\n           \\n        net.train()\\n        for loader in loaders:\\n            print(loader)\\n            for ii, sample_batched in enumerate(loader):\\n                \\n    \\n                inputs, labels = sample_batched[\\'image\\'], sample_batched[\\'label\\']\\n                # Forward-Backward of the mini-batch\\n                inputs, labels = Variable(inputs, requires_grad=True), Variable(labels)\\n                #print(\\'labels size\\', inputs.size() , labels.size())\\n                global_step += inputs.data.shape[0]\\n                #print(\"Glopal Step\",global_step)4,8,12,16\\n                if CONFIG.USING_GPU:\\n                    inputs, labels = inputs.cuda(), labels.cuda()\\n                \\n                optimizer.zero_grad()\\n                outputs = net.forward(inputs)\\n                loss = criterion(outputs, labels,reduct=\\'sum\\',weight=None)#sum\\n                loss.backward()\\n                optimizer.step()\\n                ls=loss.item()\\n                running_loss_tr += ls\\n    #            if ii% 10 == 0:\\n    #                print(ls)\\n                # Print stuff\\n                if ii % num_img_tr == (num_img_tr - 1):\\n                    running_loss_tr = running_loss_tr / num_img_tr\\n                    writer.add_scalar(\\'data/total_loss_epoch\\', running_loss_tr, epoch)\\n                    print(\\'[Epoch: %d, numImages: %5d]\\' % (epoch, ii * p[\\'trainBatch\\'] + inputs.data.shape[0]))\\n                    print(\\'Loss: %f\\' % running_loss_tr)\\n                    running_loss_tr = 0\\n                    stop_time = timeit.default_timer()\\n                    print(\"Execution time: \" + str(stop_time - start_time) + \"\\n\")\\n       \\n                \\n                # Update the weights once in p[\\'nAveGrad\\'] forward passes \\n                writer.add_scalar(\\'data/total_loss_iter\\', loss.item(), ii + num_img_tr * epoch)\\n                \\n    \\n                # Show 10 * 3 images results each epoch\\n                if ii % (num_img_tr // 10) == 0:\\n                    grid_image = make_grid(inputs[:3].clone().cpu().data, 3, normalize=True)\\n                    writer.add_image(\\'Image\\', grid_image, global_step)\\n                    grid_image = make_grid(\\n                        decode_seg_map_sequence(torch.max(outputs[:3], 1)[1].detach().cpu().numpy(), \\'cityscapes\\'), 3,\\n                        normalize=False,\\n                        range=(0, 255))\\n                    writer.add_image(\\'Predicted label\\', grid_image, global_step)\\n                    grid_image = make_grid(\\n                        decode_seg_map_sequence(torch.squeeze(labels[:3], 1).detach().cpu().numpy(), \\'cityscapes\\'), 3,\\n                        normalize=False, range=(0, 255))\\n                    writer.add_image(\\'Groundtruth label\\', grid_image, global_step)\\n\\n        # One testing epoch\\n        if (epoch % nValInterval == (nValInterval - 1)) or epoch==0:\\n            total_miou = 0.0\\n            net.eval()\\n            for ii, sample_batched in enumerate(valloader):\\n                inputs, labels = sample_batched[\\'image\\'], sample_batched[\\'label\\']\\n\\n                # Forward pass of the mini-batch\\n                inputs, labels = Variable(inputs, requires_grad=True), Variable(labels)\\n                if CONFIG.USING_GPU:\\n                    inputs, labels = inputs.cuda(), labels.cuda()\\n\\n                with torch.no_grad():\\n                    outputs = net.forward(inputs)\\n\\n                predictions = torch.max(outputs, 1)[1]\\n\\n                loss = criterion(outputs, labels,reduct=\\'sum\\',weight=None)#sum elementwise_mean\\n                running_loss_vl += loss.item()\\n               \\n                \\n                y = torch.ones(labels.size()[2], labels.size()[3]).mul(19).cuda()\\n                labels=labels.where(labels !=255, y)\\n                \\n                iev.addBatch(predictions.unsqueeze(1).data,labels)\\n                \\n                \\n                # Print stuff\\n                if ii % num_img_vl == num_img_vl - 1:\\n                    miou=iev.getIoU()[0]\\n                    running_loss_vl = running_loss_vl / num_img_vl\\n                    print(\\'Validation:\\')\\n                    print(\\'[Epoch: %d, numImages: %5d]\\' % (epoch, ii * testBatch + inputs.data.shape[0]))\\n                    writer.add_scalar(\\'data/test_loss_epoch\\', running_loss_vl, epoch)\\n                    writer.add_scalar(\\'data/test_miour\\', iev.getIoU()[0], epoch)\\n                    print(\\'Loss: %f\\' % running_loss_vl)\\n                    print(\"Predi iou\",iev.getIoU())\\n                    running_loss_vl = 0\\n                    iev.reset()\\n\\n        # Save the model\\n        if (epoch % snapshot) == snapshot - 1 :#and miou > previous_miou\\n            previous_miou = miou\\n            torch.save(net.state_dict(), os.path.join(save_path, \\'models\\', modelName + \\'_epoch-\\' + str(epoch) + \\'.pth\\'))\\n            print(\"Save model at {}\\n\".format(\\n                os.path.join(save_path, \\'models\\', modelName + \\'_epoch-\\' + str(epoch) + \\'.pth\\')))\\n\\n    writer.close()\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import socket\n",
    "import timeit\n",
    "from datetime import datetime\n",
    "import os\n",
    "import glob\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import yaml\n",
    "from addict import Dict\n",
    "import argparse\n",
    "\n",
    "# PyTorch includes\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "# Tensorboard include\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "# Custom includes\n",
    "#from dataloaders import cityscapes\n",
    "#from dataloaders import utils\n",
    "#from dataloaders import augmentation as augment\n",
    "#from models.liteseg import LiteSeg\n",
    "#from utils import loss as losses\n",
    "#from utils import iou_eval\n",
    "\n",
    "\n",
    "#To make reproducible results  \n",
    "torch.manual_seed(125)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(125)\n",
    "\n",
    "#경로 변경 주의\n",
    "CONFIG=Dict(yaml.load(open('/home/aiffel-dj17/aiffel/siaiffel/LiteSeg-master/config/training.yaml')))\n",
    "\n",
    "#ap, args 함수 : shell 창에서 어떤 backbone 을 쓸지, model path, pretrained_mode\n",
    "#ap = argparse.ArgumentParser()\n",
    "#ap.add_argument('--backbone_network', required=True,\n",
    "                #help = 'name of backbone network',default='darknet')#shufflenet, mobilenet, and darknet\n",
    "#ap.add_argument('--model_path_coarse', required=False,\n",
    "                #help = 'path to pretrained model on coarse data',default='pretrained_models/liteseg-darknet-cityscapes.pth')\n",
    "#ap.add_argument('--model_path_resume', required=False,\n",
    "                #help = 'path to a model to resume from',default='pretrained_models/liteseg-darknet-cityscapes.pth')\n",
    "\n",
    "#args = ap.parse_args()\n",
    "#backbone_network= 'mobilenet'\n",
    "backbone_network= 'darknet' # backbone_network 뭘로 할지 str로 입력\n",
    "#model_path_resume=args.model_path_resume\n",
    "#경로 변경 주의\n",
    "model_path_coarse ='/home/aiffel-dj17/aiffel/siaiffel/LiteSeg-master/pretrained_models/liteseg-darknet-cityscapes.pth'\n",
    "#model_path_coarse = '/home/aiffel-dj17/aiffel/siaiffel/LiteSeg-master/pretrained_models/liteseg-mobilenet-cityscapes.pth'\n",
    "model_path_resume = '/home/aiffel-dj17/aiffel/siaiffel/LiteSeg-master/pretrained_models/liteseg-darknet-cityscapes.pth'\n",
    "\n",
    "\n",
    "# Setting parameters\n",
    "nEpochs =10  # Number of epochs for training 150 종료epoch\n",
    "resume_epoch = 0  # Default is 0, change if want to resume 0 시작epoch\n",
    "\n",
    "p = OrderedDict()  # Parameters to include in report\n",
    "p['trainBatch'] =4  # Training batch size\n",
    "p['lr'] =1e-7 # Learning rate  1e-8 for darknet and 1e-7 shufflenet and mobilenet\n",
    "p['wd'] = 5e-4  # Weight decay\n",
    "p['momentum'] = 0.9  # Momentum\n",
    "p['epoch_size'] =5  # epochs to change learning rate\n",
    "\n",
    "testBatch = 1  # Testing batch size\n",
    "nValInterval = 2  # Run on test set every nTestInterval epochs\n",
    "snapshot = 2  # Store a model every snapshot epochs\n",
    "\n",
    "# 저장 경로 설정\n",
    "save_dir_root = os.getenv('HOME')+'/aiffel/siaiffel/LiteSeg-master/handmade_weights'\n",
    "\n",
    "dataset_path=CONFIG.DATASET_FINE\n",
    "if CONFIG.USING_COARSE:\n",
    "    print(\"Training on Coarse Data\")\n",
    "    dataset_path=CONFIG.DATASET_COARSE\n",
    "    p['epoch_size'] =10 #we increase the number of epochs to change LR as we train on one scale\n",
    "\n",
    "\n",
    "#txt파일 이름 설정\n",
    "# exp_name = os.path.dirname(os.path.abspath(__file__)).split('/')[-1]\n",
    "exp_name = 'sangmin_0520'\n",
    "\n",
    "class_weight = np.array([0.05570516, 0.32337477, 0.08998544, 1.03602707, 1.03413147, 1.68195437,\n",
    "                                 5.58540548, 3.56563995, 0.12704978, 1.,         0.46783719, 1.34551528,\n",
    "                                 5.29974114, 0.28342531, 0.9396095,  0.81551811, 0.42679146, 3.6399074,\n",
    "                                 2.78376194], dtype=float)\n",
    "class_weight = torch.from_numpy(class_weight).float().cuda()\n",
    "\n",
    "\n",
    "\n",
    "#make a folder -with name of current time- for every experiment\n",
    "experiment_id=datetime.now().strftime(\"%Y-%m-%d_%H_%M\")\n",
    "save_path = os.path.join(save_dir_root, 'experiments', 'experiment_' + str(experiment_id))\n",
    "print(save_path)\n",
    "\n",
    "\n",
    "# Network definition\n",
    "# LiteSeg.build 괄호 첫번째에 backbone_network 뭘로 할지 str로 입력\n",
    "modelpath = model_path_coarse\n",
    "net=LiteSeg.build(backbone_network, modelpath ,CONFIG, is_train=True)\n",
    "\n",
    "if CONFIG.USING_GPU:\n",
    "    torch.cuda.set_device(device=CONFIG.GPU_ID)\n",
    "    net.cuda()\n",
    "\n",
    "\n",
    "#using the trained model on the coarse data\n",
    "#If you want to train model on fine data directley, comment the next 3 lines.\n",
    "if not CONFIG.USING_COARSE:\n",
    "    print(\"Using a weights from training coarse data from: {}...\".format(model_path_coarse))\n",
    "    net.load_state_dict(torch.load(model_path_coarse)) \n",
    "\n",
    "\n",
    "#resume tarining from a given model, \n",
    "#Attention! the learnig rate which used for resuming training, is not the intial one.\n",
    "if resume_epoch == 0:\n",
    "    print(\"Training Network...\")\n",
    "else:\n",
    "    print(\"Resume training from a model at: {}...\".format(model_path_resume))\n",
    "    net.load_state_dict(torch.load(model_path_resume))\n",
    "\n",
    "    \n",
    "modelName = 'LiteSeg-' + backbone_network + '-cityscapes'\n",
    "print(modelName)\n",
    "\n",
    "criterion = cross_entropy2d\n",
    "\n",
    "# 여기서부터 시작\n",
    "if resume_epoch != nEpochs+1:\n",
    "    # Logging into Tensorboard / path 설정\n",
    "    log_dir = os.path.join(save_path, 'models', datetime.now().strftime('%b%d_%H-%M-%S') + '_' + socket.gethostname())\n",
    "    writer = SummaryWriter(log_dir=log_dir) #summary 설정\n",
    "\n",
    "    # Use the following optimizer 옵티마이저 설정\n",
    "    optimizer = optim.SGD(net.parameters(), lr=p['lr'], momentum=p['momentum'], weight_decay=p['wd'])\n",
    "    #optimizer = optim.Adam(net.parameters(), 5e-4, (0.9, 0.999), eps=1e-08, weight_decay=1e-4) \n",
    "    p['optimizer'] = str(optimizer) \n",
    "\n",
    "    #augment\n",
    "    #augment. 삭제\n",
    "    composed_transforms_tr = transforms.Compose([\n",
    "        RandomHorizontalFlip(),\n",
    "        RandomScale((0.2, .8)),\n",
    "        RandomCrop(( 512,1024)),\n",
    "        RandomRotate(5),\n",
    "        Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensor()])\n",
    "   \n",
    "    composed_transforms_tr1 = transforms.Compose([\n",
    "        RandomHorizontalFlip(),\n",
    "        RandomScale((0.2, .8)),\n",
    "        RandomCrop(( 768,1536)),\n",
    "        Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensor()])\n",
    "    composed_transforms_tr2 = transforms.Compose([\n",
    "        RandomHorizontalFlip(),\n",
    "        RandomScale((0.2, .8)),\n",
    "        RandomCrop(( 360,640)),\n",
    "        Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensor()])\n",
    "    composed_transforms_tr3 = transforms.Compose([\n",
    "        RandomHorizontalFlip(),\n",
    "        RandomScale((0.2, .8)),\n",
    "        RandomCrop(( 720,1280)),\n",
    "        Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensor()])\n",
    "    \n",
    "    composed_transforms_ts = transforms.Compose([\n",
    "        RandomHorizontalFlip(),\n",
    "        #augment.Scale((819, 1638)),\n",
    "        CenterCrop(( 512,1024)),\n",
    "        Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),#augment. Normalize_cityscapes(mean=(72.39, 82.91, 73.16)),\n",
    "        ToTensor()])\n",
    "#augment train, val data load\n",
    "#cityscapes. 삭제\n",
    "    cityscapes_train = Cityscapes(root=dataset_path,extra=CONFIG.USING_COARSE,split='train',transform=composed_transforms_tr)\n",
    "    cityscapes_train1 = Cityscapes(root=dataset_path,extra=CONFIG.USING_COARSE,split='train',transform=composed_transforms_tr1)\n",
    "    cityscapes_train2 = Cityscapes(root=dataset_path,extra=CONFIG.USING_COARSE,split='train',transform=composed_transforms_tr2)\n",
    "    cityscapes_train3 = Cityscapes(root=dataset_path,extra=CONFIG.USING_COARSE,split='train',transform=composed_transforms_tr3)\n",
    "    \n",
    "    cityscapes_val = Cityscapes(root=dataset_path,extra=CONFIG.USING_COARSE,split='val', transform=composed_transforms_ts)\n",
    "    \n",
    "#augment train, val data load , train batch ==4\n",
    "    trainloader = DataLoader(cityscapes_train, batch_size=p['trainBatch'], shuffle=True, num_workers=0)\n",
    "    trainloader1 = DataLoader(cityscapes_train1, batch_size=p['trainBatch'], shuffle=True, num_workers=0)\n",
    "    trainloader2 = DataLoader(cityscapes_train2, batch_size=p['trainBatch'], shuffle=True, num_workers=0)\n",
    "    trainloader3 = DataLoader(cityscapes_train3, batch_size=p['trainBatch'], shuffle=True, num_workers=0)\n",
    "\n",
    "\n",
    "    valloader = DataLoader(cityscapes_val, batch_size=testBatch, shuffle=True, num_workers=0)\n",
    "    \n",
    "    if CONFIG.USING_COARSE:#in case of training coarse data, I just used one scale to train. 상황에 따라 일반적인 tarin data를 쓸지, augment한 train data를 쓸지 정함 \n",
    "        loaders=[ trainloader ]\n",
    "    else:\n",
    "        loaders=[ trainloader ,trainloader1 ,trainloader2 ,trainloader3]\n",
    "    #generate 앞에 utils. 삭제\n",
    "    generate_param_report(os.path.join(save_path, exp_name + '.txt'), p)\n",
    "\n",
    "    num_img_tr = len(trainloader)\n",
    "    num_img_vl = len(valloader)\n",
    "    running_loss_tr = 0.0\n",
    "    running_loss_vl = 0.0\n",
    "    previous_miou = -1.0\n",
    "    global_step = 0\n",
    "    iev = Eval(20,19)\n",
    "    \n",
    "    # Main Training and Testing Loop 메인 학습 0부터 100까지\n",
    "    for epoch in range(resume_epoch, nEpochs):\n",
    "        start_time = timeit.default_timer()\n",
    "\n",
    "        if epoch % p['epoch_size'] == p['epoch_size'] - 1:\n",
    "            lr_ = lr_poly(p['lr'], epoch, nEpochs, 0.9)\n",
    "            print('(poly lr policy) learning rate: ', lr_)\n",
    "            optimizer = optim.SGD(net.parameters(), lr=lr_, momentum=p['momentum'], weight_decay=p['wd'])\n",
    "           \n",
    "        net.train()\n",
    "        for loader in loaders:\n",
    "            print(loader)\n",
    "            for ii, sample_batched in enumerate(loader):\n",
    "                \n",
    "    \n",
    "                inputs, labels = sample_batched['image'], sample_batched['label']\n",
    "                # Forward-Backward of the mini-batch\n",
    "                inputs, labels = Variable(inputs, requires_grad=True), Variable(labels)\n",
    "                #print('labels size', inputs.size() , labels.size())\n",
    "                global_step += inputs.data.shape[0]\n",
    "                #print(\"Glopal Step\",global_step)4,8,12,16\n",
    "                if CONFIG.USING_GPU:\n",
    "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = net.forward(inputs)\n",
    "                loss = criterion(outputs, labels,reduct='sum',weight=None)#sum\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                ls=loss.item()\n",
    "                running_loss_tr += ls\n",
    "    #            if ii% 10 == 0:\n",
    "    #                print(ls)\n",
    "                # Print stuff\n",
    "                if ii % num_img_tr == (num_img_tr - 1):\n",
    "                    running_loss_tr = running_loss_tr / num_img_tr\n",
    "                    writer.add_scalar('data/total_loss_epoch', running_loss_tr, epoch)\n",
    "                    print('[Epoch: %d, numImages: %5d]' % (epoch, ii * p['trainBatch'] + inputs.data.shape[0]))\n",
    "                    print('Loss: %f' % running_loss_tr)\n",
    "                    running_loss_tr = 0\n",
    "                    stop_time = timeit.default_timer()\n",
    "                    print(\"Execution time: \" + str(stop_time - start_time) + \"\\n\")\n",
    "       \n",
    "                \n",
    "                # Update the weights once in p['nAveGrad'] forward passes \n",
    "                writer.add_scalar('data/total_loss_iter', loss.item(), ii + num_img_tr * epoch)\n",
    "                \n",
    "    \n",
    "                # Show 10 * 3 images results each epoch\n",
    "                if ii % (num_img_tr // 10) == 0:\n",
    "                    grid_image = make_grid(inputs[:3].clone().cpu().data, 3, normalize=True)\n",
    "                    writer.add_image('Image', grid_image, global_step)\n",
    "                    grid_image = make_grid(\n",
    "                        decode_seg_map_sequence(torch.max(outputs[:3], 1)[1].detach().cpu().numpy(), 'cityscapes'), 3,\n",
    "                        normalize=False,\n",
    "                        range=(0, 255))\n",
    "                    writer.add_image('Predicted label', grid_image, global_step)\n",
    "                    grid_image = make_grid(\n",
    "                        decode_seg_map_sequence(torch.squeeze(labels[:3], 1).detach().cpu().numpy(), 'cityscapes'), 3,\n",
    "                        normalize=False, range=(0, 255))\n",
    "                    writer.add_image('Groundtruth label', grid_image, global_step)\n",
    "\n",
    "        # One testing epoch\n",
    "        if (epoch % nValInterval == (nValInterval - 1)) or epoch==0:\n",
    "            total_miou = 0.0\n",
    "            net.eval()\n",
    "            for ii, sample_batched in enumerate(valloader):\n",
    "                inputs, labels = sample_batched['image'], sample_batched['label']\n",
    "                print(inputs.shape)\n",
    "                # Forward pass of the mini-batch\n",
    "                inputs, labels = Variable(inputs, requires_grad=True), Variable(labels)\n",
    "                if CONFIG.USING_GPU:\n",
    "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    outputs = net.forward(inputs)\n",
    "\n",
    "                predictions = torch.max(outputs, 1)[1]\n",
    "\n",
    "                loss = criterion(outputs, labels,reduct='sum',weight=None)#sum elementwise_mean\n",
    "                running_loss_vl += loss.item()\n",
    "               \n",
    "                \n",
    "                y = torch.ones(labels.size()[2], labels.size()[3]).mul(19).cuda()\n",
    "                labels=labels.where(labels !=255, y)\n",
    "                \n",
    "                iev.addBatch(predictions.unsqueeze(1).data,labels)\n",
    "                \n",
    "                \n",
    "                # Print stuff\n",
    "                if ii % num_img_vl == num_img_vl - 1:\n",
    "                    miou=iev.getIoU()[0]\n",
    "                    running_loss_vl = running_loss_vl / num_img_vl\n",
    "                    print('Validation:')\n",
    "                    print('[Epoch: %d, numImages: %5d]' % (epoch, ii * testBatch + inputs.data.shape[0]))\n",
    "                    writer.add_scalar('data/test_loss_epoch', running_loss_vl, epoch)\n",
    "                    writer.add_scalar('data/test_miour', iev.getIoU()[0], epoch)\n",
    "                    print('Loss: %f' % running_loss_vl)\n",
    "                    print(\"Predi iou\",iev.getIoU())\n",
    "                    running_loss_vl = 0\n",
    "                    iev.reset()\n",
    "\n",
    "        # Save the model\n",
    "        if (epoch % snapshot) == snapshot - 1 :#and miou > previous_miou\n",
    "            previous_miou = miou\n",
    "            torch.save(net.state_dict(), os.path.join(save_path, 'models', modelName + '_epoch-' + str(epoch) + '.pth'))\n",
    "            print(\"Save model at {}\\n\".format(\n",
    "                os.path.join(save_path, 'models', modelName + '_epoch-' + str(epoch) + '.pth')))\n",
    "\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mobilenetv2\n",
    "- pretrained download : https://github.com/d-li14/mobilenetv2.pytorch/blob/master/pretrained/mobilenetv2_1.0-0c6065bc.pth\n",
    "- 경로 설정 주의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj17/anaconda3/envs/sia/lib/python3.7/site-packages/ipykernel_launcher.py:39: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj17/aiffel/siaiffel/LiteSeg-master/handmade_weights/experiments/experiment_2021-05-21_16_24\n",
      "LiteSeg-MobileNet...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for MobileNetV2:\n\tMissing key(s) in state_dict: \"features.0.0.weight\", \"features.0.1.weight\", \"features.0.1.bias\", \"features.0.1.running_mean\", \"features.0.1.running_var\", \"features.1.conv.0.weight\", \"features.1.conv.1.weight\", \"features.1.conv.1.bias\", \"features.1.conv.1.running_mean\", \"features.1.conv.1.running_var\", \"features.1.conv.3.weight\", \"features.1.conv.4.weight\", \"features.1.conv.4.bias\", \"features.1.conv.4.running_mean\", \"features.1.conv.4.running_var\", \"features.2.conv.0.weight\", \"features.2.conv.1.weight\", \"features.2.conv.1.bias\", \"features.2.conv.1.running_mean\", \"features.2.conv.1.running_var\", \"features.2.conv.3.weight\", \"features.2.conv.4.weight\", \"features.2.conv.4.bias\", \"features.2.conv.4.running_mean\", \"features.2.conv.4.running_var\", \"features.2.conv.6.weight\", \"features.2.conv.7.weight\", \"features.2.conv.7.bias\", \"features.2.conv.7.running_mean\", \"features.2.conv.7.running_var\", \"features.3.conv.0.weight\", \"features.3.conv.1.weight\", \"features.3.conv.1.bias\", \"features.3.conv.1.running_mean\", \"features.3.conv.1.running_var\", \"features.3.conv.3.weight\", \"features.3.conv.4.weight\", \"features.3.conv.4.bias\", \"features.3.conv.4.running_mean\", \"features.3.conv.4.running_var\", \"features.3.conv.6.weight\", \"features.3.conv.7.weight\", \"features.3.conv.7.bias\", \"features.3.conv.7.running_mean\", \"features.3.conv.7.running_var\", \"features.4.conv.0.weight\", \"features.4.conv.1.weight\", \"features.4.conv.1.bias\", \"features.4.conv.1.running_mean\", \"features.4.conv.1.running_var\", \"features.4.conv.3.weight\", \"features.4.conv.4.weight\", \"features.4.conv.4.bias\", \"features.4.conv.4.running_mean\", \"features.4.conv.4.running_var\", \"features.4.conv.6.weight\", \"features.4.conv.7.weight\", \"features.4.conv.7.bias\", \"features.4.conv.7.running_mean\", \"features.4.conv.7.running_var\", \"features.5.conv.0.weight\", \"features.5.conv.1.weight\", \"features.5.conv.1.bias\", \"features.5.conv.1.running_mean\", \"features.5.conv.1.running_var\", \"features.5.conv.3.weight\", \"features.5.conv.4.weight\", \"features.5.conv.4.bias\", \"features.5.conv.4.running_mean\", \"features.5.conv.4.running_var\", \"features.5.conv.6.weight\", \"features.5.conv.7.weight\", \"features.5.conv.7.bias\", \"features.5.conv.7.running_mean\", \"features.5.conv.7.running_var\", \"features.6.conv.0.weight\", \"features.6.conv.1.weight\", \"features.6.conv.1.bias\", \"features.6.conv.1.running_mean\", \"features.6.conv.1.running_var\", \"features.6.conv.3.weight\", \"features.6.conv.4.weight\", \"features.6.conv.4.bias\", \"features.6.conv.4.running_mean\", \"features.6.conv.4.running_var\", \"features.6.conv.6.weight\", \"features.6.conv.7.weight\", \"features.6.conv.7.bias\", \"features.6.conv.7.running_mean\", \"features.6.conv.7.running_var\", \"features.7.conv.0.weight\", \"features.7.conv.1.weight\", \"features.7.conv.1.bias\", \"features.7.conv.1.running_mean\", \"features.7.conv.1.running_var\", \"features.7.conv.3.weight\", \"features.7.conv.4.weight\", \"features.7.conv.4.bias\", \"features.7.conv.4.running_mean\", \"features.7.conv.4.running_var\", \"features.7.conv.6.weight\", \"features.7.conv.7.weight\", \"features.7.conv.7.bias\", \"features.7.conv.7.running_mean\", \"features.7.conv.7.running_var\", \"features.8.conv.0.weight\", \"features.8.conv.1.weight\", \"features.8.conv.1.bias\", \"features.8.conv.1.running_mean\", \"features.8.conv.1.running_var\", \"features.8.conv.3.weight\", \"features.8.conv.4.weight\", \"features.8.conv.4.bias\", \"features.8.conv.4.running_mean\", \"features.8.conv.4.running_var\", \"features.8.conv.6.weight\", \"features.8.conv.7.weight\", \"features.8.conv.7.bias\", \"features.8.conv.7.running_mean\", \"features.8.conv.7.running_var\", \"features.9.conv.0.weight\", \"features.9.conv.1.weight\", \"features.9.conv.1.bias\", \"features.9.conv.1.running_mean\", \"features.9.conv.1.running_var\", \"features.9.conv.3.weight\", \"features.9.conv.4.weight\", \"features.9.conv.4.bias\", \"features.9.conv.4.running_mean\", \"features.9.conv.4.running_var\", \"features.9.conv.6.weight\", \"features.9.conv.7.weight\", \"features.9.conv.7.bias\", \"features.9.conv.7.running_mean\", \"features.9.conv.7.running_var\", \"features.10.conv.0.weight\", \"features.10.conv.1.weight\", \"features.10.conv.1.bias\", \"features.10.conv.1.running_mean\", \"features.10.conv.1.running_var\", \"features.10.conv.3.weight\", \"features.10.conv.4.weight\", \"features.10.conv.4.bias\", \"features.10.conv.4.running_mean\", \"features.10.conv.4.running_var\", \"features.10.conv.6.weight\", \"features.10.conv.7.weight\", \"features.10.conv.7.bias\", \"features.10.conv.7.running_mean\", \"features.10.conv.7.running_var\", \"features.11.conv.0.weight\", \"features.11.conv.1.weight\", \"features.11.conv.1.bias\", \"features.11.conv.1.running_mean\", \"features.11.conv.1.running_var\", \"features.11.conv.3.weight\", \"features.11.conv.4.weight\", \"features.11.conv.4.bias\", \"features.11.conv.4.running_mean\", \"features.11.conv.4.running_var\", \"features.11.conv.6.weight\", \"features.11.conv.7.weight\", \"features.11.conv.7.bias\", \"features.11.conv.7.running_mean\", \"features.11.conv.7.running_var\", \"features.12.conv.0.weight\", \"features.12.conv.1.weight\", \"features.12.conv.1.bias\", \"features.12.conv.1.running_mean\", \"features.12.conv.1.running_var\", \"features.12.conv.3.weight\", \"features.12.conv.4.weight\", \"features.12.conv.4.bias\", \"features.12.conv.4.running_mean\", \"features.12.conv.4.running_var\", \"features.12.conv.6.weight\", \"features.12.conv.7.weight\", \"features.12.conv.7.bias\", \"features.12.conv.7.running_mean\", \"features.12.conv.7.running_var\", \"features.13.conv.0.weight\", \"features.13.conv.1.weight\", \"features.13.conv.1.bias\", \"features.13.conv.1.running_mean\", \"features.13.conv.1.running_var\", \"features.13.conv.3.weight\", \"features.13.conv.4.weight\", \"features.13.conv.4.bias\", \"features.13.conv.4.running_mean\", \"features.13.conv.4.running_var\", \"features.13.conv.6.weight\", \"features.13.conv.7.weight\", \"features.13.conv.7.bias\", \"features.13.conv.7.running_mean\", \"features.13.conv.7.running_var\", \"features.14.conv.0.weight\", \"features.14.conv.1.weight\", \"features.14.conv.1.bias\", \"features.14.conv.1.running_mean\", \"features.14.conv.1.running_var\", \"features.14.conv.3.weight\", \"features.14.conv.4.weight\", \"features.14.conv.4.bias\", \"features.14.conv.4.running_mean\", \"features.14.conv.4.running_var\", \"features.14.conv.6.weight\", \"features.14.conv.7.weight\", \"features.14.conv.7.bias\", \"features.14.conv.7.running_mean\", \"features.14.conv.7.running_var\", \"features.15.conv.0.weight\", \"features.15.conv.1.weight\", \"features.15.conv.1.bias\", \"features.15.conv.1.running_mean\", \"features.15.conv.1.running_var\", \"features.15.conv.3.weight\", \"features.15.conv.4.weight\", \"features.15.conv.4.bias\", \"features.15.conv.4.running_mean\", \"features.15.conv.4.running_var\", \"features.15.conv.6.weight\", \"features.15.conv.7.weight\", \"features.15.conv.7.bias\", \"features.15.conv.7.running_mean\", \"features.15.conv.7.running_var\", \"features.16.conv.0.weight\", \"features.16.conv.1.weight\", \"features.16.conv.1.bias\", \"features.16.conv.1.running_mean\", \"features.16.conv.1.running_var\", \"features.16.conv.3.weight\", \"features.16.conv.4.weight\", \"features.16.conv.4.bias\", \"features.16.conv.4.running_mean\", \"features.16.conv.4.running_var\", \"features.16.conv.6.weight\", \"features.16.conv.7.weight\", \"features.16.conv.7.bias\", \"features.16.conv.7.running_mean\", \"features.16.conv.7.running_var\", \"features.17.conv.0.weight\", \"features.17.conv.1.weight\", \"features.17.conv.1.bias\", \"features.17.conv.1.running_mean\", \"features.17.conv.1.running_var\", \"features.17.conv.3.weight\", \"features.17.conv.4.weight\", \"features.17.conv.4.bias\", \"features.17.conv.4.running_mean\", \"features.17.conv.4.running_var\", \"features.17.conv.6.weight\", \"features.17.conv.7.weight\", \"features.17.conv.7.bias\", \"features.17.conv.7.running_mean\", \"features.17.conv.7.running_var\", \"features.18.0.weight\", \"features.18.1.weight\", \"features.18.1.bias\", \"features.18.1.running_mean\", \"features.18.1.running_var\", \"classifier.1.weight\", \"classifier.1.bias\". \n\tUnexpected key(s) in state_dict: \"module.features.0.0.weight\", \"module.features.0.1.weight\", \"module.features.0.1.bias\", \"module.features.0.1.running_mean\", \"module.features.0.1.running_var\", \"module.features.1.conv.0.weight\", \"module.features.1.conv.1.weight\", \"module.features.1.conv.1.bias\", \"module.features.1.conv.1.running_mean\", \"module.features.1.conv.1.running_var\", \"module.features.1.conv.3.weight\", \"module.features.1.conv.4.weight\", \"module.features.1.conv.4.bias\", \"module.features.1.conv.4.running_mean\", \"module.features.1.conv.4.running_var\", \"module.features.1.conv.6.weight\", \"module.features.1.conv.7.weight\", \"module.features.1.conv.7.bias\", \"module.features.1.conv.7.running_mean\", \"module.features.1.conv.7.running_var\", \"module.features.2.conv.0.weight\", \"module.features.2.conv.1.weight\", \"module.features.2.conv.1.bias\", \"module.features.2.conv.1.running_mean\", \"module.features.2.conv.1.running_var\", \"module.features.2.conv.3.weight\", \"module.features.2.conv.4.weight\", \"module.features.2.conv.4.bias\", \"module.features.2.conv.4.running_mean\", \"module.features.2.conv.4.running_var\", \"module.features.2.conv.6.weight\", \"module.features.2.conv.7.weight\", \"module.features.2.conv.7.bias\", \"module.features.2.conv.7.running_mean\", \"module.features.2.conv.7.running_var\", \"module.features.3.conv.0.weight\", \"module.features.3.conv.1.weight\", \"module.features.3.conv.1.bias\", \"module.features.3.conv.1.running_mean\", \"module.features.3.conv.1.running_var\", \"module.features.3.conv.3.weight\", \"module.features.3.conv.4.weight\", \"module.features.3.conv.4.bias\", \"module.features.3.conv.4.running_mean\", \"module.features.3.conv.4.running_var\", \"module.features.3.conv.6.weight\", \"module.features.3.conv.7.weight\", \"module.features.3.conv.7.bias\", \"module.features.3.conv.7.running_mean\", \"module.features.3.conv.7.running_var\", \"module.features.4.conv.0.weight\", \"module.features.4.conv.1.weight\", \"module.features.4.conv.1.bias\", \"module.features.4.conv.1.running_mean\", \"module.features.4.conv.1.running_var\", \"module.features.4.conv.3.weight\", \"module.features.4.conv.4.weight\", \"module.features.4.conv.4.bias\", \"module.features.4.conv.4.running_mean\", \"module.features.4.conv.4.running_var\", \"module.features.4.conv.6.weight\", \"module.features.4.conv.7.weight\", \"module.features.4.conv.7.bias\", \"module.features.4.conv.7.running_mean\", \"module.features.4.conv.7.running_var\", \"module.features.5.conv.0.weight\", \"module.features.5.conv.1.weight\", \"module.features.5.conv.1.bias\", \"module.features.5.conv.1.running_mean\", \"module.features.5.conv.1.running_var\", \"module.features.5.conv.3.weight\", \"module.features.5.conv.4.weight\", \"module.features.5.conv.4.bias\", \"module.features.5.conv.4.running_mean\", \"module.features.5.conv.4.running_var\", \"module.features.5.conv.6.weight\", \"module.features.5.conv.7.weight\", \"module.features.5.conv.7.bias\", \"module.features.5.conv.7.running_mean\", \"module.features.5.conv.7.running_var\", \"module.features.6.conv.0.weight\", \"module.features.6.conv.1.weight\", \"module.features.6.conv.1.bias\", \"module.features.6.conv.1.running_mean\", \"module.features.6.conv.1.running_var\", \"module.features.6.conv.3.weight\", \"module.features.6.conv.4.weight\", \"module.features.6.conv.4.bias\", \"module.features.6.conv.4.running_mean\", \"module.features.6.conv.4.running_var\", \"module.features.6.conv.6.weight\", \"module.features.6.conv.7.weight\", \"module.features.6.conv.7.bias\", \"module.features.6.conv.7.running_mean\", \"module.features.6.conv.7.running_var\", \"module.features.7.conv.0.weight\", \"module.features.7.conv.1.weight\", \"module.features.7.conv.1.bias\", \"module.features.7.conv.1.running_mean\", \"module.features.7.conv.1.running_var\", \"module.features.7.conv.3.weight\", \"module.features.7.conv.4.weight\", \"module.features.7.conv.4.bias\", \"module.features.7.conv.4.running_mean\", \"module.features.7.conv.4.running_var\", \"module.features.7.conv.6.weight\", \"module.features.7.conv.7.weight\", \"module.features.7.conv.7.bias\", \"module.features.7.conv.7.running_mean\", \"module.features.7.conv.7.running_var\", \"module.features.8.conv.0.weight\", \"module.features.8.conv.1.weight\", \"module.features.8.conv.1.bias\", \"module.features.8.conv.1.running_mean\", \"module.features.8.conv.1.running_var\", \"module.features.8.conv.3.weight\", \"module.features.8.conv.4.weight\", \"module.features.8.conv.4.bias\", \"module.features.8.conv.4.running_mean\", \"module.features.8.conv.4.running_var\", \"module.features.8.conv.6.weight\", \"module.features.8.conv.7.weight\", \"module.features.8.conv.7.bias\", \"module.features.8.conv.7.running_mean\", \"module.features.8.conv.7.running_var\", \"module.features.9.conv.0.weight\", \"module.features.9.conv.1.weight\", \"module.features.9.conv.1.bias\", \"module.features.9.conv.1.running_mean\", \"module.features.9.conv.1.running_var\", \"module.features.9.conv.3.weight\", \"module.features.9.conv.4.weight\", \"module.features.9.conv.4.bias\", \"module.features.9.conv.4.running_mean\", \"module.features.9.conv.4.running_var\", \"module.features.9.conv.6.weight\", \"module.features.9.conv.7.weight\", \"module.features.9.conv.7.bias\", \"module.features.9.conv.7.running_mean\", \"module.features.9.conv.7.running_var\", \"module.features.10.conv.0.weight\", \"module.features.10.conv.1.weight\", \"module.features.10.conv.1.bias\", \"module.features.10.conv.1.running_mean\", \"module.features.10.conv.1.running_var\", \"module.features.10.conv.3.weight\", \"module.features.10.conv.4.weight\", \"module.features.10.conv.4.bias\", \"module.features.10.conv.4.running_mean\", \"module.features.10.conv.4.running_var\", \"module.features.10.conv.6.weight\", \"module.features.10.conv.7.weight\", \"module.features.10.conv.7.bias\", \"module.features.10.conv.7.running_mean\", \"module.features.10.conv.7.running_var\", \"module.features.11.conv.0.weight\", \"module.features.11.conv.1.weight\", \"module.features.11.conv.1.bias\", \"module.features.11.conv.1.running_mean\", \"module.features.11.conv.1.running_var\", \"module.features.11.conv.3.weight\", \"module.features.11.conv.4.weight\", \"module.features.11.conv.4.bias\", \"module.features.11.conv.4.running_mean\", \"module.features.11.conv.4.running_var\", \"module.features.11.conv.6.weight\", \"module.features.11.conv.7.weight\", \"module.features.11.conv.7.bias\", \"module.features.11.conv.7.running_mean\", \"module.features.11.conv.7.running_var\", \"module.features.12.conv.0.weight\", \"module.features.12.conv.1.weight\", \"module.features.12.conv.1.bias\", \"module.features.12.conv.1.running_mean\", \"module.features.12.conv.1.running_var\", \"module.features.12.conv.3.weight\", \"module.features.12.conv.4.weight\", \"module.features.12.conv.4.bias\", \"module.features.12.conv.4.running_mean\", \"module.features.12.conv.4.running_var\", \"module.features.12.conv.6.weight\", \"module.features.12.conv.7.weight\", \"module.features.12.conv.7.bias\", \"module.features.12.conv.7.running_mean\", \"module.features.12.conv.7.running_var\", \"module.features.13.conv.0.weight\", \"module.features.13.conv.1.weight\", \"module.features.13.conv.1.bias\", \"module.features.13.conv.1.running_mean\", \"module.features.13.conv.1.running_var\", \"module.features.13.conv.3.weight\", \"module.features.13.conv.4.weight\", \"module.features.13.conv.4.bias\", \"module.features.13.conv.4.running_mean\", \"module.features.13.conv.4.running_var\", \"module.features.13.conv.6.weight\", \"module.features.13.conv.7.weight\", \"module.features.13.conv.7.bias\", \"module.features.13.conv.7.running_mean\", \"module.features.13.conv.7.running_var\", \"module.features.14.conv.0.weight\", \"module.features.14.conv.1.weight\", \"module.features.14.conv.1.bias\", \"module.features.14.conv.1.running_mean\", \"module.features.14.conv.1.running_var\", \"module.features.14.conv.3.weight\", \"module.features.14.conv.4.weight\", \"module.features.14.conv.4.bias\", \"module.features.14.conv.4.running_mean\", \"module.features.14.conv.4.running_var\", \"module.features.14.conv.6.weight\", \"module.features.14.conv.7.weight\", \"module.features.14.conv.7.bias\", \"module.features.14.conv.7.running_mean\", \"module.features.14.conv.7.running_var\", \"module.features.15.conv.0.weight\", \"module.features.15.conv.1.weight\", \"module.features.15.conv.1.bias\", \"module.features.15.conv.1.running_mean\", \"module.features.15.conv.1.running_var\", \"module.features.15.conv.3.weight\", \"module.features.15.conv.4.weight\", \"module.features.15.conv.4.bias\", \"module.features.15.conv.4.running_mean\", \"module.features.15.conv.4.running_var\", \"module.features.15.conv.6.weight\", \"module.features.15.conv.7.weight\", \"module.features.15.conv.7.bias\", \"module.features.15.conv.7.running_mean\", \"module.features.15.conv.7.running_var\", \"module.features.16.conv.0.weight\", \"module.features.16.conv.1.weight\", \"module.features.16.conv.1.bias\", \"module.features.16.conv.1.running_mean\", \"module.features.16.conv.1.running_var\", \"module.features.16.conv.3.weight\", \"module.features.16.conv.4.weight\", \"module.features.16.conv.4.bias\", \"module.features.16.conv.4.running_mean\", \"module.features.16.conv.4.running_var\", \"module.features.16.conv.6.weight\", \"module.features.16.conv.7.weight\", \"module.features.16.conv.7.bias\", \"module.features.16.conv.7.running_mean\", \"module.features.16.conv.7.running_var\", \"module.features.17.conv.0.weight\", \"module.features.17.conv.1.weight\", \"module.features.17.conv.1.bias\", \"module.features.17.conv.1.running_mean\", \"module.features.17.conv.1.running_var\", \"module.features.17.conv.3.weight\", \"module.features.17.conv.4.weight\", \"module.features.17.conv.4.bias\", \"module.features.17.conv.4.running_mean\", \"module.features.17.conv.4.running_var\", \"module.features.17.conv.6.weight\", \"module.features.17.conv.7.weight\", \"module.features.17.conv.7.bias\", \"module.features.17.conv.7.running_mean\", \"module.features.17.conv.7.running_var\", \"module.features.18.0.weight\", \"module.features.18.1.weight\", \"module.features.18.1.bias\", \"module.features.18.1.running_mean\", \"module.features.18.1.running_var\", \"module.classifier.1.weight\", \"module.classifier.1.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-e99acbf2cd81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;31m# LiteSeg.build 괄호 첫번째에 backbone_network 뭘로 할지 str로 입력\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0mmodelpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_path_coarse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLiteSeg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackbone_network\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodelpath\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mCONFIG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mCONFIG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUSING_GPU\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-e4f5f36c53f2>\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(backbone_network, modelpath, CONFIG, is_train)\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshufflenet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m19\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPRETRAINED_WEIGHTS\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCONFIG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPRETRAINED_SHUFFLENET\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mbackbone_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'mobilenet'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMobileNet_RT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m19\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPRETRAINED_WEIGHTS\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCONFIG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPRETRAINED_MOBILENET\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-a4f81e94cc78>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, n_classes, PRETRAINED_WEIGHTS, pretrained)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpretrained\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mstate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPRETRAINED_WEIGHTS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmobile_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mrates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sia/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    717\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m--> 719\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m    720\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for MobileNetV2:\n\tMissing key(s) in state_dict: \"features.0.0.weight\", \"features.0.1.weight\", \"features.0.1.bias\", \"features.0.1.running_mean\", \"features.0.1.running_var\", \"features.1.conv.0.weight\", \"features.1.conv.1.weight\", \"features.1.conv.1.bias\", \"features.1.conv.1.running_mean\", \"features.1.conv.1.running_var\", \"features.1.conv.3.weight\", \"features.1.conv.4.weight\", \"features.1.conv.4.bias\", \"features.1.conv.4.running_mean\", \"features.1.conv.4.running_var\", \"features.2.conv.0.weight\", \"features.2.conv.1.weight\", \"features.2.conv.1.bias\", \"features.2.conv.1.running_mean\", \"features.2.conv.1.running_var\", \"features.2.conv.3.weight\", \"features.2.conv.4.weight\", \"features.2.conv.4.bias\", \"features.2.conv.4.running_mean\", \"features.2.conv.4.running_var\", \"features.2.conv.6.weight\", \"features.2.conv.7.weight\", \"features.2.conv.7.bias\", \"features.2.conv.7.running_mean\", \"features.2.conv.7.running_var\", \"features.3.conv.0.weight\", \"features.3.conv.1.weight\", \"features.3.conv.1.bias\", \"features.3.conv.1.running_mean\", \"features.3.conv.1.running_var\", \"features.3.conv.3.weight\", \"features.3.conv.4.weight\", \"features.3.conv.4.bias\", \"features.3.conv.4.running_mean\", \"features.3.conv.4.running_var\", \"features.3.conv.6.weight\", \"features.3.conv.7.weight\", \"features.3.conv.7.bias\", \"features.3.conv.7.running_mean\", \"features.3.conv.7.running_var\", \"features.4.conv.0.weight\", \"features.4.conv.1.weight\", \"features.4.conv.1.bias\", \"features.4.conv.1.running_mean\", \"features.4.conv.1.running_var\", \"features.4.conv.3.weight\", \"features.4.conv.4.weight\", \"features.4.conv.4.bias\", \"features.4.conv.4.running_mean\", \"features.4.conv.4.running_var\", \"features.4.conv.6.weight\", \"features.4.conv.7.weight\", \"features.4.conv.7.bias\", \"features.4.conv.7.running_mean\", \"features.4.conv.7.running_var\", \"features.5.conv.0.weight\", \"features.5.conv.1.weight\", \"features.5.conv.1.bias\", \"features.5.conv.1.running_mean\", \"features.5.conv.1.running_var\", \"features.5.conv.3.weight\", \"features.5.conv.4.weight\", \"features.5.conv.4.bias\", \"features.5.conv.4.running_mean\", \"features.5.conv.4.running_var\", \"features.5.conv.6.weight\", \"features.5.conv.7.weight\", \"features.5.conv.7.bias\", \"features.5.conv.7.running_mean\", \"features.5.conv.7.running_var\", \"features.6.conv.0.weight\", \"features.6.conv.1.weight\", \"features.6.conv.1.bias\", \"features.6.conv.1.running_mean\", \"features.6.conv.1.running_var\", \"features.6.conv.3.weight\", \"features.6.conv.4.weight\", \"features.6.conv.4.bias\", \"features.6.conv.4.running_mean\", \"features.6.conv.4.running_var\", \"features.6.conv.6.weight\", \"features.6.conv.7.weight\", \"features.6.conv.7.bias\", \"features.6.conv.7.running_mean\", \"features.6.conv.7.running_var\", \"features.7.conv.0.weight\", \"features.7.conv.1.weight\", \"features.7.conv.1.bias\", \"features.7.conv.1.running_mean\", \"features.7.conv.1.running_var\", \"features.7.conv.3.weight\", \"features.7.conv.4.weight\", \"features.7.conv.4.bias\", \"features.7.conv.4.running_mean\", \"features.7.conv.4.running_var\", \"features.7.conv.6.weight\", \"features.7.conv.7.weight\", \"features.7.conv.7.bias\", \"features.7.conv.7.running_mean\", \"features.7.conv.7.running_var\", \"features.8.conv.0.weight\", \"features.8.conv.1.weight\", \"features.8.conv.1.bias\", \"features.8.conv.1.running_mean\", \"features.8.conv.1.running_var\", \"features.8.conv.3.weight\", \"features.8.conv.4.weight\", \"features.8.conv.4.bias\", \"features.8.conv.4.running_mean\", \"features.8.conv.4.running_var\", \"features.8.conv.6.weight\", \"features.8.conv.7.weight\", \"features.8.conv.7.bias\", \"features.8.conv.7.running_mean\", \"features.8.conv.7.running_var\", \"features.9.conv.0.weight\", \"features.9.conv.1.weight\", \"features.9.conv.1.bias\", \"features.9.conv.1.running_mean\", \"features.9.conv.1.running_var\", \"features.9.conv.3.weight\", \"features.9.conv.4.weight\", \"features.9.conv.4.bias\", \"features.9.conv.4.running_mean\", \"features.9.conv.4.running_var\", \"features.9.conv.6.weight\", \"features.9.conv.7.weight\", \"features.9.conv.7.bias\", \"features.9.conv.7.running_mean\", \"features.9.conv.7.running_var\", \"features.10.conv.0.weight\", \"features.10.conv.1.weight\", \"features.10.conv.1.bias\", \"features.10.conv.1.running_mean\", \"features.10.conv.1.running_var\", \"features.10.conv.3.weight\", \"features.10.conv.4.weight\", \"features.10.conv.4.bias\", \"features.10.conv.4.running_mean\", \"features.10.conv.4.running_var\", \"features.10.conv.6.weight\", \"features.10.conv.7.weight\", \"features.10.conv.7.bias\", \"features.10.conv.7.running_mean\", \"features.10.conv.7.running_var\", \"features.11.conv.0.weight\", \"features.11.conv.1.weight\", \"features.11.conv.1.bias\", \"features.11.conv.1.running_mean\", \"features.11.conv.1.running_var\", \"features.11.conv.3.weight\", \"features.11.conv.4.weight\", \"features.11.conv.4.bias\", \"features.11.conv.4.running_mean\", \"features.11.conv.4.running_var\", \"features.11.conv.6.weight\", \"features.11.conv.7.weight\", \"features.11.conv.7.bias\", \"features.11.conv.7.running_mean\", \"features.11.conv.7.running_var\", \"features.12.conv.0.weight\", \"features.12.conv.1.weight\", \"features.12.conv.1.bias\", \"features.12.conv.1.running_mean\", \"features.12.conv.1.running_var\", \"features.12.conv.3.weight\", \"features.12.conv.4.weight\", \"features.12.conv.4.bias\", \"features.12.conv.4.running_mean\", \"features.12.conv.4.running_var\", \"features.12.conv.6.weight\", \"features.12.conv.7.weight\", \"features.12.conv.7.bias\", \"features.12.conv.7.running_mean\", \"features.12.conv.7.running_var\", \"features.13.conv.0.weight\", \"features.13.conv.1.weight\", \"features.13.conv.1.bias\", \"features.13.conv.1.running_mean\", \"features.13.conv.1.running_var\", \"features.13.conv.3.weight\", \"features.13.conv.4.weight\", \"features.13.conv.4.bias\", \"features.13.conv.4.running_mean\", \"features.13.conv.4.running_var\", \"features.13.conv.6.weight\", \"features.13.conv.7.weight\", \"features.13.conv.7.bias\", \"features.13.conv.7.running_mean\", \"features.13.conv.7.running_var\", \"features.14.conv.0.weight\", \"features.14.conv.1.weight\", \"features.14.conv.1.bias\", \"features.14.conv.1.running_mean\", \"features.14.conv.1.running_var\", \"features.14.conv.3.weight\", \"features.14.conv.4.weight\", \"features.14.conv.4.bias\", \"features.14.conv.4.running_mean\", \"features.14.conv.4.running_var\", \"features.14.conv.6.weight\", \"features.14.conv.7.weight\", \"features.14.conv.7.bias\", \"features.14.conv.7.running_mean\", \"features.14.conv.7.running_var\", \"features.15.conv.0.weight\", \"features.15.conv.1.weight\", \"features.15.conv.1.bias\", \"features.15.conv.1.running_mean\", \"features.15.conv.1.running_var\", \"features.15.conv.3.weight\", \"features.15.conv.4.weight\", \"features.15.conv.4.bias\", \"features.15.conv.4.running_mean\", \"features.15.conv.4.running_var\", \"features.15.conv.6.weight\", \"features.15.conv.7.weight\", \"features.15.conv.7.bias\", \"features.15.conv.7.running_mean\", \"features.15.conv.7.running_var\", \"features.16.conv.0.weight\", \"features.16.conv.1.weight\", \"features.16.conv.1.bias\", \"features.16.conv.1.running_mean\", \"features.16.conv.1.running_var\", \"features.16.conv.3.weight\", \"features.16.conv.4.weight\", \"features.16.conv.4.bias\", \"features.16.conv.4.running_mean\", \"features.16.conv.4.running_var\", \"features.16.conv.6.weight\", \"features.16.conv.7.weight\", \"features.16.conv.7.bias\", \"features.16.conv.7.running_mean\", \"features.16.conv.7.running_var\", \"features.17.conv.0.weight\", \"features.17.conv.1.weight\", \"features.17.conv.1.bias\", \"features.17.conv.1.running_mean\", \"features.17.conv.1.running_var\", \"features.17.conv.3.weight\", \"features.17.conv.4.weight\", \"features.17.conv.4.bias\", \"features.17.conv.4.running_mean\", \"features.17.conv.4.running_var\", \"features.17.conv.6.weight\", \"features.17.conv.7.weight\", \"features.17.conv.7.bias\", \"features.17.conv.7.running_mean\", \"features.17.conv.7.running_var\", \"features.18.0.weight\", \"features.18.1.weight\", \"features.18.1.bias\", \"features.18.1.running_mean\", \"features.18.1.running_var\", \"classifier.1.weight\", \"classifier.1.bias\". \n\tUnexpected key(s) in state_dict: \"module.features.0.0.weight\", \"module.features.0.1.weight\", \"module.features.0.1.bias\", \"module.features.0.1.running_mean\", \"module.features.0.1.running_var\", \"module.features.1.conv.0.weight\", \"module.features.1.conv.1.weight\", \"module.features.1.conv.1.bias\", \"module.features.1.conv.1.running_mean\", \"module.features.1.conv.1.running_var\", \"module.features.1.conv.3.weight\", \"module.features.1.conv.4.weight\", \"module.features.1.conv.4.bias\", \"module.features.1.conv.4.running_mean\", \"module.features.1.conv.4.running_var\", \"module.features.1.conv.6.weight\", \"module.features.1.conv.7.weight\", \"module.features.1.conv.7.bias\", \"module.features.1.conv.7.running_mean\", \"module.features.1.conv.7.running_var\", \"module.features.2.conv.0.weight\", \"module.features.2.conv.1.weight\", \"module.features.2.conv.1.bias\", \"module.features.2.conv.1.running_mean\", \"module.features.2.conv.1.running_var\", \"module.features.2.conv.3.weight\", \"module.features.2.conv.4.weight\", \"module.features.2.conv.4.bias\", \"module.features.2.conv.4.running_mean\", \"module.features.2.conv.4.running_var\", \"module.features.2.conv.6.weight\", \"module.features.2.conv.7.weight\", \"module.features.2.conv.7.bias\", \"module.features.2.conv.7.running_mean\", \"module.features.2.conv.7.running_var\", \"module.features.3.conv.0.weight\", \"module.features.3.conv.1.weight\", \"module.features.3.conv.1.bias\", \"module.features.3.conv.1.running_mean\", \"module.features.3.conv.1.running_var\", \"module.features.3.conv.3.weight\", \"module.features.3.conv.4.weight\", \"module.features.3.conv.4.bias\", \"module.features.3.conv.4.running_mean\", \"module.features.3.conv.4.running_var\", \"module.features.3.conv.6.weight\", \"module.features.3.conv.7.weight\", \"module.features.3.conv.7.bias\", \"module.features.3.conv.7.running_mean\", \"module.features.3.conv.7.running_var\", \"module.features.4.conv.0.weight\", \"module.features.4.conv.1.weight\", \"module.features.4.conv.1.bias\", \"module.features.4.conv.1.running_mean\", \"module.features.4.conv.1.running_var\", \"module.features.4.conv.3.weight\", \"module.features.4.conv.4.weight\", \"module.features.4.conv.4.bias\", \"module.features.4.conv.4.running_mean\", \"module.features.4.conv.4.running_var\", \"module.features.4.conv.6.weight\", \"module.features.4.conv.7.weight\", \"module.features.4.conv.7.bias\", \"module.features.4.conv.7.running_mean\", \"module.features.4.conv.7.running_var\", \"module.features.5.conv.0.weight\", \"module.features.5.conv.1.weight\", \"module.features.5.conv.1.bias\", \"module.features.5.conv.1.running_mean\", \"module.features.5.conv.1.running_var\", \"module.features.5.conv.3.weight\", \"module.features.5.conv.4.weight\", \"module.features.5.conv.4.bias\", \"module.features.5.conv.4.running_mean\", \"module.features.5.conv.4.running_var\", \"module.features.5.conv.6.weight\", \"module.features.5.conv.7.weight\", \"module.features.5.conv.7.bias\", \"module.features.5.conv.7.running_mean\", \"module.features.5.conv.7.running_var\", \"module.features.6.conv.0.weight\", \"module.features.6.conv.1.weight\", \"module.features.6.conv.1.bias\", \"module.features.6.conv.1.running_mean\", \"module.features.6.conv.1.running_var\", \"module.features.6.conv.3.weight\", \"module.features.6.conv.4.weight\", \"module.features.6.conv.4.bias\", \"module.features.6.conv.4.running_mean\", \"module.features.6.conv.4.running_var\", \"module.features.6.conv.6.weight\", \"module.features.6.conv.7.weight\", \"module.features.6.conv.7.bias\", \"module.features.6.conv.7.running_mean\", \"module.features.6.conv.7.running_var\", \"module.features.7.conv.0.weight\", \"module.features.7.conv.1.weight\", \"module.features.7.conv.1.bias\", \"module.features.7.conv.1.running_mean\", \"module.features.7.conv.1.running_var\", \"module.features.7.conv.3.weight\", \"module.features.7.conv.4.weight\", \"module.features.7.conv.4.bias\", \"module.features.7.conv.4.running_mean\", \"module.features.7.conv.4.running_var\", \"module.features.7.conv.6.weight\", \"module.features.7.conv.7.weight\", \"module.features.7.conv.7.bias\", \"module.features.7.conv.7.running_mean\", \"module.features.7.conv.7.running_var\", \"module.features.8.conv.0.weight\", \"module.features.8.conv.1.weight\", \"module.features.8.conv.1.bias\", \"module.features.8.conv.1.running_mean\", \"module.features.8.conv.1.running_var\", \"module.features.8.conv.3.weight\", \"module.features.8.conv.4.weight\", \"module.features.8.conv.4.bias\", \"module.features.8.conv.4.running_mean\", \"module.features.8.conv.4.running_var\", \"module.features.8.conv.6.weight\", \"module.features.8.conv.7.weight\", \"module.features.8.conv.7.bias\", \"module.features.8.conv.7.running_mean\", \"module.features.8.conv.7.running_var\", \"module.features.9.conv.0.weight\", \"module.features.9.conv.1.weight\", \"module.features.9.conv.1.bias\", \"module.features.9.conv.1.running_mean\", \"module.features.9.conv.1.running_var\", \"module.features.9.conv.3.weight\", \"module.features.9.conv.4.weight\", \"module.features.9.conv.4.bias\", \"module.features.9.conv.4.running_mean\", \"module.features.9.conv.4.running_var\", \"module.features.9.conv.6.weight\", \"module.features.9.conv.7.weight\", \"module.features.9.conv.7.bias\", \"module.features.9.conv.7.running_mean\", \"module.features.9.conv.7.running_var\", \"module.features.10.conv.0.weight\", \"module.features.10.conv.1.weight\", \"module.features.10.conv.1.bias\", \"module.features.10.conv.1.running_mean\", \"module.features.10.conv.1.running_var\", \"module.features.10.conv.3.weight\", \"module.features.10.conv.4.weight\", \"module.features.10.conv.4.bias\", \"module.features.10.conv.4.running_mean\", \"module.features.10.conv.4.running_var\", \"module.features.10.conv.6.weight\", \"module.features.10.conv.7.weight\", \"module.features.10.conv.7.bias\", \"module.features.10.conv.7.running_mean\", \"module.features.10.conv.7.running_var\", \"module.features.11.conv.0.weight\", \"module.features.11.conv.1.weight\", \"module.features.11.conv.1.bias\", \"module.features.11.conv.1.running_mean\", \"module.features.11.conv.1.running_var\", \"module.features.11.conv.3.weight\", \"module.features.11.conv.4.weight\", \"module.features.11.conv.4.bias\", \"module.features.11.conv.4.running_mean\", \"module.features.11.conv.4.running_var\", \"module.features.11.conv.6.weight\", \"module.features.11.conv.7.weight\", \"module.features.11.conv.7.bias\", \"module.features.11.conv.7.running_mean\", \"module.features.11.conv.7.running_var\", \"module.features.12.conv.0.weight\", \"module.features.12.conv.1.weight\", \"module.features.12.conv.1.bias\", \"module.features.12.conv.1.running_mean\", \"module.features.12.conv.1.running_var\", \"module.features.12.conv.3.weight\", \"module.features.12.conv.4.weight\", \"module.features.12.conv.4.bias\", \"module.features.12.conv.4.running_mean\", \"module.features.12.conv.4.running_var\", \"module.features.12.conv.6.weight\", \"module.features.12.conv.7.weight\", \"module.features.12.conv.7.bias\", \"module.features.12.conv.7.running_mean\", \"module.features.12.conv.7.running_var\", \"module.features.13.conv.0.weight\", \"module.features.13.conv.1.weight\", \"module.features.13.conv.1.bias\", \"module.features.13.conv.1.running_mean\", \"module.features.13.conv.1.running_var\", \"module.features.13.conv.3.weight\", \"module.features.13.conv.4.weight\", \"module.features.13.conv.4.bias\", \"module.features.13.conv.4.running_mean\", \"module.features.13.conv.4.running_var\", \"module.features.13.conv.6.weight\", \"module.features.13.conv.7.weight\", \"module.features.13.conv.7.bias\", \"module.features.13.conv.7.running_mean\", \"module.features.13.conv.7.running_var\", \"module.features.14.conv.0.weight\", \"module.features.14.conv.1.weight\", \"module.features.14.conv.1.bias\", \"module.features.14.conv.1.running_mean\", \"module.features.14.conv.1.running_var\", \"module.features.14.conv.3.weight\", \"module.features.14.conv.4.weight\", \"module.features.14.conv.4.bias\", \"module.features.14.conv.4.running_mean\", \"module.features.14.conv.4.running_var\", \"module.features.14.conv.6.weight\", \"module.features.14.conv.7.weight\", \"module.features.14.conv.7.bias\", \"module.features.14.conv.7.running_mean\", \"module.features.14.conv.7.running_var\", \"module.features.15.conv.0.weight\", \"module.features.15.conv.1.weight\", \"module.features.15.conv.1.bias\", \"module.features.15.conv.1.running_mean\", \"module.features.15.conv.1.running_var\", \"module.features.15.conv.3.weight\", \"module.features.15.conv.4.weight\", \"module.features.15.conv.4.bias\", \"module.features.15.conv.4.running_mean\", \"module.features.15.conv.4.running_var\", \"module.features.15.conv.6.weight\", \"module.features.15.conv.7.weight\", \"module.features.15.conv.7.bias\", \"module.features.15.conv.7.running_mean\", \"module.features.15.conv.7.running_var\", \"module.features.16.conv.0.weight\", \"module.features.16.conv.1.weight\", \"module.features.16.conv.1.bias\", \"module.features.16.conv.1.running_mean\", \"module.features.16.conv.1.running_var\", \"module.features.16.conv.3.weight\", \"module.features.16.conv.4.weight\", \"module.features.16.conv.4.bias\", \"module.features.16.conv.4.running_mean\", \"module.features.16.conv.4.running_var\", \"module.features.16.conv.6.weight\", \"module.features.16.conv.7.weight\", \"module.features.16.conv.7.bias\", \"module.features.16.conv.7.running_mean\", \"module.features.16.conv.7.running_var\", \"module.features.17.conv.0.weight\", \"module.features.17.conv.1.weight\", \"module.features.17.conv.1.bias\", \"module.features.17.conv.1.running_mean\", \"module.features.17.conv.1.running_var\", \"module.features.17.conv.3.weight\", \"module.features.17.conv.4.weight\", \"module.features.17.conv.4.bias\", \"module.features.17.conv.4.running_mean\", \"module.features.17.conv.4.running_var\", \"module.features.17.conv.6.weight\", \"module.features.17.conv.7.weight\", \"module.features.17.conv.7.bias\", \"module.features.17.conv.7.running_mean\", \"module.features.17.conv.7.running_var\", \"module.features.18.0.weight\", \"module.features.18.1.weight\", \"module.features.18.1.bias\", \"module.features.18.1.running_mean\", \"module.features.18.1.running_var\", \"module.classifier.1.weight\", \"module.classifier.1.bias\". "
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import timeit\n",
    "from datetime import datetime\n",
    "import os\n",
    "import glob\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import yaml\n",
    "from addict import Dict\n",
    "import argparse\n",
    "\n",
    "# PyTorch includes\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "# Tensorboard include\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "# Custom includes\n",
    "#from dataloaders import cityscapes\n",
    "#from dataloaders import utils\n",
    "#from dataloaders import augmentation as augment\n",
    "#from models.liteseg import LiteSeg\n",
    "#from utils import loss as losses\n",
    "#from utils import iou_eval\n",
    "\n",
    "\n",
    "#To make reproducible results  \n",
    "torch.manual_seed(125)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(125)\n",
    "\n",
    "#경로 변경 주의\n",
    "CONFIG=Dict(yaml.load(open('/home/aiffel-dj17/aiffel/siaiffel/LiteSeg-master/config/training.yaml')))\n",
    "\n",
    "#ap, args 함수 : shell 창에서 어떤 backbone 을 쓸지, model path, pretrained_mode\n",
    "#ap = argparse.ArgumentParser()\n",
    "#ap.add_argument('--backbone_network', required=True,\n",
    "                #help = 'name of backbone network',default='darknet')#shufflenet, mobilenet, and darknet\n",
    "#ap.add_argument('--model_path_coarse', required=False,\n",
    "                #help = 'path to pretrained model on coarse data',default='pretrained_models/liteseg-darknet-cityscapes.pth')\n",
    "#ap.add_argument('--model_path_resume', required=False,\n",
    "                #help = 'path to a model to resume from',default='pretrained_models/liteseg-darknet-cityscapes.pth')\n",
    "\n",
    "#args = ap.parse_args()\n",
    "backbone_network= 'mobilenet'\n",
    "#backbone_network= 'darknet' # backbone_network 뭘로 할지 str로 입력\n",
    "#model_path_resume=args.model_path_resume\n",
    "#경로 변경 주의\n",
    "#model_path_coarse ='/home/aiffel-dj17/aiffel/siaiffel/LiteSeg-master/pretrained_models/liteseg-darknet-cityscapes.pth'\n",
    "model_path_coarse = '/home/aiffel-dj17/aiffel/siaiffel/LiteSeg-master/pretrained_models/liteseg-mobilenet-cityscapes.pth'\n",
    "#model_path_resume = '/home/aiffel-dj17/aiffel/siaiffel/LiteSeg-master/pretrained_models/liteseg-darknet-cityscapes.pth'\n",
    "\n",
    "\n",
    "# Setting parameters\n",
    "nEpochs =10  # Number of epochs for training 150 종료epoch\n",
    "resume_epoch = 0  # Default is 0, change if want to resume 0 시작epoch\n",
    "\n",
    "p = OrderedDict()  # Parameters to include in report\n",
    "p['trainBatch'] =4  # Training batch size\n",
    "p['lr'] =1e-7 # Learning rate  1e-8 for darknet and 1e-7 shufflenet and mobilenet\n",
    "p['wd'] = 5e-4  # Weight decay\n",
    "p['momentum'] = 0.9  # Momentum\n",
    "p['epoch_size'] =5  # epochs to change learning rate\n",
    "\n",
    "testBatch = 1  # Testing batch size\n",
    "nValInterval = 2  # Run on test set every nTestInterval epochs\n",
    "snapshot = 2  # Store a model every snapshot epochs\n",
    "\n",
    "# 저장 경로 설정\n",
    "save_dir_root = os.getenv('HOME')+'/aiffel/siaiffel/LiteSeg-master/handmade_weights'\n",
    "\n",
    "dataset_path=CONFIG.DATASET_FINE\n",
    "if CONFIG.USING_COARSE:\n",
    "    print(\"Training on Coarse Data\")\n",
    "    dataset_path=CONFIG.DATASET_COARSE\n",
    "    p['epoch_size'] =10 #we increase the number of epochs to change LR as we train on one scale\n",
    "\n",
    "\n",
    "#txt파일 이름 설정\n",
    "# exp_name = os.path.dirname(os.path.abspath(__file__)).split('/')[-1]\n",
    "exp_name = 'sangmin_0520'\n",
    "\n",
    "class_weight = np.array([0.05570516, 0.32337477, 0.08998544, 1.03602707, 1.03413147, 1.68195437,\n",
    "                                 5.58540548, 3.56563995, 0.12704978, 1.,         0.46783719, 1.34551528,\n",
    "                                 5.29974114, 0.28342531, 0.9396095,  0.81551811, 0.42679146, 3.6399074,\n",
    "                                 2.78376194], dtype=float)\n",
    "class_weight = torch.from_numpy(class_weight).float().cuda()\n",
    "\n",
    "\n",
    "\n",
    "#make a folder -with name of current time- for every experiment\n",
    "experiment_id=datetime.now().strftime(\"%Y-%m-%d_%H_%M\")\n",
    "save_path = os.path.join(save_dir_root, 'experiments', 'experiment_' + str(experiment_id))\n",
    "print(save_path)\n",
    "\n",
    "\n",
    "# Network definition\n",
    "# LiteSeg.build 괄호 첫번째에 backbone_network 뭘로 할지 str로 입력\n",
    "modelpath = model_path_coarse\n",
    "net=LiteSeg.build(backbone_network, modelpath ,CONFIG, is_train=True)\n",
    "\n",
    "if CONFIG.USING_GPU:\n",
    "    torch.cuda.set_device(device=CONFIG.GPU_ID)\n",
    "    net.cuda()\n",
    "\n",
    "\n",
    "#using the trained model on the coarse data\n",
    "#If you want to train model on fine data directley, comment the next 3 lines.\n",
    "if not CONFIG.USING_COARSE:\n",
    "    print(\"Using a weights from training coarse data from: {}...\".format(model_path_coarse))\n",
    "    net.load_state_dict(torch.load(model_path_coarse)) \n",
    "\n",
    "\n",
    "#resume tarining from a given model, \n",
    "#Attention! the learnig rate which used for resuming training, is not the intial one.\n",
    "if resume_epoch == 0:\n",
    "    print(\"Training Network...\")\n",
    "else:\n",
    "    print(\"Resume training from a model at: {}...\".format(model_path_resume))\n",
    "    net.load_state_dict(torch.load(model_path_resume))\n",
    "\n",
    "    \n",
    "modelName = 'LiteSeg-' + backbone_network + '-cityscapes'\n",
    "print(modelName)\n",
    "\n",
    "criterion = cross_entropy2d\n",
    "\n",
    "# 여기서부터 시작\n",
    "if resume_epoch != nEpochs+1:\n",
    "    # Logging into Tensorboard / path 설정\n",
    "    log_dir = os.path.join(save_path, 'models', datetime.now().strftime('%b%d_%H-%M-%S') + '_' + socket.gethostname())\n",
    "    writer = SummaryWriter(log_dir=log_dir) #summary 설정\n",
    "\n",
    "    # Use the following optimizer 옵티마이저 설정\n",
    "    optimizer = optim.SGD(net.parameters(), lr=p['lr'], momentum=p['momentum'], weight_decay=p['wd'])\n",
    "    #optimizer = optim.Adam(net.parameters(), 5e-4, (0.9, 0.999), eps=1e-08, weight_decay=1e-4) \n",
    "    p['optimizer'] = str(optimizer) \n",
    "\n",
    "    #augment\n",
    "    #augment. 삭제\n",
    "    composed_transforms_tr = transforms.Compose([\n",
    "        RandomHorizontalFlip(),\n",
    "        RandomScale((0.2, .8)),\n",
    "        RandomCrop(( 512,1024)),\n",
    "        RandomRotate(5),\n",
    "        Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensor()])\n",
    "   \n",
    "    composed_transforms_tr1 = transforms.Compose([\n",
    "        RandomHorizontalFlip(),\n",
    "        RandomScale((0.2, .8)),\n",
    "        RandomCrop(( 768,1536)),\n",
    "        Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensor()])\n",
    "    composed_transforms_tr2 = transforms.Compose([\n",
    "        RandomHorizontalFlip(),\n",
    "        RandomScale((0.2, .8)),\n",
    "        RandomCrop(( 360,640)),\n",
    "        Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensor()])\n",
    "    composed_transforms_tr3 = transforms.Compose([\n",
    "        RandomHorizontalFlip(),\n",
    "        RandomScale((0.2, .8)),\n",
    "        RandomCrop(( 720,1280)),\n",
    "        Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensor()])\n",
    "    \n",
    "    composed_transforms_ts = transforms.Compose([\n",
    "        RandomHorizontalFlip(),\n",
    "        #augment.Scale((819, 1638)),\n",
    "        CenterCrop(( 512,1024)),\n",
    "        Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),#augment. Normalize_cityscapes(mean=(72.39, 82.91, 73.16)),\n",
    "        ToTensor()])\n",
    "#augment train, val data load\n",
    "#cityscapes. 삭제\n",
    "    cityscapes_train = Cityscapes(root=dataset_path,extra=CONFIG.USING_COARSE,split='train',transform=composed_transforms_tr)\n",
    "    cityscapes_train1 = Cityscapes(root=dataset_path,extra=CONFIG.USING_COARSE,split='train',transform=composed_transforms_tr1)\n",
    "    cityscapes_train2 = Cityscapes(root=dataset_path,extra=CONFIG.USING_COARSE,split='train',transform=composed_transforms_tr2)\n",
    "    cityscapes_train3 = Cityscapes(root=dataset_path,extra=CONFIG.USING_COARSE,split='train',transform=composed_transforms_tr3)\n",
    "    \n",
    "    cityscapes_val = Cityscapes(root=dataset_path,extra=CONFIG.USING_COARSE,split='val', transform=composed_transforms_ts)\n",
    "    \n",
    "#augment train, val data load , train batch ==4\n",
    "    trainloader = DataLoader(cityscapes_train, batch_size=p['trainBatch'], shuffle=True, num_workers=0)\n",
    "    trainloader1 = DataLoader(cityscapes_train1, batch_size=p['trainBatch'], shuffle=True, num_workers=0)\n",
    "    trainloader2 = DataLoader(cityscapes_train2, batch_size=p['trainBatch'], shuffle=True, num_workers=0)\n",
    "    trainloader3 = DataLoader(cityscapes_train3, batch_size=p['trainBatch'], shuffle=True, num_workers=0)\n",
    "\n",
    "\n",
    "    valloader = DataLoader(cityscapes_val, batch_size=testBatch, shuffle=True, num_workers=0)\n",
    "    \n",
    "    if CONFIG.USING_COARSE:#in case of training coarse data, I just used one scale to train. 상황에 따라 일반적인 tarin data를 쓸지, augment한 train data를 쓸지 정함 \n",
    "        loaders=[ trainloader ]\n",
    "    else:\n",
    "        loaders=[ trainloader ,trainloader1 ,trainloader2 ,trainloader3]\n",
    "    #generate 앞에 utils. 삭제\n",
    "    generate_param_report(os.path.join(save_path, exp_name + '.txt'), p)\n",
    "\n",
    "    num_img_tr = len(trainloader)\n",
    "    num_img_vl = len(valloader)\n",
    "    running_loss_tr = 0.0\n",
    "    running_loss_vl = 0.0\n",
    "    previous_miou = -1.0\n",
    "    global_step = 0\n",
    "    iev = Eval(20,19)\n",
    "    \n",
    "    # Main Training and Testing Loop 메인 학습 0부터 100까지\n",
    "    for epoch in range(resume_epoch, nEpochs):\n",
    "        start_time = timeit.default_timer()\n",
    "\n",
    "        if epoch % p['epoch_size'] == p['epoch_size'] - 1:\n",
    "            lr_ = lr_poly(p['lr'], epoch, nEpochs, 0.9)\n",
    "            print('(poly lr policy) learning rate: ', lr_)\n",
    "            optimizer = optim.SGD(net.parameters(), lr=lr_, momentum=p['momentum'], weight_decay=p['wd'])\n",
    "           \n",
    "        net.train()\n",
    "        for loader in loaders:\n",
    "            print(loader)\n",
    "            for ii, sample_batched in enumerate(loader):\n",
    "                \n",
    "    \n",
    "                inputs, labels = sample_batched['image'], sample_batched['label']\n",
    "                # Forward-Backward of the mini-batch\n",
    "                inputs, labels = Variable(inputs, requires_grad=True), Variable(labels)\n",
    "                #print('labels size', inputs.size() , labels.size())\n",
    "                global_step += inputs.data.shape[0]\n",
    "                #print(\"Glopal Step\",global_step)4,8,12,16\n",
    "                if CONFIG.USING_GPU:\n",
    "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = net.forward(inputs)\n",
    "                loss = criterion(outputs, labels,reduct='sum',weight=None)#sum\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                ls=loss.item()\n",
    "                running_loss_tr += ls\n",
    "    #            if ii% 10 == 0:\n",
    "    #                print(ls)\n",
    "                # Print stuff\n",
    "                if ii % num_img_tr == (num_img_tr - 1):\n",
    "                    running_loss_tr = running_loss_tr / num_img_tr\n",
    "                    writer.add_scalar('data/total_loss_epoch', running_loss_tr, epoch)\n",
    "                    print('[Epoch: %d, numImages: %5d]' % (epoch, ii * p['trainBatch'] + inputs.data.shape[0]))\n",
    "                    print('Loss: %f' % running_loss_tr)\n",
    "                    running_loss_tr = 0\n",
    "                    stop_time = timeit.default_timer()\n",
    "                    print(\"Execution time: \" + str(stop_time - start_time) + \"\\n\")\n",
    "       \n",
    "                \n",
    "                # Update the weights once in p['nAveGrad'] forward passes \n",
    "                writer.add_scalar('data/total_loss_iter', loss.item(), ii + num_img_tr * epoch)\n",
    "                \n",
    "    \n",
    "                # Show 10 * 3 images results each epoch\n",
    "                if ii % (num_img_tr // 10) == 0:\n",
    "                    grid_image = make_grid(inputs[:3].clone().cpu().data, 3, normalize=True)\n",
    "                    writer.add_image('Image', grid_image, global_step)\n",
    "                    grid_image = make_grid(\n",
    "                        decode_seg_map_sequence(torch.max(outputs[:3], 1)[1].detach().cpu().numpy(), 'cityscapes'), 3,\n",
    "                        normalize=False,\n",
    "                        range=(0, 255))\n",
    "                    writer.add_image('Predicted label', grid_image, global_step)\n",
    "                    grid_image = make_grid(\n",
    "                        decode_seg_map_sequence(torch.squeeze(labels[:3], 1).detach().cpu().numpy(), 'cityscapes'), 3,\n",
    "                        normalize=False, range=(0, 255))\n",
    "                    writer.add_image('Groundtruth label', grid_image, global_step)\n",
    "\n",
    "        # One testing epoch\n",
    "        if (epoch % nValInterval == (nValInterval - 1)) or epoch==0:\n",
    "            total_miou = 0.0\n",
    "            net.eval()\n",
    "            for ii, sample_batched in enumerate(valloader):\n",
    "                inputs, labels = sample_batched['image'], sample_batched['label']\n",
    "\n",
    "                # Forward pass of the mini-batch\n",
    "                inputs, labels = Variable(inputs, requires_grad=True), Variable(labels)\n",
    "                if CONFIG.USING_GPU:\n",
    "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    outputs = net.forward(inputs)\n",
    "\n",
    "                predictions = torch.max(outputs, 1)[1]\n",
    "\n",
    "                loss = criterion(outputs, labels,reduct='sum',weight=None)#sum elementwise_mean\n",
    "                running_loss_vl += loss.item()\n",
    "               \n",
    "                \n",
    "                y = torch.ones(labels.size()[2], labels.size()[3]).mul(19).cuda()\n",
    "                labels=labels.where(labels !=255, y)\n",
    "                \n",
    "                iev.addBatch(predictions.unsqueeze(1).data,labels)\n",
    "                \n",
    "                \n",
    "                # Print stuff\n",
    "                if ii % num_img_vl == num_img_vl - 1:\n",
    "                    miou=iev.getIoU()[0]\n",
    "                    running_loss_vl = running_loss_vl / num_img_vl\n",
    "                    print('Validation:')\n",
    "                    print('[Epoch: %d, numImages: %5d]' % (epoch, ii * testBatch + inputs.data.shape[0]))\n",
    "                    writer.add_scalar('data/test_loss_epoch', running_loss_vl, epoch)\n",
    "                    writer.add_scalar('data/test_miour', iev.getIoU()[0], epoch)\n",
    "                    print('Loss: %f' % running_loss_vl)\n",
    "                    print(\"Predi iou\",iev.getIoU())\n",
    "                    running_loss_vl = 0\n",
    "                    iev.reset()\n",
    "\n",
    "        # Save the model\n",
    "        if (epoch % snapshot) == snapshot - 1 :#and miou > previous_miou\n",
    "            previous_miou = miou\n",
    "            torch.save(net.state_dict(), os.path.join(save_path, 'models', modelName + '_epoch-' + str(epoch) + '.pth'))\n",
    "            print(\"Save model at {}\\n\".format(\n",
    "                os.path.join(save_path, 'models', modelName + '_epoch-' + str(epoch) + '.pth')))\n",
    "\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import brambox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sia",
   "language": "python",
   "name": "sia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
