{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "82524b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a7d09dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torchvision.models.utils import load_state_dict_from_url\n",
    "import torch.nn.functional as F\n",
    "\n",
    "__all__ = ['MobileNetV2', 'mobilenet_v2']\n",
    "\n",
    "\n",
    "model_urls = {\n",
    "    'mobilenet_v2': 'https://download.pytorch.org/models/mobilenet_v2-b0353104.pth',\n",
    "}\n",
    "\n",
    "\n",
    "def _make_divisible(v, divisor, min_value=None):\n",
    "    \"\"\"\n",
    "    This function is taken from the original tf repo.\n",
    "    It ensures that all layers have a channel number that is divisible by 8\n",
    "    It can be seen here:\n",
    "    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n",
    "    :param v:\n",
    "    :param divisor:\n",
    "    :param min_value:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if min_value is None:\n",
    "        min_value = divisor\n",
    "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if new_v < 0.9 * v:\n",
    "        new_v += divisor\n",
    "    return new_v\n",
    "\n",
    "\n",
    "class ConvBNReLU(nn.Sequential):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size=3, stride=1, dilation=1, groups=1):\n",
    "        #padding = (kernel_size - 1) // 2\n",
    "        super(ConvBNReLU, self).__init__(\n",
    "            nn.Conv2d(in_planes, out_planes, kernel_size, stride, 0, dilation=dilation, groups=groups, bias=False),\n",
    "            nn.BatchNorm2d(out_planes),\n",
    "            nn.ReLU6(inplace=True)\n",
    "        )\n",
    "\n",
    "def fixed_padding(kernel_size, dilation):\n",
    "    kernel_size_effective = kernel_size + (kernel_size - 1) * (dilation - 1)\n",
    "    pad_total = kernel_size_effective - 1\n",
    "    pad_beg = pad_total // 2\n",
    "    pad_end = pad_total - pad_beg\n",
    "    return (pad_beg, pad_end, pad_beg, pad_end) \n",
    "\n",
    "class InvertedResidual(nn.Module):\n",
    "    def __init__(self, inp, oup, stride, dilation, expand_ratio):\n",
    "        super(InvertedResidual, self).__init__()\n",
    "        self.stride = stride\n",
    "        assert stride in [1, 2]\n",
    "\n",
    "        hidden_dim = int(round(inp * expand_ratio))\n",
    "        self.use_res_connect = self.stride == 1 and inp == oup\n",
    "\n",
    "        layers = []\n",
    "        if expand_ratio != 1:\n",
    "            # pw\n",
    "            layers.append(ConvBNReLU(inp, hidden_dim, kernel_size=1))\n",
    "\n",
    "        layers.extend([\n",
    "            # dw\n",
    "            ConvBNReLU(hidden_dim, hidden_dim, stride=stride, dilation=dilation, groups=hidden_dim),\n",
    "            # pw-linear\n",
    "            nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(oup),\n",
    "        ])\n",
    "        self.conv = nn.Sequential(*layers)\n",
    "\n",
    "        self.input_padding = fixed_padding( 3, dilation )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_pad = F.pad(x, self.input_padding)\n",
    "        if self.use_res_connect:\n",
    "            return x + self.conv(x_pad)\n",
    "        else:\n",
    "            return self.conv(x_pad)\n",
    "\n",
    "class MobileNetV2(nn.Module):\n",
    "    def __init__(self, num_classes=1000, output_stride=8, width_mult=1.0, inverted_residual_setting=None, round_nearest=8):\n",
    "        \"\"\"\n",
    "        MobileNet V2 main class\n",
    "\n",
    "        Args:\n",
    "            num_classes (int): Number of classes\n",
    "            width_mult (float): Width multiplier - adjusts number of channels in each layer by this amount\n",
    "            inverted_residual_setting: Network structure\n",
    "            round_nearest (int): Round the number of channels in each layer to be a multiple of this number\n",
    "            Set to 1 to turn off rounding\n",
    "        \"\"\"\n",
    "        super(MobileNetV2, self).__init__()\n",
    "        block = InvertedResidual\n",
    "        input_channel = 32\n",
    "        last_channel = 1280\n",
    "        self.output_stride = output_stride\n",
    "        current_stride = 1\n",
    "        if inverted_residual_setting is None:\n",
    "            inverted_residual_setting = [\n",
    "                # t, c, n, s\n",
    "                [1, 16, 1, 1],\n",
    "                [6, 24, 2, 2],\n",
    "                [6, 32, 3, 2],\n",
    "                [6, 64, 4, 2],\n",
    "                [6, 96, 3, 1],\n",
    "                [6, 160, 3, 2],\n",
    "                [6, 320, 1, 1],\n",
    "            ]\n",
    "\n",
    "        # only check the first element, assuming user knows t,c,n,s are required\n",
    "        if len(inverted_residual_setting) == 0 or len(inverted_residual_setting[0]) != 4:\n",
    "            raise ValueError(\"inverted_residual_setting should be non-empty \"\n",
    "                             \"or a 4-element list, got {}\".format(inverted_residual_setting))\n",
    "\n",
    "        # building first layer\n",
    "        input_channel = _make_divisible(input_channel * width_mult, round_nearest)\n",
    "        self.last_channel = _make_divisible(last_channel * max(1.0, width_mult), round_nearest)\n",
    "        features = [ConvBNReLU(3, input_channel, stride=2)]\n",
    "        current_stride *= 2\n",
    "        dilation=1\n",
    "        previous_dilation = 1\n",
    "\n",
    "        # building inverted residual blocks\n",
    "        for t, c, n, s in inverted_residual_setting:\n",
    "            output_channel = _make_divisible(c * width_mult, round_nearest)\n",
    "            previous_dilation = dilation\n",
    "            if current_stride == output_stride:\n",
    "                stride = 1\n",
    "                dilation *= s\n",
    "            else:\n",
    "                stride = s\n",
    "                current_stride *= s\n",
    "            output_channel = int(c * width_mult)\n",
    "\n",
    "            for i in range(n):\n",
    "                if i==0:\n",
    "                    features.append(block(input_channel, output_channel, stride, previous_dilation, expand_ratio=t))\n",
    "                else:\n",
    "                    features.append(block(input_channel, output_channel, 1, dilation, expand_ratio=t))\n",
    "                input_channel = output_channel\n",
    "        # building last several layers\n",
    "        features.append(ConvBNReLU(input_channel, self.last_channel, kernel_size=1))\n",
    "        # make it nn.Sequential\n",
    "        self.features = nn.Sequential(*features)\n",
    "\n",
    "        # building classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(self.last_channel, num_classes),\n",
    "        )\n",
    "\n",
    "        # weight initialization\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.ones_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.mean([2, 3])\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def mobilenet_v2(pretrained=False, progress=True, **kwargs):\n",
    "    \"\"\"\n",
    "    Constructs a MobileNetV2 architecture from\n",
    "    `\"MobileNetV2: Inverted Residuals and Linear Bottlenecks\" <https://arxiv.org/abs/1801.04381>`_.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    model = MobileNetV2(**kwargs)\n",
    "    if pretrained:\n",
    "        state_dict = load_state_dict_from_url(model_urls['mobilenet_v2'],\n",
    "                                              progress=progress)\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7c88f693",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "\n",
    "class _SimpleSegmentationModel(nn.Module):\n",
    "    def __init__(self, backbone, classifier):\n",
    "        super(_SimpleSegmentationModel, self).__init__()\n",
    "        self.backbone = backbone\n",
    "        self.classifier = classifier\n",
    "        \n",
    "    def forward(self, x):\n",
    "        input_shape = x.shape[-2:]\n",
    "        print()\n",
    "        features = self.backbone(x)\n",
    "        x = self.classifier(features)\n",
    "        x = F.interpolate(x, size=input_shape, mode='bilinear', align_corners=False)\n",
    "        return x\n",
    "\n",
    "\n",
    "class IntermediateLayerGetter(nn.ModuleDict):\n",
    "    \"\"\"\n",
    "    Module wrapper that returns intermediate layers from a model\n",
    "\n",
    "    It has a strong assumption that the modules have been registered\n",
    "    into the model in the same order as they are used.\n",
    "    This means that one should **not** reuse the same nn.Module\n",
    "    twice in the forward if you want this to work.\n",
    "\n",
    "    Additionally, it is only able to query submodules that are directly\n",
    "    assigned to the model. So if `model` is passed, `model.feature1` can\n",
    "    be returned, but not `model.feature1.layer2`.\n",
    "\n",
    "    Arguments:\n",
    "        model (nn.Module): model on which we will extract the features\n",
    "        return_layers (Dict[name, new_name]): a dict containing the names\n",
    "            of the modules for which the activations will be returned as\n",
    "            the key of the dict, and the value of the dict is the name\n",
    "            of the returned activation (which the user can specify).\n",
    "\n",
    "    Examples::\n",
    "\n",
    "        >>> m = torchvision.models.resnet18(pretrained=True)\n",
    "        >>> # extract layer1 and layer3, giving as names `feat1` and feat2`\n",
    "        >>> new_m = torchvision.models._utils.IntermediateLayerGetter(m,\n",
    "        >>>     {'layer1': 'feat1', 'layer3': 'feat2'})\n",
    "        >>> out = new_m(torch.rand(1, 3, 224, 224))\n",
    "        >>> print([(k, v.shape) for k, v in out.items()])\n",
    "        >>>     [('feat1', torch.Size([1, 64, 56, 56])),\n",
    "        >>>      ('feat2', torch.Size([1, 256, 14, 14]))]\n",
    "    \"\"\"\n",
    "    def __init__(self, model, return_layers):\n",
    "        if not set(return_layers).issubset([name for name, _ in model.named_children()]):\n",
    "            raise ValueError(\"return_layers are not present in model\")\n",
    "\n",
    "        orig_return_layers = return_layers\n",
    "        return_layers = {k: v for k, v in return_layers.items()}\n",
    "        layers = OrderedDict()\n",
    "        for name, module in model.named_children():\n",
    "            layers[name] = module\n",
    "            if name in return_layers:\n",
    "                del return_layers[name]\n",
    "            if not return_layers:\n",
    "                break\n",
    "\n",
    "        super(IntermediateLayerGetter, self).__init__(layers)\n",
    "        self.return_layers = orig_return_layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = OrderedDict()\n",
    "        for name, module in self.named_children():\n",
    "            x = module(x)\n",
    "            if name in self.return_layers:\n",
    "                out_name = self.return_layers[name]\n",
    "                out[out_name] = x\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cbc22687",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "#from .utils import _SimpleSegmentationModel\n",
    "\n",
    "\n",
    "__all__ = [\"DeepLabV3\"]\n",
    "\n",
    "\n",
    "class DeepLabV3(_SimpleSegmentationModel):\n",
    "    \"\"\"\n",
    "    Implements DeepLabV3 model from\n",
    "    `\"Rethinking Atrous Convolution for Semantic Image Segmentation\"\n",
    "    <https://arxiv.org/abs/1706.05587>`_.\n",
    "\n",
    "    Arguments:\n",
    "        backbone (nn.Module): the network used to compute the features for the model.\n",
    "            The backbone should return an OrderedDict[Tensor], with the key being\n",
    "            \"out\" for the last feature map used, and \"aux\" if an auxiliary classifier\n",
    "            is used.\n",
    "        classifier (nn.Module): module that takes the \"out\" element returned from\n",
    "            the backbone and returns a dense prediction.\n",
    "        aux_classifier (nn.Module, optional): auxiliary classifier used during training\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "class DeepLabHeadV3Plus(nn.Module):\n",
    "    def __init__(self, in_channels, low_level_channels, num_classes, aspp_dilate=[12, 24, 36]):\n",
    "        super(DeepLabHeadV3Plus, self).__init__()\n",
    "        self.project = nn.Sequential( \n",
    "            nn.Conv2d(low_level_channels, 48, 1, bias=False),\n",
    "            nn.BatchNorm2d(48),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.aspp = ASPP(in_channels, aspp_dilate)\n",
    "#수정 사항 reverse separable conv\n",
    "        self.classifier = nn.Sequential(\n",
    "            # PointWise Conv            \n",
    "            nn.Conv2d( in_channels=304, out_channels=256, kernel_size=1, stride=1, padding=0, bias=False), \n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),            \n",
    "            # Separable Conv            \n",
    "            nn.Conv2d( in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=0, dilation=1, bias=False, groups=256 ),\n",
    "\n",
    "\n",
    "            nn.Conv2d(256, num_classes, 1)\n",
    "        )\n",
    "        self._init_weight()\n",
    "\n",
    "    def forward(self, feature):\n",
    "        low_level_feature = self.project( feature['low_level'] )\n",
    "        output_feature = self.aspp(feature['out'])\n",
    "        output_feature = F.interpolate(output_feature, size=low_level_feature.shape[2:], mode='bilinear', align_corners=False)\n",
    "        return self.classifier( torch.cat( [ low_level_feature, output_feature ], dim=1 ) )\n",
    "    \n",
    "    def _init_weight(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "class DeepLabHead(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes, aspp_dilate=[12, 24, 36]):\n",
    "        super(DeepLabHead, self).__init__()\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            ASPP(in_channels, aspp_dilate),\n",
    "            nn.Conv2d(256, 256, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, num_classes, 1)\n",
    "        )\n",
    "        self._init_weight()\n",
    "\n",
    "    def forward(self, feature):\n",
    "        return self.classifier( feature['out'] )\n",
    "\n",
    "    def _init_weight(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "class AtrousSeparableConvolution(nn.Module):\n",
    "    \"\"\" Atrous Separable Convolution\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size,\n",
    "                            stride=1, padding=0, dilation=1, bias=True):\n",
    "        super(AtrousSeparableConvolution, self).__init__()\n",
    "        self.body = nn.Sequential(\n",
    "            # Separable Conv\n",
    "            nn.Conv2d( in_channels, in_channels, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, bias=bias, groups=in_channels ),\n",
    "            # PointWise Conv\n",
    "            nn.Conv2d( in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=bias),\n",
    "        )\n",
    "        \n",
    "        self._init_weight()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.body(x)\n",
    "\n",
    "    def _init_weight(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "class ASPPConv(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels, dilation):\n",
    "        modules = [\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=dilation, dilation=dilation, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ]\n",
    "        super(ASPPConv, self).__init__(*modules)\n",
    "\n",
    "class ASPPPooling(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ASPPPooling, self).__init__(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(in_channels, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        size = x.shape[-2:]\n",
    "        x = super(ASPPPooling, self).forward(x)\n",
    "        return F.interpolate(x, size=size, mode='bilinear', align_corners=False)\n",
    "## rate 추가, 모듈 어펜드 추가, 5->6\n",
    "class ASPP(nn.Module):\n",
    "    def __init__(self, in_channels, atrous_rates):\n",
    "        super(ASPP, self).__init__()\n",
    "        out_channels = 256\n",
    "        modules = []\n",
    "        modules.append(nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)))\n",
    "\n",
    "        rate1, rate2, rate3, rate4 = tuple(atrous_rates)\n",
    "        modules.append(ASPPConv(in_channels, out_channels, rate1))\n",
    "        modules.append(ASPPConv(in_channels, out_channels, rate2))\n",
    "        modules.append(ASPPConv(in_channels, out_channels, rate3))\n",
    "        modules.append(ASPPConv(in_channels, out_channels, rate4))        \n",
    "        modules.append(ASPPPooling(in_channels, out_channels))\n",
    "\n",
    "        self.convs = nn.ModuleList(modules)\n",
    "\n",
    "        self.project = nn.Sequential(\n",
    "            nn.Conv2d(6 * out_channels, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.1),)\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = []\n",
    "        for conv in self.convs:\n",
    "            res.append(conv(x))\n",
    "        res = torch.cat(res, dim=1)\n",
    "        return self.project(res)\n",
    "\n",
    "\n",
    "\n",
    "def convert_to_separable_conv(module):\n",
    "    new_module = module\n",
    "    if isinstance(module, nn.Conv2d) and module.kernel_size[0]>1 and module.in_channels == 320 :\n",
    "        print('nn.Conv2d : ', nn.Conv2d)\n",
    "        #print('nn.Conv2d[0] : ', nn.Conv2d[0])\n",
    "        print('module.in_channels : ', module.in_channels)\n",
    "        #print('module.in_channels[0] : ', module.in_channels[0])\n",
    "        new_module = AtrousSeparableConvolution(module.in_channels,\n",
    "                                      module.out_channels, \n",
    "                                      module.kernel_size,\n",
    "                                      module.stride,\n",
    "                                      module.padding,\n",
    "                                      module.dilation,\n",
    "                                      module.bias)\n",
    "    for name, child in module.named_children():\n",
    "        new_module.add_module(name, convert_to_separable_conv(child))\n",
    "    return new_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1e673e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#    from .utils import IntermediateLayerGetter\n",
    "#    from ._deeplab import DeepLabHead, DeepLabHeadV3Plus, DeepLabV3\n",
    "#    from .backbone import resnet\n",
    "#    from .backbone import mobilenetv2\n",
    "\n",
    "def _segm_resnet(name, backbone_name, num_classes, output_stride, pretrained_backbone):\n",
    "\n",
    "    if output_stride==8:\n",
    "        replace_stride_with_dilation=[False, True, True]\n",
    "        aspp_dilate = [12, 24, 36]\n",
    "    else:\n",
    "        replace_stride_with_dilation=[False, False, True]\n",
    "        aspp_dilate = [6, 12, 18, 24]\n",
    "\n",
    "    backbone = resnet.__dict__[backbone_name](\n",
    "        pretrained=pretrained_backbone,\n",
    "        replace_stride_with_dilation=replace_stride_with_dilation)\n",
    "    \n",
    "    inplanes = 2048\n",
    "    low_level_planes = 256\n",
    "\n",
    "    if name=='deeplabv3plus':\n",
    "        return_layers = {'layer4': 'out', 'layer1': 'low_level'}\n",
    "        classifier = DeepLabHeadV3Plus(inplanes, low_level_planes, num_classes, aspp_dilate)\n",
    "    elif name=='deeplabv3':\n",
    "        return_layers = {'layer4': 'out'}\n",
    "        classifier = DeepLabHead(inplanes , num_classes, aspp_dilate)\n",
    "    backbone = IntermediateLayerGetter(backbone, return_layers=return_layers)\n",
    "\n",
    "    model = DeepLabV3(backbone, classifier)\n",
    "    return model\n",
    "##여기 추가\n",
    "def _segm_mobilenet(name, backbone_name, num_classes, output_stride, pretrained_backbone):\n",
    "    if output_stride==8:\n",
    "        aspp_dilate = [12, 24, 36]\n",
    "    else:\n",
    "        aspp_dilate = [6, 12, 18, 24]\n",
    "\n",
    "    backbone = mobilenet_v2(pretrained=pretrained_backbone, output_stride=output_stride)\n",
    "    \n",
    "    # rename layers\n",
    "    backbone.low_level_features = backbone.features[0:4]\n",
    "    backbone.high_level_features = backbone.features[4:-1]\n",
    "    backbone.features = None\n",
    "    backbone.classifier = None\n",
    "\n",
    "    inplanes = 320\n",
    "    low_level_planes = 24\n",
    "    \n",
    "    if name=='deeplabv3plus':\n",
    "        return_layers = {'high_level_features': 'out', 'low_level_features': 'low_level'}\n",
    "        classifier = DeepLabHeadV3Plus(inplanes, low_level_planes, num_classes, aspp_dilate)\n",
    "    elif name=='deeplabv3':\n",
    "        return_layers = {'high_level_features': 'out'}\n",
    "        classifier = DeepLabHead(inplanes , num_classes, aspp_dilate)\n",
    "    backbone = IntermediateLayerGetter(backbone, return_layers=return_layers)\n",
    "\n",
    "    model = DeepLabV3(backbone, classifier)\n",
    "    return model\n",
    "\n",
    "def _load_model(arch_type, backbone, num_classes, output_stride, pretrained_backbone):\n",
    "\n",
    "    if backbone=='mobilenetv2':\n",
    "        model = _segm_mobilenet(arch_type, backbone, num_classes, output_stride=output_stride, pretrained_backbone=pretrained_backbone)\n",
    "    elif backbone.startswith('resnet'):\n",
    "        model = _segm_resnet(arch_type, backbone, num_classes, output_stride=output_stride, pretrained_backbone=pretrained_backbone)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    return model\n",
    "\n",
    "\n",
    "# Deeplab v3\n",
    "\n",
    "def deeplabv3_resnet50(num_classes=21, output_stride=8, pretrained_backbone=True):\n",
    "    \"\"\"Constructs a DeepLabV3 model with a ResNet-50 backbone.\n",
    "\n",
    "    Args:\n",
    "        num_classes (int): number of classes.\n",
    "        output_stride (int): output stride for deeplab.\n",
    "        pretrained_backbone (bool): If True, use the pretrained backbone.\n",
    "    \"\"\"\n",
    "    return _load_model('deeplabv3', 'resnet50', num_classes, output_stride=output_stride, pretrained_backbone=pretrained_backbone)\n",
    "\n",
    "def deeplabv3_resnet101(num_classes=21, output_stride=8, pretrained_backbone=True):\n",
    "    \"\"\"Constructs a DeepLabV3 model with a ResNet-101 backbone.\n",
    "\n",
    "    Args:\n",
    "        num_classes (int): number of classes.\n",
    "        output_stride (int): output stride for deeplab.\n",
    "        pretrained_backbone (bool): If True, use the pretrained backbone.\n",
    "    \"\"\"\n",
    "    return _load_model('deeplabv3', 'resnet101', num_classes, output_stride=output_stride, pretrained_backbone=pretrained_backbone)\n",
    "\n",
    "def deeplabv3_mobilenet(num_classes=21, output_stride=8, pretrained_backbone=True, **kwargs):\n",
    "    \"\"\"Constructs a DeepLabV3 model with a MobileNetv2 backbone.\n",
    "\n",
    "    Args:\n",
    "        num_classes (int): number of classes.\n",
    "        output_stride (int): output stride for deeplab.\n",
    "        pretrained_backbone (bool): If True, use the pretrained backbone.\n",
    "    \"\"\"\n",
    "    return _load_model('deeplabv3', 'mobilenetv2', num_classes, output_stride=output_stride, pretrained_backbone=pretrained_backbone)\n",
    "\n",
    "\n",
    "# Deeplab v3+\n",
    "\n",
    "def deeplabv3plus_resnet50(num_classes=21, output_stride=8, pretrained_backbone=True):\n",
    "    \"\"\"Constructs a DeepLabV3 model with a ResNet-50 backbone.\n",
    "\n",
    "    Args:\n",
    "        num_classes (int): number of classes.\n",
    "        output_stride (int): output stride for deeplab.\n",
    "        pretrained_backbone (bool): If True, use the pretrained backbone.\n",
    "    \"\"\"\n",
    "    return _load_model('deeplabv3plus', 'resnet50', num_classes, output_stride=output_stride, pretrained_backbone=pretrained_backbone)\n",
    "\n",
    "\n",
    "def deeplabv3plus_resnet101(num_classes=21, output_stride=8, pretrained_backbone=True):\n",
    "    \"\"\"Constructs a DeepLabV3+ model with a ResNet-101 backbone.\n",
    "\n",
    "    Args:\n",
    "        num_classes (int): number of classes.\n",
    "        output_stride (int): output stride for deeplab.\n",
    "        pretrained_backbone (bool): If True, use the pretrained backbone.\n",
    "    \"\"\"\n",
    "    return _load_model('deeplabv3plus', 'resnet101', num_classes, output_stride=output_stride, pretrained_backbone=pretrained_backbone)\n",
    "\n",
    "\n",
    "def deeplabv3plus_mobilenet(num_classes=21, output_stride=16, pretrained_backbone=True):\n",
    "    \"\"\"Constructs a DeepLabV3+ model with a MobileNetv2 backbone.\n",
    "\n",
    "    Args:\n",
    "        num_classes (int): number of classes.\n",
    "        output_stride (int): output stride for deeplab.\n",
    "        pretrained_backbone (bool): If True, use the pretrained backbone.\n",
    "    \"\"\"\n",
    "    return _load_model('deeplabv3plus', 'mobilenetv2', num_classes, output_stride=output_stride, pretrained_backbone=pretrained_backbone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "de9e58cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "129f17c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CKPT_PATH =\"/home/aiffel-dj54/aiffel/siaiffel/DeepLabV3Plus-Pytorch-master/checkpoints/best_deeplabv3plus_mobilenet_satellites_multi_os16.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "41f413fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import network\n",
    "model_map = {\n",
    "        'deeplabv3_resnet50': deeplabv3_resnet50,\n",
    "        'deeplabv3plus_resnet50': deeplabv3plus_resnet50,\n",
    "        'deeplabv3_resnet101': deeplabv3_resnet101,\n",
    "        'deeplabv3plus_resnet101': deeplabv3plus_resnet101,\n",
    "        'deeplabv3_mobilenet': deeplabv3_mobilenet,\n",
    "        'deeplabv3plus_mobilenet': deeplabv3plus_mobilenet\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6501e60d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nn.Conv2d :  <class 'torch.nn.modules.conv.Conv2d'>\n",
      "module.in_channels :  320\n",
      "nn.Conv2d :  <class 'torch.nn.modules.conv.Conv2d'>\n",
      "module.in_channels :  320\n",
      "nn.Conv2d :  <class 'torch.nn.modules.conv.Conv2d'>\n",
      "module.in_channels :  320\n",
      "nn.Conv2d :  <class 'torch.nn.modules.conv.Conv2d'>\n",
      "module.in_channels :  320\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeepLabHeadV3Plus(\n",
       "  (project): Sequential(\n",
       "    (0): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (aspp): ASPP(\n",
       "    (convs): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(320, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): ASPPConv(\n",
       "        (0): AtrousSeparableConvolution(\n",
       "          (body): Sequential(\n",
       "            (0): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(6, 6), dilation=(6, 6), groups=320, bias=False)\n",
       "            (1): Conv2d(320, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): ASPPConv(\n",
       "        (0): AtrousSeparableConvolution(\n",
       "          (body): Sequential(\n",
       "            (0): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(12, 12), dilation=(12, 12), groups=320, bias=False)\n",
       "            (1): Conv2d(320, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): ASPPConv(\n",
       "        (0): AtrousSeparableConvolution(\n",
       "          (body): Sequential(\n",
       "            (0): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(18, 18), dilation=(18, 18), groups=320, bias=False)\n",
       "            (1): Conv2d(320, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): ASPPConv(\n",
       "        (0): AtrousSeparableConvolution(\n",
       "          (body): Sequential(\n",
       "            (0): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(24, 24), dilation=(24, 24), groups=320, bias=False)\n",
       "            (1): Conv2d(320, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): ASPPPooling(\n",
       "        (0): AdaptiveAvgPool2d(output_size=1)\n",
       "        (1): Conv2d(320, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (project): Sequential(\n",
       "      (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Conv2d(304, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), groups=256, bias=False)\n",
       "    (4): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1 = model_map['deeplabv3plus_mobilenet'](num_classes=3, output_stride=16)\n",
    "convert_to_separable_conv(model_1.classifier)\n",
    "#model.load_state_dict(checkpoint[\"model_state\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6d5e6a1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeepLabV3(\n",
       "  (backbone): IntermediateLayerGetter(\n",
       "    (low_level_features): Sequential(\n",
       "      (0): ConvBNReLU(\n",
       "        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6(inplace=True)\n",
       "      )\n",
       "      (1): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNReLU(\n",
       "            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), groups=32, bias=False)\n",
       "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (2): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNReLU(\n",
       "            (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): ConvBNReLU(\n",
       "            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), groups=96, bias=False)\n",
       "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (3): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNReLU(\n",
       "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): ConvBNReLU(\n",
       "            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), groups=144, bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (high_level_features): Sequential(\n",
       "      (4): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNReLU(\n",
       "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): ConvBNReLU(\n",
       "            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), groups=144, bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (5): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNReLU(\n",
       "            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): ConvBNReLU(\n",
       "            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), groups=192, bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (6): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNReLU(\n",
       "            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): ConvBNReLU(\n",
       "            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), groups=192, bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (7): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNReLU(\n",
       "            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): ConvBNReLU(\n",
       "            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), groups=192, bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (8): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNReLU(\n",
       "            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): ConvBNReLU(\n",
       "            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), groups=384, bias=False)\n",
       "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (9): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNReLU(\n",
       "            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): ConvBNReLU(\n",
       "            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), groups=384, bias=False)\n",
       "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (10): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNReLU(\n",
       "            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): ConvBNReLU(\n",
       "            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), groups=384, bias=False)\n",
       "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (11): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNReLU(\n",
       "            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): ConvBNReLU(\n",
       "            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), groups=384, bias=False)\n",
       "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (12): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNReLU(\n",
       "            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): ConvBNReLU(\n",
       "            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), groups=576, bias=False)\n",
       "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (13): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNReLU(\n",
       "            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): ConvBNReLU(\n",
       "            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), groups=576, bias=False)\n",
       "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (14): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNReLU(\n",
       "            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): ConvBNReLU(\n",
       "            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), groups=576, bias=False)\n",
       "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (15): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNReLU(\n",
       "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): ConvBNReLU(\n",
       "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), dilation=(2, 2), groups=960, bias=False)\n",
       "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (16): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNReLU(\n",
       "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): ConvBNReLU(\n",
       "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), dilation=(2, 2), groups=960, bias=False)\n",
       "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (17): InvertedResidual(\n",
       "        (conv): Sequential(\n",
       "          (0): ConvBNReLU(\n",
       "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): ConvBNReLU(\n",
       "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), dilation=(2, 2), groups=960, bias=False)\n",
       "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): DeepLabHeadV3Plus(\n",
       "    (project): Sequential(\n",
       "      (0): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (aspp): ASPP(\n",
       "      (convs): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): Conv2d(320, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): ASPPConv(\n",
       "          (0): AtrousSeparableConvolution(\n",
       "            (body): Sequential(\n",
       "              (0): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(6, 6), dilation=(6, 6), groups=320, bias=False)\n",
       "              (1): Conv2d(320, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "          )\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): ASPPConv(\n",
       "          (0): AtrousSeparableConvolution(\n",
       "            (body): Sequential(\n",
       "              (0): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(12, 12), dilation=(12, 12), groups=320, bias=False)\n",
       "              (1): Conv2d(320, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "          )\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): ASPPConv(\n",
       "          (0): AtrousSeparableConvolution(\n",
       "            (body): Sequential(\n",
       "              (0): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(18, 18), dilation=(18, 18), groups=320, bias=False)\n",
       "              (1): Conv2d(320, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "          )\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): ASPPConv(\n",
       "          (0): AtrousSeparableConvolution(\n",
       "            (body): Sequential(\n",
       "              (0): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(24, 24), dilation=(24, 24), groups=320, bias=False)\n",
       "              (1): Conv2d(320, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "          )\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): ASPPPooling(\n",
       "          (0): AdaptiveAvgPool2d(output_size=1)\n",
       "          (1): Conv2d(320, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (3): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (project): Sequential(\n",
       "        (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (classifier): Sequential(\n",
       "      (0): Conv2d(304, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), groups=256, bias=False)\n",
       "      (4): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3263a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "log_dir = \"./runs/new_one\"\n",
    "writer = SummaryWriter(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7453fc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "import torchvision.transforms.functional as F\n",
    "import random \n",
    "import numbers\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "#\n",
    "#  Extended Transforms for Semantic Segmentation\n",
    "#\n",
    "class ExtRandomHorizontalFlip(object):\n",
    "    \"\"\"Horizontally flip the given PIL Image randomly with a given probability.\n",
    "\n",
    "    Args:\n",
    "        p (float): probability of the image being flipped. Default value is 0.5\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, img, lbl):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (PIL Image): Image to be flipped.\n",
    "\n",
    "        Returns:\n",
    "            PIL Image: Randomly flipped image.\n",
    "        \"\"\"\n",
    "        if random.random() < self.p:\n",
    "            return F.hflip(img), F.hflip(lbl)\n",
    "        return img, lbl\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(p={})'.format(self.p)\n",
    "\n",
    "\n",
    "\n",
    "class ExtCompose(object):\n",
    "    \"\"\"Composes several transforms together.\n",
    "    Args:\n",
    "        transforms (list of ``Transform`` objects): list of transforms to compose.\n",
    "    Example:\n",
    "        >>> transforms.Compose([\n",
    "        >>>     transforms.CenterCrop(10),\n",
    "        >>>     transforms.ToTensor(),\n",
    "        >>> ])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, img, lbl):\n",
    "        for t in self.transforms:\n",
    "            img, lbl = t(img, lbl)\n",
    "        return img, lbl\n",
    "\n",
    "    def __repr__(self):\n",
    "        format_string = self.__class__.__name__ + '('\n",
    "        for t in self.transforms:\n",
    "            format_string += '\\n'\n",
    "            format_string += '    {0}'.format(t)\n",
    "        format_string += '\\n)'\n",
    "        return format_string\n",
    "\n",
    "\n",
    "class ExtCenterCrop(object):\n",
    "    \"\"\"Crops the given PIL Image at the center.\n",
    "    Args:\n",
    "        size (sequence or int): Desired output size of the crop. If size is an\n",
    "            int instead of sequence like (h, w), a square crop (size, size) is\n",
    "            made.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size):\n",
    "        if isinstance(size, numbers.Number):\n",
    "            self.size = (int(size), int(size))\n",
    "        else:\n",
    "            self.size = size\n",
    "\n",
    "    def __call__(self, img, lbl):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (PIL Image): Image to be cropped.\n",
    "        Returns:\n",
    "            PIL Image: Cropped image.\n",
    "        \"\"\"\n",
    "        return F.center_crop(img, self.size), F.center_crop(lbl, self.size)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(size={0})'.format(self.size)\n",
    "\n",
    "\n",
    "class ExtRandomScale(object):\n",
    "    def __init__(self, scale_range, interpolation=Image.BILINEAR):\n",
    "        self.scale_range = scale_range\n",
    "        self.interpolation = interpolation\n",
    "\n",
    "    def __call__(self, img, lbl):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (PIL Image): Image to be scaled.\n",
    "            lbl (PIL Image): Label to be scaled.\n",
    "        Returns:\n",
    "            PIL Image: Rescaled image.\n",
    "            PIL Image: Rescaled label.\n",
    "        \"\"\"\n",
    "        assert img.size == lbl.size\n",
    "        scale = random.uniform(self.scale_range[0], self.scale_range[1])\n",
    "        target_size = ( int(img.size[1]*scale), int(img.size[0]*scale) )\n",
    "        return F.resize(img, target_size, self.interpolation), F.resize(lbl, target_size, Image.NEAREST)\n",
    "\n",
    "    def __repr__(self):\n",
    "        interpolate_str = _pil_interpolation_to_str[self.interpolation]\n",
    "        return self.__class__.__name__ + '(size={0}, interpolation={1})'.format(self.size, interpolate_str)\n",
    "\n",
    "class ExtScale(object):\n",
    "    \"\"\"Resize the input PIL Image to the given scale.\n",
    "    Args:\n",
    "        Scale (sequence or int): scale factors\n",
    "        interpolation (int, optional): Desired interpolation. Default is\n",
    "            ``PIL.Image.BILINEAR``\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, scale, interpolation=Image.BILINEAR):\n",
    "        self.scale = scale\n",
    "        self.interpolation = interpolation\n",
    "\n",
    "    def __call__(self, img, lbl):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (PIL Image): Image to be scaled.\n",
    "            lbl (PIL Image): Label to be scaled.\n",
    "        Returns:\n",
    "            PIL Image: Rescaled image.\n",
    "            PIL Image: Rescaled label.\n",
    "        \"\"\"\n",
    "        assert img.size == lbl.size\n",
    "        target_size = ( int(img.size[1]*self.scale), int(img.size[0]*self.scale) ) # (H, W)\n",
    "        return F.resize(img, target_size, self.interpolation), F.resize(lbl, target_size, Image.NEAREST)\n",
    "\n",
    "    def __repr__(self):\n",
    "        interpolate_str = _pil_interpolation_to_str[self.interpolation]\n",
    "        return self.__class__.__name__ + '(size={0}, interpolation={1})'.format(self.size, interpolate_str)\n",
    "\n",
    "\n",
    "class ExtRandomRotation(object):\n",
    "    \"\"\"Rotate the image by angle.\n",
    "    Args:\n",
    "        degrees (sequence or float or int): Range of degrees to select from.\n",
    "            If degrees is a number instead of sequence like (min, max), the range of degrees\n",
    "            will be (-degrees, +degrees).\n",
    "        resample ({PIL.Image.NEAREST, PIL.Image.BILINEAR, PIL.Image.BICUBIC}, optional):\n",
    "            An optional resampling filter.\n",
    "            See http://pillow.readthedocs.io/en/3.4.x/handbook/concepts.html#filters\n",
    "            If omitted, or if the image has mode \"1\" or \"P\", it is set to PIL.Image.NEAREST.\n",
    "        expand (bool, optional): Optional expansion flag.\n",
    "            If true, expands the output to make it large enough to hold the entire rotated image.\n",
    "            If false or omitted, make the output image the same size as the input image.\n",
    "            Note that the expand flag assumes rotation around the center and no translation.\n",
    "        center (2-tuple, optional): Optional center of rotation.\n",
    "            Origin is the upper left corner.\n",
    "            Default is the center of the image.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, degrees, resample=False, expand=False, center=None):\n",
    "        if isinstance(degrees, numbers.Number):\n",
    "            if degrees < 0:\n",
    "                raise ValueError(\"If degrees is a single number, it must be positive.\")\n",
    "            self.degrees = (-degrees, degrees)\n",
    "        else:\n",
    "            if len(degrees) != 2:\n",
    "                raise ValueError(\"If degrees is a sequence, it must be of len 2.\")\n",
    "            self.degrees = degrees\n",
    "\n",
    "        self.resample = resample\n",
    "        self.expand = expand\n",
    "        self.center = center\n",
    "\n",
    "    @staticmethod\n",
    "    def get_params(degrees):\n",
    "        \"\"\"Get parameters for ``rotate`` for a random rotation.\n",
    "        Returns:\n",
    "            sequence: params to be passed to ``rotate`` for random rotation.\n",
    "        \"\"\"\n",
    "        angle = random.uniform(degrees[0], degrees[1])\n",
    "\n",
    "        return angle\n",
    "\n",
    "    def __call__(self, img, lbl):\n",
    "        \"\"\"\n",
    "            img (PIL Image): Image to be rotated.\n",
    "            lbl (PIL Image): Label to be rotated.\n",
    "        Returns:\n",
    "            PIL Image: Rotated image.\n",
    "            PIL Image: Rotated label.\n",
    "        \"\"\"\n",
    "\n",
    "        angle = self.get_params(self.degrees)\n",
    "\n",
    "        return F.rotate(img, angle, self.resample, self.expand, self.center), F.rotate(lbl, angle, self.resample, self.expand, self.center)\n",
    "\n",
    "    def __repr__(self):\n",
    "        format_string = self.__class__.__name__ + '(degrees={0}'.format(self.degrees)\n",
    "        format_string += ', resample={0}'.format(self.resample)\n",
    "        format_string += ', expand={0}'.format(self.expand)\n",
    "        if self.center is not None:\n",
    "            format_string += ', center={0}'.format(self.center)\n",
    "        format_string += ')'\n",
    "        return format_string\n",
    "\n",
    "class ExtRandomHorizontalFlip(object):\n",
    "    \"\"\"Horizontally flip the given PIL Image randomly with a given probability.\n",
    "    Args:\n",
    "        p (float): probability of the image being flipped. Default value is 0.5\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, img, lbl):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (PIL Image): Image to be flipped.\n",
    "        Returns:\n",
    "            PIL Image: Randomly flipped image.\n",
    "        \"\"\"\n",
    "        if random.random() < self.p:\n",
    "            return F.hflip(img), F.hflip(lbl)\n",
    "        return img, lbl\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(p={})'.format(self.p)\n",
    "\n",
    "\n",
    "class ExtRandomVerticalFlip(object):\n",
    "    \"\"\"Vertically flip the given PIL Image randomly with a given probability.\n",
    "    Args:\n",
    "        p (float): probability of the image being flipped. Default value is 0.5\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, img, lbl):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (PIL Image): Image to be flipped.\n",
    "            lbl (PIL Image): Label to be flipped.\n",
    "        Returns:\n",
    "            PIL Image: Randomly flipped image.\n",
    "            PIL Image: Randomly flipped label.\n",
    "        \"\"\"\n",
    "        if random.random() < self.p:\n",
    "            return F.vflip(img), F.vflip(lbl)\n",
    "        return img, lbl\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(p={})'.format(self.p)\n",
    "\n",
    "class ExtPad(object):\n",
    "    def __init__(self, diviser=32):\n",
    "        self.diviser = diviser\n",
    "    \n",
    "    def __call__(self, img, lbl):\n",
    "        h, w = img.size\n",
    "        ph = (h//32+1)*32 - h if h%32!=0 else 0\n",
    "        pw = (w//32+1)*32 - w if w%32!=0 else 0\n",
    "        im = F.pad(img, ( pw//2, pw-pw//2, ph//2, ph-ph//2) )\n",
    "        lbl = F.pad(lbl, ( pw//2, pw-pw//2, ph//2, ph-ph//2))\n",
    "        return im, lbl\n",
    "\n",
    "class ExtToTensor(object):\n",
    "    \"\"\"Convert a ``PIL Image`` or ``numpy.ndarray`` to tensor.\n",
    "    Converts a PIL Image or numpy.ndarray (H x W x C) in the range\n",
    "    [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0].\n",
    "    \"\"\"\n",
    "    def __init__(self, normalize=True, target_type='uint8'):\n",
    "        self.normalize = normalize\n",
    "        self.target_type = target_type\n",
    "    def __call__(self, pic, lbl):\n",
    "        \"\"\"\n",
    "        Note that labels will not be normalized to [0, 1].\n",
    "        Args:\n",
    "            pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\n",
    "            lbl (PIL Image or numpy.ndarray): Label to be converted to tensor. \n",
    "        Returns:\n",
    "            Tensor: Converted image and label\n",
    "        \"\"\"\n",
    "        if self.normalize:\n",
    "            return F.to_tensor(pic), torch.from_numpy( np.array( lbl, dtype=self.target_type) )\n",
    "        else:\n",
    "            return torch.from_numpy( np.array( pic, dtype=np.float32).transpose(2, 0, 1) ), torch.from_numpy( np.array( lbl, dtype=self.target_type) )\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '()'\n",
    "\n",
    "class ExtNormalize(object):\n",
    "    \"\"\"Normalize a tensor image with mean and standard deviation.\n",
    "    Given mean: ``(M1,...,Mn)`` and std: ``(S1,..,Sn)`` for ``n`` channels, this transform\n",
    "    will normalize each channel of the input ``torch.*Tensor`` i.e.\n",
    "    ``input[channel] = (input[channel] - mean[channel]) / std[channel]``\n",
    "    Args:\n",
    "        mean (sequence): Sequence of means for each channel.\n",
    "        std (sequence): Sequence of standard deviations for each channel.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor, lbl):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n",
    "            tensor (Tensor): Tensor of label. A dummy input for ExtCompose\n",
    "        Returns:\n",
    "            Tensor: Normalized Tensor image.\n",
    "            Tensor: Unchanged Tensor label\n",
    "        \"\"\"\n",
    "        return F.normalize(tensor, self.mean, self.std), lbl\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)\n",
    "\n",
    "\n",
    "class ExtRandomCrop(object):\n",
    "    \"\"\"Crop the given PIL Image at a random location.\n",
    "    Args:\n",
    "        size (sequence or int): Desired output size of the crop. If size is an\n",
    "            int instead of sequence like (h, w), a square crop (size, size) is\n",
    "            made.\n",
    "        padding (int or sequence, optional): Optional padding on each border\n",
    "            of the image. Default is 0, i.e no padding. If a sequence of length\n",
    "            4 is provided, it is used to pad left, top, right, bottom borders\n",
    "            respectively.\n",
    "        pad_if_needed (boolean): It will pad the image if smaller than the\n",
    "            desired size to avoid raising an exception.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, padding=0, pad_if_needed=False):\n",
    "        if isinstance(size, numbers.Number):\n",
    "            self.size = (int(size), int(size))\n",
    "        else:\n",
    "            self.size = size\n",
    "        self.padding = padding\n",
    "        self.pad_if_needed = pad_if_needed\n",
    "\n",
    "    @staticmethod\n",
    "    def get_params(img, output_size):\n",
    "        \"\"\"Get parameters for ``crop`` for a random crop.\n",
    "        Args:\n",
    "            img (PIL Image): Image to be cropped.\n",
    "            output_size (tuple): Expected output size of the crop.\n",
    "        Returns:\n",
    "            tuple: params (i, j, h, w) to be passed to ``crop`` for random crop.\n",
    "        \"\"\"\n",
    "        w, h = img.size\n",
    "        th, tw = output_size\n",
    "        if w == tw and h == th:\n",
    "            return 0, 0, h, w\n",
    "\n",
    "        i = random.randint(0, h - th)\n",
    "        j = random.randint(0, w - tw)\n",
    "        return i, j, th, tw\n",
    "\n",
    "    def __call__(self, img, lbl):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (PIL Image): Image to be cropped.\n",
    "            lbl (PIL Image): Label to be cropped.\n",
    "        Returns:\n",
    "            PIL Image: Cropped image.\n",
    "            PIL Image: Cropped label.\n",
    "        \"\"\"\n",
    "        assert img.size == lbl.size, 'size of img and lbl should be the same. %s, %s'%(img.size, lbl.size)\n",
    "        if self.padding > 0:\n",
    "            img = F.pad(img, self.padding)\n",
    "            lbl = F.pad(lbl, self.padding)\n",
    "\n",
    "        # pad the width if needed\n",
    "        if self.pad_if_needed and img.size[0] < self.size[1]:\n",
    "            img = F.pad(img, padding=int((1 + self.size[1] - img.size[0]) / 2))\n",
    "            lbl = F.pad(lbl, padding=int((1 + self.size[1] - lbl.size[0]) / 2))\n",
    "\n",
    "        # pad the height if needed\n",
    "        if self.pad_if_needed and img.size[1] < self.size[0]:\n",
    "            img = F.pad(img, padding=int((1 + self.size[0] - img.size[1]) / 2))\n",
    "            lbl = F.pad(lbl, padding=int((1 + self.size[0] - lbl.size[1]) / 2))\n",
    "\n",
    "        i, j, h, w = self.get_params(img, self.size)\n",
    "\n",
    "        return F.crop(img, i, j, h, w), F.crop(lbl, i, j, h, w)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(size={0}, padding={1})'.format(self.size, self.padding)\n",
    "\n",
    "\n",
    "class ExtResize(object):\n",
    "    \"\"\"Resize the input PIL Image to the given size.\n",
    "    Args:\n",
    "        size (sequence or int): Desired output size. If size is a sequence like\n",
    "            (h, w), output size will be matched to this. If size is an int,\n",
    "            smaller edge of the image will be matched to this number.\n",
    "            i.e, if height > width, then image will be rescaled to\n",
    "            (size * height / width, size)\n",
    "        interpolation (int, optional): Desired interpolation. Default is\n",
    "            ``PIL.Image.BILINEAR``\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, interpolation=Image.BILINEAR):\n",
    "        assert isinstance(size, int) or (isinstance(size, collections.Iterable) and len(size) == 2)\n",
    "        self.size = size\n",
    "        self.interpolation = interpolation\n",
    "\n",
    "    def __call__(self, img, lbl):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (PIL Image): Image to be scaled.\n",
    "        Returns:\n",
    "            PIL Image: Rescaled image.\n",
    "        \"\"\"\n",
    "        return F.resize(img, self.size, self.interpolation), F.resize(lbl, self.size, Image.NEAREST)\n",
    "\n",
    "    def __repr__(self):\n",
    "        interpolate_str = _pil_interpolation_to_str[self.interpolation]\n",
    "        return self.__class__.__name__ + '(size={0}, interpolation={1})'.format(self.size, interpolate_str) \n",
    "    \n",
    "class ExtColorJitter(object):\n",
    "    \"\"\"Randomly change the brightness, contrast and saturation of an image.\n",
    "\n",
    "    Args:\n",
    "        brightness (float or tuple of float (min, max)): How much to jitter brightness.\n",
    "            brightness_factor is chosen uniformly from [max(0, 1 - brightness), 1 + brightness]\n",
    "            or the given [min, max]. Should be non negative numbers.\n",
    "        contrast (float or tuple of float (min, max)): How much to jitter contrast.\n",
    "            contrast_factor is chosen uniformly from [max(0, 1 - contrast), 1 + contrast]\n",
    "            or the given [min, max]. Should be non negative numbers.\n",
    "        saturation (float or tuple of float (min, max)): How much to jitter saturation.\n",
    "            saturation_factor is chosen uniformly from [max(0, 1 - saturation), 1 + saturation]\n",
    "            or the given [min, max]. Should be non negative numbers.\n",
    "        hue (float or tuple of float (min, max)): How much to jitter hue.\n",
    "            hue_factor is chosen uniformly from [-hue, hue] or the given [min, max].\n",
    "            Should have 0<= hue <= 0.5 or -0.5 <= min <= max <= 0.5.\n",
    "    \"\"\"\n",
    "    def __init__(self, brightness=0, contrast=0, saturation=0, hue=0):\n",
    "        self.brightness = self._check_input(brightness, 'brightness')\n",
    "        self.contrast = self._check_input(contrast, 'contrast')\n",
    "        self.saturation = self._check_input(saturation, 'saturation')\n",
    "        self.hue = self._check_input(hue, 'hue', center=0, bound=(-0.5, 0.5),\n",
    "                                     clip_first_on_zero=False)\n",
    "\n",
    "    def _check_input(self, value, name, center=1, bound=(0, float('inf')), clip_first_on_zero=True):\n",
    "        if isinstance(value, numbers.Number):\n",
    "            if value < 0:\n",
    "                raise ValueError(\"If {} is a single number, it must be non negative.\".format(name))\n",
    "            value = [center - value, center + value]\n",
    "            if clip_first_on_zero:\n",
    "                value[0] = max(value[0], 0)\n",
    "        elif isinstance(value, (tuple, list)) and len(value) == 2:\n",
    "            if not bound[0] <= value[0] <= value[1] <= bound[1]:\n",
    "                raise ValueError(\"{} values should be between {}\".format(name, bound))\n",
    "        else:\n",
    "            raise TypeError(\"{} should be a single number or a list/tuple with lenght 2.\".format(name))\n",
    "\n",
    "        # if value is 0 or (1., 1.) for brightness/contrast/saturation\n",
    "        # or (0., 0.) for hue, do nothing\n",
    "        if value[0] == value[1] == center:\n",
    "            value = None\n",
    "        return value\n",
    "\n",
    "    @staticmethod\n",
    "    def get_params(brightness, contrast, saturation, hue):\n",
    "        \"\"\"Get a randomized transform to be applied on image.\n",
    "\n",
    "        Arguments are same as that of __init__.\n",
    "\n",
    "        Returns:\n",
    "            Transform which randomly adjusts brightness, contrast and\n",
    "            saturation in a random order.\n",
    "        \"\"\"\n",
    "        transforms = []\n",
    "\n",
    "        if brightness is not None:\n",
    "            brightness_factor = random.uniform(brightness[0], brightness[1])\n",
    "            transforms.append(Lambda(lambda img: F.adjust_brightness(img, brightness_factor)))\n",
    "\n",
    "        if contrast is not None:\n",
    "            contrast_factor = random.uniform(contrast[0], contrast[1])\n",
    "            transforms.append(Lambda(lambda img: F.adjust_contrast(img, contrast_factor)))\n",
    "\n",
    "        if saturation is not None:\n",
    "            saturation_factor = random.uniform(saturation[0], saturation[1])\n",
    "            transforms.append(Lambda(lambda img: F.adjust_saturation(img, saturation_factor)))\n",
    "\n",
    "        if hue is not None:\n",
    "            hue_factor = random.uniform(hue[0], hue[1])\n",
    "            transforms.append(Lambda(lambda img: F.adjust_hue(img, hue_factor)))\n",
    "\n",
    "        random.shuffle(transforms)\n",
    "        transform = Compose(transforms)\n",
    "\n",
    "        return transform\n",
    "\n",
    "    def __call__(self, img, lbl):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (PIL Image): Input image.\n",
    "\n",
    "        Returns:\n",
    "            PIL Image: Color jittered image.\n",
    "        \"\"\"\n",
    "        transform = self.get_params(self.brightness, self.contrast,\n",
    "                                    self.saturation, self.hue)\n",
    "        return transform(img), lbl\n",
    "\n",
    "    def __repr__(self):\n",
    "        format_string = self.__class__.__name__ + '('\n",
    "        format_string += 'brightness={0}'.format(self.brightness)\n",
    "        format_string += ', contrast={0}'.format(self.contrast)\n",
    "        format_string += ', saturation={0}'.format(self.saturation)\n",
    "        format_string += ', hue={0})'.format(self.hue)\n",
    "        return format_string\n",
    "\n",
    "class Lambda(object):\n",
    "    \"\"\"Apply a user-defined lambda as a transform.\n",
    "\n",
    "    Args:\n",
    "        lambd (function): Lambda/function to be used for transform.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lambd):\n",
    "        assert callable(lambd), repr(type(lambd).__name__) + \" object is not callable\"\n",
    "        self.lambd = lambd\n",
    "\n",
    "    def __call__(self, img):\n",
    "        return self.lambd(img)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '()'\n",
    "\n",
    "\n",
    "class Compose(object):\n",
    "    \"\"\"Composes several transforms together.\n",
    "\n",
    "    Args:\n",
    "        transforms (list of ``Transform`` objects): list of transforms to compose.\n",
    "\n",
    "    Example:\n",
    "        >>> transforms.Compose([\n",
    "        >>>     transforms.CenterCrop(10),\n",
    "        >>>     transforms.ToTensor(),\n",
    "        >>> ])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, img):\n",
    "        for t in self.transforms:\n",
    "            img = t(img)\n",
    "        return img\n",
    "\n",
    "    def __repr__(self):\n",
    "        format_string = self.__class__.__name__ + '('\n",
    "        for t in self.transforms:\n",
    "            format_string += '\\n'\n",
    "            format_string += '    {0}'.format(t)\n",
    "        format_string += '\\n)'\n",
    "        return format_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb563f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from collections import namedtuple\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class Satellites(data.Dataset):\n",
    "    \"\"\"Cityscapes <http://www.cityscapes-dataset.com/> Dataset.\n",
    "    \n",
    "    **Parameters:**\n",
    "        - **root** (string): Root directory of dataset where directory 'leftImg8bit' and 'gtFine' or 'gtCoarse' are located.\n",
    "        - **split** (string, optional): The image split to use, 'train', 'test' or 'val' if mode=\"gtFine\" otherwise 'train', 'train_extra' or 'val'\n",
    "        - **mode** (string, optional): The quality mode to use, 'gtFine' or 'gtCoarse' or 'color'. Can also be a list to output a tuple with all specified target types.\n",
    "        - **transform** (callable, optional): A function/transform that takes in a PIL image and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
    "        - **target_transform** (callable, optional): A function/transform that takes in the target and transforms it.\n",
    "    \"\"\"\n",
    "\n",
    "    # Based on sia\n",
    "    SatellitesClass = namedtuple('SatellitesClass', ['name', 'id', 'train_id', 'category', 'category_id',\n",
    "                                                     'has_instances', 'ignore_in_eval', 'color'])\n",
    "    #class\n",
    "    classes = [\n",
    "        SatellitesClass('unlabeled',            0, 254, 'void', 0, False, True, (0, 0, 0)),\n",
    "        SatellitesClass('road',                 1, 2, 'flat', 1, False, False, (128, 64, 128)),\n",
    "        SatellitesClass('building',             2, 1, 'flat', 1, True, False, (70, 70, 70)),\n",
    "    ]\n",
    "\n",
    "    train_id_to_color = [c.color for c in classes if (c.train_id != -1 and c.train_id != 255)]\n",
    "    train_id_to_color.append([0, 0, 0])\n",
    "    train_id_to_color = np.array(train_id_to_color)\n",
    "    id_to_train_id = np.array([c.train_id for c in classes])\n",
    "    \n",
    "    #train_id_to_color = [(0, 0, 0), (128, 64, 128), (70, 70, 70), (153, 153, 153), (107, 142, 35),\n",
    "    #                      (70, 130, 180), (220, 20, 60), (0, 0, 142)]\n",
    "    #train_id_to_color = np.array(train_id_to_color)\n",
    "    #id_to_train_id = np.array([c.category_id for c in classes], dtype='uint8') - 1\n",
    "\n",
    "    def __init__(self, root, split='train', mode='gtfine', target_type='sia', transform=None):\n",
    "        root = '/home/aiffel-dj17/aiffel/siaiffel/DeepLabV3Plus-Pytorch-master/datasets/data/SIA/roads'\n",
    "        self.root = os.path.expanduser(root)\n",
    "        \n",
    "        self.mode = 'gtFine'\n",
    "        self.target_type = target_type\n",
    "        self.images_dir = os.path.join(self.root, 'leftImg8bit', split)\n",
    "\n",
    "        self.targets_dir = os.path.join(self.root, self.mode, split)\n",
    "        self.transform = transform\n",
    "\n",
    "        self.split = split\n",
    "        self.images = []\n",
    "        self.targets = []\n",
    "\n",
    "        if split not in ['train', 'test', 'val']:\n",
    "            raise ValueError('Invalid split for mode! Please use split=\"train\", split=\"test\"'\n",
    "                             ' or split=\"val\"')\n",
    "\n",
    "        if not os.path.isdir(self.images_dir) or not os.path.isdir(self.targets_dir):\n",
    "            raise RuntimeError('Dataset not found or incomplete. Please make sure all required folders for the'\n",
    "                               ' specified \"split\" and \"mode\" are inside the \"root\" directory')\n",
    "        \n",
    "        #for city in os.listdir(self.images_dir):\n",
    "            \n",
    "        img_dir = os.path.join(self.images_dir)\n",
    "        target_dir = os.path.join(self.targets_dir)\n",
    "        \n",
    "        for file_name in os.listdir(img_dir):\n",
    "            self.images.append(os.path.join(img_dir, file_name))\n",
    "            target_name = '{}{}'.format(file_name.split('.')[0],\n",
    "                                         self._get_target_suffix(self.mode, self.target_type))\n",
    "            self.targets.append(os.path.join(target_dir, target_name))\n",
    "\n",
    "    @classmethod\n",
    "    def encode_target(cls, target):\n",
    "        return cls.id_to_train_id[np.array(target)]\n",
    "\n",
    "    @classmethod\n",
    "    def decode_target(cls, target):\n",
    "        target[target == 255] = 1\n",
    "        #target = target.astype('uint8') + 1\n",
    "        return cls.train_id_to_color[target]\n",
    "    \n",
    "    def make_encode_target(self, target):\n",
    "        target = cv2.cvtColor(target, cv2.COLOR_BGR2GRAY)\n",
    "        ret, dst = cv2.threshold(target, 20, 1, cv2.THRESH_BINARY)\n",
    "        return dst\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is a tuple of all target types if target_type is a list with more\n",
    "            than one item. Otherwise target is a json object if target_type=\"polygon\", else the image segmentation.\n",
    "        \"\"\"\n",
    "        image = Image.open(self.images[index]).convert('RGB')\n",
    "        target = Image.open(self.targets[index])\n",
    "        \n",
    "        if self.transform:\n",
    "            image, target = self.transform(image, target)\n",
    "        target = np.array(target)\n",
    "        target = self.make_encode_target(target)\n",
    "        target = np.array(target, dtype='int')\n",
    "        \n",
    "        return image, target\n",
    "\n",
    "    def __getimg__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is a tuple of all target types if target_type is a list with more\n",
    "            than one item. Otherwise target is a json object if target_type=\"polygon\", else the image segmentation.\n",
    "        \"\"\"\n",
    "        image = Image.open(self.images[index]).convert('RGB')\n",
    "        print(self.images[index])\n",
    "        #target = Image.open(self.targets[index])\n",
    "        #if self.transform:\n",
    "        #    image, target = self.transform(image, target)\n",
    "        #target = self.encode_target(target)\n",
    "        return image #, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def _load_json(self, path):\n",
    "        with open(path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        return data\n",
    "\n",
    "    def _get_target_suffix(self, mode, target_type):\n",
    "        if target_type == 'instance':\n",
    "            return '{}_instanceIds.png'.format(mode)\n",
    "        elif target_type == 'semantic':\n",
    "            return '{}_labelIds.png'.format(mode)\n",
    "        elif target_type == 'color':\n",
    "            return '{}_color.png'.format(mode)\n",
    "        elif target_type == 'polygon':\n",
    "            return '{}_polygons.json'.format(mode)\n",
    "        elif target_type == 'depth':\n",
    "            return '{}_disparity.png'.format(mode)\n",
    "        elif target_type == 'sia':\n",
    "            return '.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a75c1b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = '/home/aiffel-dj17/aiffel/siaiffel/DeepLabV3Plus-Pytorch-master/datasets/data/SIA/roads'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05ee2429",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(opts):\n",
    "    \"\"\" Dataset And Augmentation\n",
    "    \"\"\"\n",
    "    if opts == 'cityscapes':\n",
    "        train_transform = ExtCompose([\n",
    "            #et.ExtResize( 512 ),\n",
    "            ExtRandomCrop(size=(512, 512)),\n",
    "            ExtColorJitter( brightness=0.5, contrast=0.5, saturation=0.5 ),\n",
    "            ExtRandomHorizontalFlip(),\n",
    "            ExtToTensor(),\n",
    "            ExtNormalize(mean=[0.485, 0.456, 0.406],\n",
    "                            std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "        val_transform = ExtCompose([\n",
    "            #et.ExtResize( 512 ),\n",
    "            ExtToTensor(),\n",
    "            ExtNormalize(mean=[0.485, 0.456, 0.406],\n",
    "                            std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "        train_dst = Cityscapes(root=data_root,\n",
    "                               split='train', transform=train_transform)\n",
    "        val_dst = Cityscapes(root=data_root,\n",
    "                             split='val', transform=val_transform)\n",
    "#trnasforms  \n",
    "#satellites 추가 \n",
    "    if opts == 'satellites':\n",
    "        train_transform = ExtCompose([\n",
    "            #et.ExtResize( 512 ),\n",
    "            ExtRandomCrop(size=(512, 512)),\n",
    "            ExtColorJitter( brightness=0.5, contrast=0.5, saturation=0.5 ),\n",
    "            ExtRandomHorizontalFlip(),\n",
    "            ExtToTensor(),\n",
    "            ExtNormalize(mean=[0.485, 0.456, 0.406],\n",
    "                            std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "        val_transform = ExtCompose([\n",
    "            #et.ExtResize( 512 ),\n",
    "            ExtToTensor(),\n",
    "            ExtNormalize(mean=[0.485, 0.456, 0.406],\n",
    "                            std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "#datasets\n",
    "        train_dst = Satellites(root=data_root,\n",
    "                               split='train', transform=train_transform)\n",
    "        val_dst = Satellites(root=data_root,\n",
    "                             split='val', transform=val_transform)\n",
    "    return train_dst, val_dst\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75d456cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dst, val_dst = get_dataset('satellites')\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dst, batch_size=8, \n",
    "                                           shuffle=True, num_workers=2)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(val_dst, batch_size=8,\n",
    "                                        shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c307443b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matplotlib_imshow(img, one_channel=False):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a077b230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# images, label = next(iter(train_loader))\n",
    "# images.shape, label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c8bc133b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 임의의 학습 이미지를 가져옵니다\n",
    "# dataiter = iter(get_dataset) #여기에 추가 \n",
    "\n",
    "images, labels = next(iter(train_loader))\n",
    "images = torch.Tensor(images)\n",
    "# labels = torch.Tensor(labels)\n",
    "\n",
    "# 이미지 그리드를 만듭니다. #이미지 훈련시킬때 중간에 시각화하는 함수 \n",
    "img_grid = torchvision.utils.make_grid(images) #이미지가 tensor or list\n",
    "\n",
    "# 이미지를 보여줍니다.\n",
    "matplotlib_imshow(img_grid, one_channel=True)\n",
    "\n",
    "# tensorboard에 기록합니다. #이미지 기록 \n",
    "writer.add_image('images', img_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "49d72b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'torchvision.transforms.functional' has no attribute 'interpolate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-b8e198865652>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#모델\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/torch/utils/tensorboard/writer.py\u001b[0m in \u001b[0;36madd_graph\u001b[0;34m(self, model, input_to_model, verbose)\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'forward'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m             \u001b[0;31m# A valid PyTorch model should have a 'forward' method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 723\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_file_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_to_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    724\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0;31m# Caffe2 models do not have the 'forward' method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/torch/utils/tensorboard/_pytorch_graph.py\u001b[0m in \u001b[0;36mgraph\u001b[0;34m(model, args, verbose)\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_model_mode_for_export\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrainingMode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEVAL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# TODO: move outside of torch.onnx?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             \u001b[0mtrace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m             \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_pass_inline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/torch/jit/_trace.py\u001b[0m in \u001b[0;36mtrace\u001b[0;34m(func, example_inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit)\u001b[0m\n\u001b[1;32m    740\u001b[0m             \u001b[0mstrict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m             \u001b[0m_force_outplace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 742\u001b[0;31m             \u001b[0m_module_class\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    743\u001b[0m         )\n\u001b[1;32m    744\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/torch/jit/_trace.py\u001b[0m in \u001b[0;36mtrace_module\u001b[0;34m(mod, inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit)\u001b[0m\n\u001b[1;32m    938\u001b[0m                 \u001b[0mvar_lookup_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m                 \u001b[0mstrict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 940\u001b[0;31m                 \u001b[0m_force_outplace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    941\u001b[0m             )\n\u001b[1;32m    942\u001b[0m             \u001b[0mcheck_trace_method\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 887\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mrecording_scopes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 860\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    861\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrecording_scopes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-038fbcc30085>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bilinear'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malign_corners\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 887\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mrecording_scopes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 860\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    861\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrecording_scopes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-69c644393fd8>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, feature)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mlow_level_feature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproject\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'low_level'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0moutput_feature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maspp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'out'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0moutput_feature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_feature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlow_level_feature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bilinear'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malign_corners\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mlow_level_feature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_feature\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 887\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mrecording_scopes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 860\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    861\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrecording_scopes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-69c644393fd8>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mconv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproject\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 887\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mrecording_scopes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 860\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    861\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrecording_scopes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-69c644393fd8>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mASPPPooling\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bilinear'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malign_corners\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mASPP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torchvision.transforms.functional' has no attribute 'interpolate'"
     ]
    }
   ],
   "source": [
    "#모델\n",
    "writer.add_graph(model_1, images)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6957868f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.1+cu102\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "863e9e8c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tensorboard' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-d0287bf0f286>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensorboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tensorboard' is not defined"
     ]
    }
   ],
   "source": [
    "print(tensorboard.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "380f490a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e6f1e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0\n"
     ]
    }
   ],
   "source": [
    "print(tensorboard.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d2f39c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
