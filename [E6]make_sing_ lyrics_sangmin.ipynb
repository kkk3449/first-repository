{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "electoral-jones",
   "metadata": {},
   "source": [
    "### Step 1. 데이터 읽어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "insured-hammer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " [' There must be some kind of way outta here', 'Said the joker to the thief', \"There's too much confusion\"]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import re                  # 정규표현식을 위한 Regex 지원 모듈 (문장 데이터를 정돈하기 위해) \n",
    "import numpy as np         # 변환된 문장 데이터(행렬)을 편하게 처리하기 위해\n",
    "import tensorflow as tf    # 대망의 텐서플로우!\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perfect-reaction",
   "metadata": {},
   "source": [
    "### Step 2. 데이터 정제\n",
    "나만의 방법으로 데이터 정제를 해보았다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "beautiful-founder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence . <end>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()       # 소문자로 바꾸고 양쪽 공백을 삭제\n",
    "  \n",
    "    # 아래 3단계를 거쳐 sentence는 스페이스 1개를 delimeter로 하는 소문자 단어 시퀀스로 바뀝니다.\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)        # 패턴의 특수문자를 만나면 특수문자 양쪽에 공백을 추가\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)                  # 공백 패턴을 만나면 스페이스 1개로 치환\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)  # a-zA-Z?.!,¿ 패턴을 제외한 모든 문자(공백문자까지도)를 스페이스 1개로 치환\n",
    "    #sentence.split()\n",
    "    \n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    sentence = '<start> ' + sentence + ' <end>'      # 이전 스텝에서 본 것처럼 문장 앞뒤로 <start>와 <end>를 단어처럼 붙여 줍니다\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))   # 이 문장이 어떻게 필터링되는지 확인해 보세요.\n",
    "#print(len(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))\n",
    "type(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geographic-european",
   "metadata": {},
   "source": [
    "#### Step 2_1. 방법1 \n",
    " - *preprocess_sentence1*은 split 후에 if - else 를 사용하여 토큰 개수를 세서 14 초과이면 14까지 슬라이싱해서 넣고 14이하이면 그냥 넣는 방법이다. 하지만 이 방법으로 데이터 정제를 한 후에 데이터 수가 너무 많아서 학습하는데 시간이 오래걸리고, 긴 문장은 제외하라는 과제에 적합하지 않아 사용하지 않았다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "monthly-encyclopedia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence .  <end>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_sentence1(sentence):\n",
    "\n",
    "    sentence = sentence.lower().strip()       # 소문자로 바꾸고 양쪽 공백을 삭제\n",
    "  \n",
    "    # 아래 3단계를 거쳐 sentence는 스페이스 1개를 delimeter로 하는 소문자 단어 시퀀스로 바뀝니다.\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)        # 패턴의 특수문자를 만나면 특수문자 양쪽에 공백을 추가\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)                  # 공백 패턴을 만나면 스페이스 1개로 치환\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)  # a-zA-Z?.!,¿ 패턴을 제외한 모든 문자(공백문자까지도)를 스페이스 1개로 치환\n",
    "    \n",
    "    sentence=re.split(\" \", sentence)\n",
    "    \n",
    "    if len(sentence) > 14 : short_sentence = sentence[:14]\n",
    "        \n",
    "    else : short_sentence = sentence\n",
    "    \n",
    "    short_sentence = \" \".join(short_sentence)\n",
    "    short_sentence = '<start> ' + short_sentence + ' <end>'      # 이전 스텝에서 본 것처럼 문장 앞뒤로 <start>와 <end>를 단어처럼 붙여 줍니다\n",
    "    \n",
    "    return short_sentence\n",
    "\n",
    "print(preprocess_sentence1(\"This @_is ;;;sample        sentence.\"))   # 이 문장이 어떻게 필터링되는지 확인해 보세요.\n",
    "#print(len(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))\n",
    "type(preprocess_sentence1(\"This @_is ;;;sample        sentence.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regular-wheat",
   "metadata": {},
   "source": [
    "#### Step 2_2. 방법2 \n",
    "- *preprocess_sentence2* 은 문장을 *split* 후에 *if - else* 를 사용하여 토큰 개수를 세서 14 이상이면 *return False* 를 하고 *else* 이면 *.join* 함수를 써서 *split* 한 것을 붙인 다음 문장을 *return* 한다. *return False* 한 값은 아래 코드 *for - if* 문에서 다시 한번 확인한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "operating-preliminary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence .  <end>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_sentence2(sentence):\n",
    "\n",
    "    sentence = sentence.lower().strip()       # 소문자로 바꾸고 양쪽 공백을 삭제\n",
    "  \n",
    "    # 아래 3단계를 거쳐 sentence는 스페이스 1개를 delimeter로 하는 소문자 단어 시퀀스로 바뀝니다.\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)        # 패턴의 특수문자를 만나면 특수문자 양쪽에 공백을 추가\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)                  # 공백 패턴을 만나면 스페이스 1개로 치환\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)  # a-zA-Z?.!,¿ 패턴을 제외한 모든 문자(공백문자까지도)를 스페이스 1개로 치환\n",
    "    \n",
    "    sentence=re.split(\" \", sentence)\n",
    "    \n",
    "    if len(sentence) > 13 : \n",
    "        return False\n",
    "    \n",
    "    else : \n",
    "        sentence = \" \".join(sentence)\n",
    "        sentence = '<start> ' + sentence + ' <end>'      # 이전 스텝에서 본 것처럼 문장 앞뒤로 <start>와 <end>를 단어처럼 붙여 줍니다\n",
    "    \n",
    "        return sentence\n",
    "\n",
    "print(preprocess_sentence2(\"This @_is ;;;sample        sentence.\"))   # 이 문장이 어떻게 필터링되는지 확인해 보세요.\n",
    "#print(len(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))\n",
    "type(preprocess_sentence2(\"This @_is ;;;sample        sentence.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "median-humor",
   "metadata": {},
   "source": [
    "#### Step 2_3. 방법2\n",
    " - *for - if* 문으로 *preprocess_sentence2* 에서 *return* 값이 *False* 이면 *continue* 로 제외시킨다.\n",
    " - tokenize 전에 정제된 데이터 수를 확인한다.\n",
    " - 정제된 데이터 수가 너무 적거나 많으면 다른 방법을 생각한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "governing-cylinder",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<start> there must be some kind of way outta here <end>', '<start> said the joker to the thief <end>', '<start> there s too much confusion <end>', '<start> i can t get no relief business men , they drink my wine <end>', '<start> plowman dig my earth <end>', '<start> none were level on the mind <end>', '<start> nobody up at his word <end>', '<start> hey , hey no reason to get excited <end>', '<start> the thief he kindly spoke <end>', '<start> there are many here among us <end>']\n",
      "<class 'list'>\n",
      "154531\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    #print(len(sentence))\n",
    "    if len(sentence) == 0: continue\n",
    "    if sentence[-1] == \":\": continue\n",
    "    if preprocess_sentence2(sentence) == False : continue\n",
    "    #print(len(sentence))\n",
    "    corpus.append(preprocess_sentence2(sentence))\n",
    "        \n",
    "print(corpus[:10])\n",
    "print(type(corpus))\n",
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metric-printing",
   "metadata": {},
   "source": [
    "#### Step 2_4. tokenize \n",
    "- *tokenize* 의 *num_words* 는 *14000* 으로 했다. *tensor*, *tokenizer* 를 출력하여 확인해보니 단어가 숫자화 되고 끝 부분은 0으로 *padding* 된 것을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "appropriate-second",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   61  272 ...    0    0    0]\n",
      " [   2  119    6 ...    0    0    0]\n",
      " [   2   61   17 ...    0    0    0]\n",
      " ...\n",
      " [   2    6  583 ...    0    0    0]\n",
      " [   2    7  214 ...    0    0    0]\n",
      " [   2    8 3374 ...    0    0    0]]\n",
      "<keras_preprocessing.text.Tokenizer object at 0x7f969c6bc710>\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "    # 텐서플로우에서 제공하는 Tokenizer 패키지를 생성\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=14000,  # 전체 단어의 개수 \n",
    "        filters=' ',    # 별도로 전처리 로직을 추가할 수 있습니다. 이번에는 사용하지 않겠습니다.\n",
    "        oov_token=\"<unk>\"  # out-of-vocabulary, 사전에 없었던 단어는 어떤 토큰으로 대체할지\n",
    "    )\n",
    "    tokenizer.fit_on_texts(corpus)   # 우리가 구축한 corpus로부터 Tokenizer가 사전을 자동구축하게 됩니다.\n",
    "\n",
    "    # 이후 tokenizer를 활용하여 모델에 입력할 데이터셋을 구축하게 됩니다.\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   # tokenizer는 구축한 사전으로부터 corpus를 해석해 Tensor로 변환합니다.\n",
    "\n",
    "    # 입력 데이터의 시퀀스 길이를 일정하게 맞추기 위한 padding  메소드를 제공합니다.\n",
    "    # maxlen의 디폴트값은 None입니다. 이 경우 corpus의 가장 긴 문장을 기준으로 시퀀스 길이가 맞춰집니다.\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  \n",
    "\n",
    "    print(tensor,sep='\\n')\n",
    "    print(tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "everyday-overall",
   "metadata": {},
   "source": [
    "#### Step 2_5. tokenize 값 확인 \n",
    "- 텐서화 된 값을 자세히 출력해보면 이와같이 앞부분엔 무조건  *start* 가 있어 2 인 것을 확인할 수 있다. 그리고 문장 끝엔 3으로  *end* 가 들어간 것을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "processed-tucson",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   61  272   27   94  551   20   86  750   90]\n",
      " [   2  119    6 6225   10    6 2289    3    0    0]\n",
      " [   2   61   17  102  187 2688    3    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "print(tensor[:3, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dated-alert",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : i\n",
      "5 : ,\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n"
     ]
    }
   ],
   "source": [
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "united-stress",
   "metadata": {},
   "source": [
    "##### 총 데이터셋 행렬 모양은 (154531, 14) 이고 src_input 과 tgt_input 모양이 같다는 것을 확인했다. \n",
    "- 문장 수 : 154531\n",
    "- 문장의 최대 단어(토큰) 개수 : 14개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "developing-swaziland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2  61 272  27  94 551  20  86 750  90   3   0   0   0]\n",
      "[ 61 272  27  94 551  20  86 750  90   3   0   0   0   0]\n",
      "154531\n",
      "154531\n",
      "<class 'numpy.ndarray'>\n",
      "(154531, 14)\n"
     ]
    }
   ],
   "source": [
    "src_input = tensor[:, :-1]  # tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성합니다. 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높습니다.\n",
    "tgt_input = tensor[:, 1:]    # tensor에서 <start>를 잘라내서 타겟 문장을 생성합니다.\n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])\n",
    "print(len(src_input))\n",
    "print(len(tgt_input))\n",
    "print(type(src_input))\n",
    "print(src_input.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "antique-conservative",
   "metadata": {},
   "source": [
    "### Step 3. 평가 데이터셋 분리\n",
    "- tokenize() 함수로 데이터를 Tensor로 변환한 후, sklearn 모듈의 train_test_split() 함수를 사용해 훈련 데이터와 평가 데이터를 분리했다.단어장의 크기는 14,000이고 총 데이터의 20%를 평가 데이터셋으로 사용했다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "alleged-wallace",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "enc_train, enc_val = train_test_split(src_input, \n",
    "                                        test_size=0.2,\n",
    "                                        random_state=34,\n",
    "                                        shuffle=True)\n",
    "dec_train, dec_val = train_test_split(tgt_input, \n",
    "                                        test_size=0.2,\n",
    "                                        random_state=34,\n",
    "                                        shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smart-prerequisite",
   "metadata": {},
   "source": [
    "### Step 3-1. 평가 데이터셋 갯수 확인\n",
    "- 데이터셋을 분리했을 때, 학습 데이터 갯수가 과제 목표 124960보다 작은 것을 확인했다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composed-scholar",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "martial-adjustment",
   "metadata": {},
   "source": [
    "### Step 3-2. BUFFER_SIZE, dataset, val_dataset 설정 \n",
    "- *BUFFER_SIZE* , *dataset* , *val_dataset* 에 분리한 데이터 셋 값들을 넣어주었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cardiac-scheme",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123624\n",
      "482\n"
     ]
    }
   ],
   "source": [
    "BUFFER_SIZE = len(enc_train)\n",
    "print(len(enc_train))\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(enc_train) // BATCH_SIZE\n",
    "print(steps_per_epoch)\n",
    "VOCAB_SIZE = tokenizer.num_words + 1    # tokenizer가 구축한 단어사전 내 7000개와, 여기 포함되지 않은 0:<pad>를 포함하여 7001개\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((enc_train, dec_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((enc_val, dec_val)).shuffle(len(enc_val))\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sexual-intention",
   "metadata": {},
   "source": [
    "### Step 4. 인공지능 만들기\n",
    "- RNN을 사용했다. hidden_size를 2048로 높게 주었다. 임베딩을 통해 단어들간의 상대적인 거리 값으로 학습한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "described-helping",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super(TextGenerator, self).__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 256\n",
    "hidden_size = 2048\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)\\\n",
    "#print(\"f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corresponding-claim",
   "metadata": {},
   "source": [
    "### Step 4_1. 셈플확인\n",
    "- 최종 출력 텐서 shape가 shape=(256, 14, 14001)인 것을 확인 했다. 14001은 Dense 레이어의 출력 차원 수 이다. 14001개의 단어 중 어느 단어의 확률이 가장 높을지를 모델링해야 하기 때문이다. 256은 배치 사이즈이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "discrete-stuart",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 14, 14001), dtype=float32, numpy=\n",
       "array([[[ 7.3888616e-05,  4.1382926e-04, -1.3355503e-04, ...,\n",
       "          2.4013220e-04, -1.6636026e-04,  3.5196939e-05],\n",
       "        [ 1.8130390e-04,  6.0105446e-04, -2.2397129e-04, ...,\n",
       "         -3.2661126e-05, -4.3243633e-04,  1.4015993e-04],\n",
       "        [ 3.1436139e-04,  8.2724454e-04,  4.0651063e-05, ...,\n",
       "         -2.1022202e-04, -6.4477813e-04,  1.5606832e-04],\n",
       "        ...,\n",
       "        [ 3.4853735e-04,  4.1987450e-04, -1.2359800e-04, ...,\n",
       "         -2.5477511e-04,  8.6093909e-04, -4.1772844e-04],\n",
       "        [ 3.9464838e-04,  7.7889890e-05, -2.5878238e-04, ...,\n",
       "          1.6956119e-04,  9.0672815e-04, -4.5339690e-04],\n",
       "        [ 4.8283782e-04, -2.8222139e-04, -3.6002140e-04, ...,\n",
       "          6.0202961e-04,  8.8616187e-04, -5.1320856e-04]],\n",
       "\n",
       "       [[ 7.3888616e-05,  4.1382926e-04, -1.3355503e-04, ...,\n",
       "          2.4013220e-04, -1.6636026e-04,  3.5196939e-05],\n",
       "        [-1.2775954e-04,  4.6104289e-04,  5.2953732e-05, ...,\n",
       "          7.0682674e-04,  1.4231347e-04,  8.6757515e-05],\n",
       "        [-3.6743257e-04,  1.5501164e-04,  2.3223563e-04, ...,\n",
       "          9.9478138e-04,  4.6682841e-04,  2.2551623e-04],\n",
       "        ...,\n",
       "        [-9.0246840e-06, -1.9317308e-04, -2.5868710e-04, ...,\n",
       "          8.7687782e-05,  4.8857584e-04,  6.0618389e-04],\n",
       "        [ 1.2282447e-04, -4.2358611e-04, -2.9387564e-04, ...,\n",
       "          3.9482181e-04,  4.0950617e-04,  4.8197788e-04],\n",
       "        [ 2.6933991e-04, -7.1526680e-04, -2.9145723e-04, ...,\n",
       "          7.0704881e-04,  2.7284917e-04,  3.5683467e-04]],\n",
       "\n",
       "       [[ 7.3888616e-05,  4.1382926e-04, -1.3355503e-04, ...,\n",
       "          2.4013220e-04, -1.6636026e-04,  3.5196939e-05],\n",
       "        [-6.2091072e-05,  4.5394819e-04, -1.1892567e-04, ...,\n",
       "          1.4104779e-04, -2.5561592e-04, -1.7880683e-04],\n",
       "        [-8.7421373e-05,  2.8161757e-04,  1.9933202e-04, ...,\n",
       "          1.9808160e-04, -1.8365891e-04, -2.9349959e-04],\n",
       "        ...,\n",
       "        [-4.2701366e-05, -8.6188910e-04, -1.0433142e-03, ...,\n",
       "          1.5645092e-03, -1.2600240e-04, -4.1870901e-04],\n",
       "        [ 1.8115275e-04, -1.1178133e-03, -8.9940231e-04, ...,\n",
       "          1.7663026e-03, -2.7359553e-04, -4.1064023e-04],\n",
       "        [ 3.6815833e-04, -1.3826637e-03, -7.2613446e-04, ...,\n",
       "          1.9508444e-03, -4.5703386e-04, -3.8655187e-04]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 7.3888616e-05,  4.1382926e-04, -1.3355503e-04, ...,\n",
       "          2.4013220e-04, -1.6636026e-04,  3.5196939e-05],\n",
       "        [ 1.8130390e-04,  6.0105446e-04, -2.2397129e-04, ...,\n",
       "         -3.2661126e-05, -4.3243633e-04,  1.4015993e-04],\n",
       "        [ 2.4012812e-04,  4.1533238e-04, -2.5336351e-04, ...,\n",
       "         -1.2239718e-04, -4.0336850e-04,  3.4885763e-04],\n",
       "        ...,\n",
       "        [ 6.3441013e-04, -1.0603374e-03,  4.6219782e-04, ...,\n",
       "          7.5813371e-04,  5.4716278e-04,  8.2879199e-04],\n",
       "        [ 7.9320429e-04, -1.3429712e-03,  4.9816637e-04, ...,\n",
       "          1.1043915e-03,  3.0608935e-04,  6.8413134e-04],\n",
       "        [ 9.0093911e-04, -1.6095814e-03,  5.3495314e-04, ...,\n",
       "          1.4162624e-03,  2.9700699e-05,  5.6597171e-04]],\n",
       "\n",
       "       [[ 7.3888616e-05,  4.1382926e-04, -1.3355503e-04, ...,\n",
       "          2.4013220e-04, -1.6636026e-04,  3.5196939e-05],\n",
       "        [ 2.9020250e-04,  8.2259672e-04, -1.1489040e-04, ...,\n",
       "          3.8035336e-04, -5.8259971e-05,  1.8948502e-05],\n",
       "        [ 4.2903589e-04,  1.1670634e-03, -9.0622103e-05, ...,\n",
       "          5.4269680e-04,  7.1514092e-05,  1.5466596e-04],\n",
       "        ...,\n",
       "        [-8.0048532e-04,  9.6310431e-04, -1.2779339e-04, ...,\n",
       "          1.5600156e-03, -3.4988683e-04,  2.1053539e-04],\n",
       "        [-7.1435160e-04,  8.2169950e-04, -1.2054307e-04, ...,\n",
       "          1.6974958e-03, -3.9406694e-04,  1.1879460e-04],\n",
       "        [-5.1701919e-04,  5.2864751e-04, -1.2909209e-04, ...,\n",
       "          1.8767455e-03, -4.2963238e-04,  1.5077914e-05]],\n",
       "\n",
       "       [[ 7.3888616e-05,  4.1382926e-04, -1.3355503e-04, ...,\n",
       "          2.4013220e-04, -1.6636026e-04,  3.5196939e-05],\n",
       "        [ 1.8130390e-04,  6.0105446e-04, -2.2397129e-04, ...,\n",
       "         -3.2661126e-05, -4.3243633e-04,  1.4015993e-04],\n",
       "        [ 5.9874216e-04,  4.9790350e-04, -3.2777651e-04, ...,\n",
       "         -2.8389230e-04, -5.9906073e-04,  2.9725398e-04],\n",
       "        ...,\n",
       "        [ 9.9340721e-04, -1.9156067e-03, -5.3988799e-04, ...,\n",
       "          1.8560750e-03,  6.5104687e-05, -4.1244991e-04],\n",
       "        [ 1.0872982e-03, -2.1593820e-03, -3.7893228e-04, ...,\n",
       "          2.0937626e-03, -8.4136045e-05, -4.4820382e-04],\n",
       "        [ 1.1397875e-03, -2.3664918e-03, -2.1492933e-04, ...,\n",
       "          2.2864440e-03, -2.8182138e-04, -4.4651513e-04]]], dtype=float32)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "unauthorized-tissue",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  3584256   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  18882560  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  33562624  \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  28688049  \n",
      "=================================================================\n",
      "Total params: 84,717,489\n",
      "Trainable params: 84,717,489\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polish-alexandria",
   "metadata": {},
   "source": [
    "### Step 4_2. model.fit\n",
    "- 만들어둔 dataset과 val_dataset 변수를 넣어주고 epoch을 10으로 하여 fit 해주었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "sustainable-distinction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "482/482 [==============================] - 214s 445ms/step - loss: 3.4614 - val_loss: 3.0340\n",
      "Epoch 2/10\n",
      "482/482 [==============================] - 216s 449ms/step - loss: 2.8494 - val_loss: 2.7582\n",
      "Epoch 3/10\n",
      "482/482 [==============================] - 216s 449ms/step - loss: 2.5453 - val_loss: 2.5826\n",
      "Epoch 4/10\n",
      "482/482 [==============================] - 216s 448ms/step - loss: 2.2619 - val_loss: 2.4511\n",
      "Epoch 5/10\n",
      "482/482 [==============================] - 216s 449ms/step - loss: 1.9966 - val_loss: 2.3524\n",
      "Epoch 6/10\n",
      "482/482 [==============================] - 216s 449ms/step - loss: 1.7533 - val_loss: 2.2795\n",
      "Epoch 7/10\n",
      "482/482 [==============================] - 211s 438ms/step - loss: 1.5392 - val_loss: 2.2299\n",
      "Epoch 8/10\n",
      "482/482 [==============================] - 211s 437ms/step - loss: 1.3593 - val_loss: 2.2049\n",
      "Epoch 9/10\n",
      "482/482 [==============================] - 215s 447ms/step - loss: 1.2165 - val_loss: 2.1962\n",
      "Epoch 10/10\n",
      "482/482 [==============================] - 216s 447ms/step - loss: 1.1121 - val_loss: 2.2056\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f969a96c190>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "#model.fit(dataset, epochs=10, validation_data = val_dataset ,steps_per_epoch = steps_per_epoch, validation_steps=len(dec_val) // BATCH_SIZE )\n",
    "model.fit(dataset, epochs=10, validation_data = val_dataset , verbose=1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absolute-banana",
   "metadata": {},
   "source": [
    "### Step 5. 인공지능 Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "related-victor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 일단 텐서로 변환합니다.\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 텍스트를 실제로 생성할때는 루프를 돌면서 단어 하나씩 생성해야 합니다. \n",
    "    while True:\n",
    "        predict = model(test_tensor)  # 입력받은 문장의 텐서를 입력합니다. \n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]   # 우리 모델이 예측한 마지막 단어가 바로 새롭게 생성한 단어가 됩니다. \n",
    "\n",
    "        # 우리 모델이 새롭게 예측한 단어를 입력 문장의 뒤에 붙여 줍니다. \n",
    "        test_tensor = tf.concat([test_tensor, \n",
    "                                                                 tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "\n",
    "        # 우리 모델이 <end>를 예측했거나, max_len에 도달하지 않았다면  while 루프를 또 돌면서 다음 단어를 예측해야 합니다.\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # 생성된 tensor 안에 있는 word index를 tokenizer.index_word 사전을 통해 실제 단어로 하나씩 변환합니다. \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated   # 이것이 최종적으로 모델이 생성한 자연어 문장입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "charitable-diploma",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love ma little nasty girl <end> '"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "entertaining-perception",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> turn up the bass til it s up in your face level <end> '"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> turn up\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "occasional-abraham",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> flex in the morning <end> '"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> flex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "worse-pittsburgh",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i make it spin on my finger im a critical thinker <end> '"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> I make it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "gross-strengthening",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> im rockin the flow i pop em the other day <end> '"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> im rockin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "vanilla-incidence",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> so easy to say <end> '"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> so easy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "joined-bermuda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> god damn , god damn <end> '"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> god\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italian-colleague",
   "metadata": {},
   "source": [
    "# 마치며 ...\n",
    " - 처음에 maxlen을 안쓰고 토큰화 하는 방법을 고민하다가 split, join, if-else-continue 방법으로 잘 되어 기뻤다. 하지만 val_loss가 잘 나와도 2.4정도까지 밖에 안내려가서 epochs도 낮춰보고, steps_per_epoch, validation_steps 값도 넣어보고, embedding_size, hidden_size 값을 낮추어 봤는데 계속 2.4 이상이 나왔다. 그래서 hidden_size 값을 2048로 대폭 올렸더니 val_loss가 2.2 이하가 나와서 기뻤다. 하지만 hidden_size 값을 2048로 주면 학습시간이 엄청 늘어났다. hidden_size = 1024 학습시간은 20분 정도 걸렸던거 같은데 hidden_size = 2048 학습시간은 1시간 정도 걸린 것 같다.   \n",
    "---\n",
    "- 노래 가사 출력은 오버피팅 되지 않고 적절하게 출력된 것 같다. 힙합을 좋아해서 힙합에서 낳오는 가사들을 몇개 넣어봤는데 만족스럽다. 근데 사실 val_loss가 2.4(hidden_size = 1024) 인 것과 2.2(hidden_size = 2048) 인 것의 출력 가사는 큰 차이는 없는 것 같다. memory 오류가 두려워 코드를 추가하지는 않겠다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beneficial-indonesia",
   "metadata": {},
   "source": [
    "평가 문항\t상세 기준\n",
    "1 . 가사 텍스트 생성 모델이 동작 하는가?\n",
    "\n",
    "텍스트 제너레이션 결과가 그럴듯한 문장으로 생성 되는가?\n",
    "2 . 데이터의 전처리와 데이터 셋 구성 과정이 체계적으로 진행 되었습니까?\n",
    "\n",
    "특수 문자 제거, 토크 나이저 생성, 패딩 처리 등의 과정이 빠짐없이 진행 되었습니까?\n",
    "3 . 텍스트 생성 모델이 안정적으로 학습 되었습니까?\n",
    "\n",
    "텍스트 생성 모델의 유효성 검사 손실 2.2 이하로 생성 는가?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
